{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOSmrurnkotheHtEG5PtaTA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d7703daafeaf4527bb55ec4de96224a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b24646f3a73d445ba3f8fb4ad8744247",
              "IPY_MODEL_98080b61b681460e9fe17756f9539293",
              "IPY_MODEL_40ba9e21a64c41539d8ea79b68b473e8"
            ],
            "layout": "IPY_MODEL_a4b541244fdc4c468b259a268f629e99"
          }
        },
        "b24646f3a73d445ba3f8fb4ad8744247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_689e7919a4b34958b26f3c39ad914ddc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9c57adc0432540119c53a463628f45f7",
            "value": "Scanning:‚Äá100%"
          }
        },
        "98080b61b681460e9fe17756f9539293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bbfdfd9791040bf8b929c59d51be14f",
            "max": 226,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aebd159cdba4421b9c648f6d9f395fa8",
            "value": 226
          }
        },
        "40ba9e21a64c41539d8ea79b68b473e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76df98f6ede24a7b8f33b1ca3311c2aa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_450cfa75048e4a8cb60c64731cdab258",
            "value": "‚Äá226/226‚Äá[10:15&lt;00:00,‚Äá‚Äá3.69s/seg]"
          }
        },
        "a4b541244fdc4c468b259a268f629e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "689e7919a4b34958b26f3c39ad914ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c57adc0432540119c53a463628f45f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bbfdfd9791040bf8b929c59d51be14f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aebd159cdba4421b9c648f6d9f395fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76df98f6ede24a7b8f33b1ca3311c2aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "450cfa75048e4a8cb60c64731cdab258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/herndoch/dermopath-ai-hub/blob/main/Knowledge_Pipeline_v4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Me\n",
        "<details>\n",
        "<summary><strong>ü§ñ AI-to-AI Handover Protocol (Click to Expand)</strong></summary>\n",
        "\n",
        "# ü§ñ AI-to-AI Handover Protocol (Read First)\n",
        "\n",
        "**‚ö†Ô∏è CRITICAL SYSTEM INVARIANTS**\n",
        "*Do not modify these settings without explicit human authorization. These constraints exist to prevent known failure modes.*\n",
        "\n",
        "### 1. Model Hierarchy & Reasoning\n",
        "*   **The \"Monolith\" Rule:** Architect Blocks (PDF Block 2 & Video Block 2) **MUST** utilize `gemini-1.5-pro-002` (or `gemini-3-pro-preview` if available).\n",
        "    *   *Why:* We tested Flash; it hallucinates summaries and omits specific IHC stains (e.g., summarizing \"CD45+, S100-\" as just \"ruled out melanoma\"). Only Pro models maintain the high fidelity required for medical RAG.\n",
        "*   **The \"Flash\" Rule:** Extraction and Consolidation (Block 1 & 3) **MUST** use `gemini-3-flash-preview`.\n",
        "    *   *Why:* Pro models have strict Rate Limits (RPM). Using Pro for simple text merging or per-page extraction causes 429 loops and crashes.\n",
        "\n",
        "### 2. Data Integrity Constraints\n",
        "*   **The \"Menu\" Method (PDFs):** Never allow the AI to hallucinate image paths.\n",
        "    *   *Invariant:* PDF Block 2 prompts **MUST** utilize the pre-validated \"Figure Menu\" generated by Block 1. If an image isn't in the menu, it does not exist.\n",
        "*   **The \"Chain of Custody\" (Videos):**\n",
        "    *   *Invariant:* Video Block 1 generates the `gs://` link. Video Block 2 must be instructed to copy that specific link field, not invent a filename.\n",
        "*   **Zero-Loss Merging:** Block 3 (Consolidator) is purely additive.\n",
        "    *   *Invariant:* When merging fragmented entities (e.g., \"Lichen Planus\" from Page 40 and Page 400), the AI must **concatenate** facts, never overwrite or summarize them away.\n",
        "\n",
        "### 3. Operational Limits (The \"Sweet Spots\")\n",
        "*   **Textbook Chunk Size:** `40 pages` (with `2 page overlap`).\n",
        "    *   *Why:* >50 pages triggers HTTP timeouts. <30 pages loses context. 40 is the empirically tested maximum for stability.\n",
        "*   **Pro Concurrency:** `Limit = 2`.\n",
        "    *   *Why:* `gemini-1.5-pro` allows fewer concurrent requests than Flash. Increasing this >2 results in immediate 429 throttling.\n",
        "*   **Flash Concurrency:** `Limit = 15-20`.\n",
        "    *   *Why:* Safe zone for high-throughput image extraction.\n",
        "\n",
        "### 4. File Format Logic\n",
        "*   **Textbooks = PNG:** Lossless quality is required for OCR to correctly read tiny font sizes in medical diagrams.\n",
        "*   **Lectures = JPG:** Compression is required to handle the volume (100+ slides per hour) without exploding storage costs.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Recovery Playbook (If Execution Fails)\n",
        "*   **IF `429 Resource Exhausted`:** Do not restart. The script has auto-resume logic. Wait 60s and re-run.\n",
        "*   **IF `Content-Generation Timeout`:** The Chunk Size is too large for the current model latency. Reduce `PAGES_PER_CHUNK` from 40 to 30.\n",
        "*   **IF `KeyError: 'gcs_content_textbooks'`:** The environment is fresh. Run **Block 0** to re-initialize the `PATHS` map.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>üß¨ Pathology Knowledge Base Pipeline (SOP) (Click to Expand)</strong></summary>\n",
        "\n",
        "**System Version:** v5.0 (High-Fidelity / Monolithic Architecture)\n",
        "**Engine:** Google Gemini (1.5 Pro / 3 Flash)\n",
        "**Infrastructure:** Google Colab $\\leftrightarrow$ Google Cloud Storage (GCS)\n",
        "\n",
        "## üìã Overview\n",
        "This pipeline converts unstructured medical data (Textbooks and Video Lectures) into a strictly standardized, ontology-tagged JSON Knowledge Base. It uses a **\"Monolith\"** approach for reasoning (processing large contexts at once) and a **\"Map-Reduce\"** approach for consolidation.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Block 0: Universal Setup\n",
        "**Status:** ‚úÖ Mandatory (Run once per session)\n",
        "\n",
        "This block installs dependencies, authenticates with Google Cloud, and establishes the global directory map (`PATHS`) to prevent file-not-found errors.\n",
        "\n",
        "*   **Inputs:** None (requires Google Drive mount).\n",
        "*   **Actions:**\n",
        "    *   Installs `PyMuPDF` (PDFs), `openai-whisper` (Audio), `opencv` (Video), `aiohttp` (Async API).\n",
        "    *   Authenticates via Colab Secrets (`GEMINI_API_KEY`).\n",
        "    *   Sets up GCS Bucket paths.\n",
        "*   **Key Variable:** `PATHS` dictionary (Routes data for both Textbooks and Lectures).\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Workflow A: Textbooks (PDF)\n",
        "\n",
        "### Block 1: The Extractor\n",
        "**Goal:** Raw Data Acquisition & Normalization.\n",
        "*   **Model:** `gemini-3-flash-preview` (Speed & Cost).\n",
        "*   **Inputs:** Raw PDF files from Google Drive.\n",
        "*   **Logic:**\n",
        "    1.  **Text:** Cleans OCR errors page-by-page.\n",
        "    2.  **Images:** Extracts images >5KB (saved as **PNG**).\n",
        "    3.  **Panel-Aware Vision:** Detects if an image is \"Figure 2.1 (A)\" vs \"(B)\" and splits captions accordingly.\n",
        "    4.  **Golden Links:** Generates permanent `gs://` links for every image.\n",
        "*   **Outputs:** `_CONTENT.json` (Text), `_FIGURES.json` (Image Metadata).\n",
        "\n",
        "### Block 2: The Architect (High-Fidelity)\n",
        "**Goal:** Medical Reasoning & Schema Enforcement.\n",
        "*   **Model:** `gemini-1.5-pro-002` or `gemini-3-pro-preview` (Deep Reasoning).\n",
        "*   **Inputs:** `_CONTENT.json` + `_FIGURES.json` + `_Tags.txt`.\n",
        "*   **Logic:**\n",
        "    1.  **The Monolith:** Processes **40 pages** in a single context window to capture full disease descriptions.\n",
        "    2.  **The Menu:** Forces AI to pick images from a pre-validated list of `gs://` links (prevents broken links).\n",
        "    3.  **Strict Extraction:** Explicitly instructed to list **stains (CD45+)** and **genetics** without summarizing.\n",
        "    4.  **Safety:** Auto-saves every 5 chunks; resumes if interrupted.\n",
        "*   **Outputs:** `_MASTER.json` (High quality, but potentially fragmented entities).\n",
        "\n",
        "### Block 3: The Consolidator\n",
        "**Goal:** Map-Reduce / De-fragmentation.\n",
        "*   **Model:** `gemini-3-flash-preview` (Logistics & Merging).\n",
        "*   **Inputs:** `_MASTER.json`.\n",
        "*   **Logic:**\n",
        "    1.  **Map:** Groups entries by Tag (e.g., finds 3 separate entries for \"Lichen Planus\" from different chapters).\n",
        "    2.  **Reduce:** Merges text, combines figure lists, and deduplicates data into one Super-Entry.\n",
        "*   **Outputs:** `_CONSOLIDATED.json` (Final Database-Ready File).\n",
        "\n",
        "---\n",
        "\n",
        "## üé• Workflow B: Lectures (Video)\n",
        "\n",
        "### Block 1: The Extractor\n",
        "**Goal:** Audio Transcription & Slide Extraction.\n",
        "*   **Model:** `whisper` (Audio) + `gemini-3-flash-preview` (Visuals).\n",
        "*   **Inputs:** MP4/MOV files from Google Drive.\n",
        "*   **Logic:**\n",
        "    1.  **Audio:** Generates timestamped transcript.\n",
        "    2.  **Visuals:** Extracts frames using **SSIM (Structural Similarity)** to deduplicate static slides (only 1 image per slide change). Saved as **JPG**.\n",
        "    3.  **Analysis:** Vision model extracts text/titles visible on the slide.\n",
        "*   **Outputs:** `_RAW.json` (List of slides with transcripts and GCS paths).\n",
        "\n",
        "### Block 2: The Architect (The Monolith)\n",
        "**Goal:** Synthesis & SOP Compliance.\n",
        "*   **Model:** `gemini-1.5-pro-002` or `gemini-3-pro-preview`.\n",
        "*   **Inputs:** `_RAW.json` + `_Tags.txt`.\n",
        "*   **Logic:**\n",
        "    1.  **Single Shot:** Feeds the **Entire Lecture** (Transcript + All Slide Images) in one massive request.\n",
        "    2.  **Visual Fidelity:** Prompt forces extraction of text labels seen on slides (e.g., \"TTF-1+\", \"CK20+\") rather than just summarizing the diagnosis.\n",
        "    3.  **Schema:** Maps the spoken lecture into the strict 18-field SOP (Clinical, Microscopic, etc.).\n",
        "*   **Outputs:** `_MASTER.json`. *(Note: Lectures rarely need Block 3 consolidation as they usually discuss a topic linearly).*\n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ Data Structure (Google Cloud)\n",
        "\n",
        "```text\n",
        "gs://pathology-hub-0/\n",
        "‚îú‚îÄ‚îÄ Tags/                        # Source of Truth (Ontology)\n",
        "‚îú‚îÄ‚îÄ _asset_library/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ textbooks/\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [Book_Name]/\n",
        "‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ figure_images/   # Saved PNGs (Lossless)\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ lectures/\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ [Video_Name]/        # Saved JPGs (Compressed)\n",
        "‚îî‚îÄ‚îÄ _content_library/\n",
        "    ‚îú‚îÄ‚îÄ textbooks/\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ [Book]_CONTENT.json\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ [Book]_FIGURES.json\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ [Book]_MASTER.json\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ [Book]_CONSOLIDATED.json    # <--- FINAL PDF RESULT\n",
        "    ‚îî‚îÄ‚îÄ lectures/\n",
        "        ‚îú‚îÄ‚îÄ [Video]_RAW.json\n",
        "        ‚îî‚îÄ‚îÄ [Video]_MASTER.json         # <--- FINAL VIDEO RESULT</details>"
      ],
      "metadata": {
        "id": "LWLrYcx3D8at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 0"
      ],
      "metadata": {
        "id": "BpbB--6v4yXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 0: UNIVERSAL SETUP (Textbooks + Lectures)\n",
        "# ==============================================================================\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive, userdata, auth\n",
        "from google.cloud import storage\n",
        "import google.generativeai as genai\n",
        "\n",
        "print(\"--- STEP 0: INITIALIZATION ---\")\n",
        "\n",
        "# 1. Install & Configure System (Textbooks + Whisper/Video tools)\n",
        "print(\"üì¶ Installing dependencies (PDF, Video, AI)...\")\n",
        "!sudo apt-get update -qq && sudo apt-get install -y ffmpeg > /dev/null 2>&1\n",
        "!pip install -q -U google-generativeai PyMuPDF scikit-image aiohttp tqdm openai-whisper opencv-python-headless\n",
        "\n",
        "# 2. Authentication\n",
        "print(\"üîë Authenticating with Google Cloud...\")\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "except Exception as e:\n",
        "    raise SystemExit(f\"‚ùå Authentication Failed: {e}\")\n",
        "\n",
        "# 3. Mount Drive (Source Storage)\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# 4. Universal Configuration\n",
        "GCS_BUCKET_NAME = 'pathology-hub-0'\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/1-Projects/Knowledge_Pipeline'\n",
        "\n",
        "# Initialize GCS Client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "\n",
        "# --- THE MASTER PATH MAP ---\n",
        "# This dictionary handles routing for BOTH workflows.\n",
        "PATHS = {\n",
        "    # --- SOURCES (Local Google Drive) ---\n",
        "    \"source_pdfs\":      os.path.join(DRIVE_ROOT, '_source_materials', 'pdfs'),\n",
        "    \"source_videos\":    os.path.join(DRIVE_ROOT, '_source_materials', 'videos'),\n",
        "\n",
        "    # --- DESTINATIONS (GCS Bucket Paths) ---\n",
        "    \"gcs_bucket\":       GCS_BUCKET_NAME,\n",
        "    \"gcs_tags\":         \"Tags\",  # Where your _Tags.txt files live\n",
        "\n",
        "    # Textbook Pipeline\n",
        "    \"gcs_asset_textbooks\":   \"_asset_library/textbooks\",\n",
        "    \"gcs_content_textbooks\": \"_content_library/textbooks\",\n",
        "\n",
        "    # Lecture Pipeline\n",
        "    \"gcs_asset_lectures\":    \"_asset_library/lectures\",\n",
        "    \"gcs_content_lectures\":  \"_content_library/lectures\"\n",
        "}\n",
        "\n",
        "# 5. Verification\n",
        "print(f\"\\n‚úÖ Connected to Bucket: gs://{GCS_BUCKET_NAME}\")\n",
        "print(f\"‚úÖ Source PDFs:   {PATHS['source_pdfs']}\")\n",
        "print(f\"‚úÖ Source Videos: {PATHS['source_videos']}\")\n",
        "print(\"\\nüöÄ SYSTEM READY. You can now run Block 1 (Textbook) or Block 1 (Lecture).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBl8jRWY41vc",
        "outputId": "a92480b9-f266-48bc-d531-97b938248726"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STEP 0: INITIALIZATION ---\n",
            "üì¶ Installing dependencies (PDF, Video, AI)...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m155.1/155.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "üîë Authenticating with Google Cloud...\n",
            "Mounted at /content/drive\n",
            "\n",
            "‚úÖ Connected to Bucket: gs://pathology-hub-0\n",
            "‚úÖ Source PDFs:   /content/drive/MyDrive/1-Projects/Knowledge_Pipeline/_source_materials/pdfs\n",
            "‚úÖ Source Videos: /content/drive/MyDrive/1-Projects/Knowledge_Pipeline/_source_materials/videos\n",
            "\n",
            "üöÄ SYSTEM READY. You can now run Block 1 (Textbook) or Block 1 (Lecture).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF BLOCK 1: TEXTBOOK EXTRACTOR (Text + Figures)"
      ],
      "metadata": {
        "id": "qp8ILGTF4otI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 1: TEXTBOOK EXTRACTOR (Gemini 3 Flash - Panel Aware)\n",
        "# ==============================================================================\n",
        "import base64\n",
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import os\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from google.cloud import storage\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TEXT_CONCURRENCY = 20\n",
        "VISION_CONCURRENCY = 20\n",
        "TEXT_MODEL_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent?key={GEMINI_API_KEY}\"\n",
        "VISION_MODEL_NAME = \"gemini-3-flash-preview\"\n",
        "\n",
        "# --- HELPER: GCS UTILS ---\n",
        "def gcs_exists(blob_path):\n",
        "    return bucket.blob(blob_path).exists()\n",
        "\n",
        "def gcs_upload_bytes(data, blob_path, content_type):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(data, content_type=content_type)\n",
        "\n",
        "def gcs_upload_json(data, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def gcs_load_json(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    if blob.exists():\n",
        "        return json.loads(blob.download_as_string())\n",
        "    return []\n",
        "\n",
        "# --- AI HELPERS ---\n",
        "async def clean_text_async(session, text, page_num, sem):\n",
        "    async with sem:\n",
        "        if not text.strip(): return page_num, \"\"\n",
        "        prompt = (\n",
        "            \"Clean this medical text. Fix OCR errors. Keep structure. \"\n",
        "            \"Preserve Figure Captions exactly. Return JSON: {\\\"markdown\\\": \\\"...\\\"}\"\n",
        "            f\"\\n\\nRAW TEXT:\\n{text}\"\n",
        "        )\n",
        "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "        try:\n",
        "            async with session.post(TEXT_MODEL_URL, json=payload) as res:\n",
        "                if res.status == 200:\n",
        "                    dat = await res.json()\n",
        "                    raw = dat['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
        "                    if match: return page_num, json.loads(match.group(0)).get(\"markdown\", text)\n",
        "                return page_num, text\n",
        "        except: return page_num, text\n",
        "\n",
        "async def analyze_figure_async(session, b64_img, context, sem):\n",
        "    \"\"\"\n",
        "    Panel-Aware Vision Analysis.\n",
        "    Tries to distinguish if the image is just Panel A or Panel B of a multipart figure.\n",
        "    \"\"\"\n",
        "    async with sem:\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{VISION_MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        PAGE CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        TASK: Analyze the image below.\n",
        "        1. Identify the Figure ID (e.g. \"Fig 2.1\") from the context that matches this image.\n",
        "        2. **MULTI-PANEL CHECK:**\n",
        "           - Does the caption describe multiple parts (e.g. \"(A) ... (B) ...\")?\n",
        "           - If yes, determine if THIS specific image is Panel A, Panel B, etc.\n",
        "           - If this image is ONLY Panel A, try to extract ONLY the caption text for (A).\n",
        "           - If you cannot split the text, return the full caption but add \"(Panel A)\" to the ID.\n",
        "\n",
        "        Return JSON: {{\"figure_id\": \"Fig X.X (Panel A)\", \"matched_caption\": \"Specific caption...\"}} or null.\n",
        "        \"\"\"\n",
        "\n",
        "        parts = [\n",
        "            {\"text\": prompt},\n",
        "            {\"inline_data\": {\"mime_type\": \"image/png\", \"data\": b64_img}}\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            async with session.post(url, json={\"contents\": [{\"parts\": parts}]}) as res:\n",
        "                if res.status == 200:\n",
        "                    dat = await res.json()\n",
        "                    raw = dat['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
        "                    if match: return json.loads(match.group(0))\n",
        "        except: return None\n",
        "        return None\n",
        "\n",
        "# --- MAIN PROCESSOR ---\n",
        "async def process_textbook(pdf_path, start_p=1, end_p=None):\n",
        "    fname = os.path.basename(pdf_path)\n",
        "    book_name = os.path.splitext(fname)[0].replace(' ', '_')\n",
        "\n",
        "    base_asset = f\"{PATHS['gcs_asset_textbooks']}/{book_name}\"\n",
        "    path_fig_imgs = f\"{base_asset}/figure_images\"\n",
        "    path_content = f\"{PATHS['gcs_content_textbooks']}/{book_name}_CONTENT.json\"\n",
        "    path_figures = f\"{PATHS['gcs_content_textbooks']}/{book_name}_FIGURES.json\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\\nüìò PROCESSING: {book_name}\\n{'='*60}\")\n",
        "\n",
        "    doc = fitz.open(pdf_path)\n",
        "    total = len(doc)\n",
        "    final_p = min(end_p or total, total)\n",
        "\n",
        "    # 1. TEXT (Skip if done)\n",
        "    existing_content = gcs_load_json(path_content)\n",
        "    if not existing_content:\n",
        "        print(f\"üìù Phase 1: Cleaning Text...\")\n",
        "        sem = asyncio.Semaphore(TEXT_CONCURRENCY)\n",
        "        async with aiohttp.ClientSession() as sess:\n",
        "            tasks = [clean_text_async(sess, doc.load_page(p).get_text(\"text\"), p+1, sem) for p in range(start_p-1, final_p)]\n",
        "            results = await tqdm_asyncio.gather(*tasks)\n",
        "        content_data = sorted([{\"source\": fname, \"page_number\": p, \"content\": t} for p, t in results], key=lambda x: x['page_number'])\n",
        "        gcs_upload_json(content_data, path_content)\n",
        "    else:\n",
        "        content_data = existing_content\n",
        "\n",
        "    content_map = {c['page_number']: c['content'] for c in content_data}\n",
        "\n",
        "    # 2. FIGURES\n",
        "    print(\"üñºÔ∏è Phase 2: Figures & Vision (Panel-Aware)...\")\n",
        "    existing_figs = gcs_load_json(path_figures)\n",
        "    processed_pages = {f['source_page'] for f in existing_figs}\n",
        "    vision_tasks = []\n",
        "    new_figures = []\n",
        "    sem_vis = asyncio.Semaphore(VISION_CONCURRENCY)\n",
        "\n",
        "    for p_idx in range(start_p-1, final_p):\n",
        "        p_num = p_idx + 1\n",
        "        if p_num in processed_pages: continue\n",
        "\n",
        "        page = doc.load_page(p_idx)\n",
        "        images = page.get_images(full=True)\n",
        "        if not images: continue\n",
        "\n",
        "        md_ctx = content_map.get(p_num, \"\")\n",
        "\n",
        "        for i, img in enumerate(images):\n",
        "            try:\n",
        "                xref = img[0]\n",
        "                base = doc.extract_image(xref)\n",
        "                if len(base[\"image\"]) < 5000: continue\n",
        "\n",
        "                img_name = f\"{book_name}_page_{p_num}_img_{i+1}.{base['ext']}\"\n",
        "                blob_path = f\"{path_fig_imgs}/{img_name}\"\n",
        "                full_uri = f\"gs://{GCS_BUCKET_NAME}/{blob_path}\"\n",
        "\n",
        "                if not gcs_exists(blob_path):\n",
        "                    gcs_upload_bytes(base[\"image\"], blob_path, f\"image/{base['ext']}\")\n",
        "\n",
        "                b64 = base64.b64encode(base[\"image\"]).decode('utf-8')\n",
        "                vision_tasks.append({\n",
        "                    \"b64\": b64, \"ctx\": md_ctx,\n",
        "                    \"meta\": {\"source_page\": p_num, \"gcs_path\": full_uri}\n",
        "                })\n",
        "            except: pass\n",
        "\n",
        "    if vision_tasks:\n",
        "        print(f\"   -> Analyzing {len(vision_tasks)} figures...\")\n",
        "        async with aiohttp.ClientSession() as sess:\n",
        "            tasks = [analyze_figure_async(sess, t['b64'], t['ctx'], sem_vis) for t in vision_tasks]\n",
        "            results = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "            for i, res in enumerate(results):\n",
        "                if res and res.get('figure_id'):\n",
        "                    meta = vision_tasks[i]['meta']\n",
        "                    new_figures.append({\n",
        "                        \"source_document\": fname,\n",
        "                        \"source_page\": meta['source_page'],\n",
        "                        \"figure_id\": res['figure_id'],\n",
        "                        \"description\": res['matched_caption'],\n",
        "                        \"gcs_path\": meta['gcs_path']\n",
        "                    })\n",
        "\n",
        "        final_list = existing_figs + new_figures\n",
        "        final_list.sort(key=lambda x: x['source_page'])\n",
        "        gcs_upload_json(final_list, path_figures)\n",
        "        print(f\"   -> Added {len(new_figures)} figures.\")\n",
        "\n",
        "# --- RUNNER ---\n",
        "async def main():\n",
        "    pdfs = sorted([f for f in os.listdir(PATHS['source_pdfs']) if f.endswith('.pdf')])\n",
        "    if not pdfs: print(\"‚ùå No PDFs found.\"); return\n",
        "\n",
        "    print(\"\\n--- AVAILABLE TEXTBOOKS ---\")\n",
        "    for i, f in enumerate(pdfs): print(f\"[{i+1}] {f}\")\n",
        "\n",
        "    sel = input(\"\\nSelect book(s) (e.g. 1, 3): \")\n",
        "    indices = [int(x)-1 for x in sel.split(',') if x.strip().isdigit()]\n",
        "\n",
        "    for idx in indices:\n",
        "        if 0 <= idx < len(pdfs):\n",
        "            await process_textbook(os.path.join(PATHS['source_pdfs'], pdfs[idx]))\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CF5BNCNP4qlL",
        "outputId": "50ebe300-f4e7-4905-91a1-6b0e210b8c8c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Configured Google AI Studio API with Key.\n",
            "üìÇ Reading local files...\n",
            "üöÄ Processing 210 entities using gemini-3-flash-preview (API Key)...\n",
            "   [Squamous cell carcinoma with s] -> Skin::Neoplastic::Epidermal::Malignant::Squamous_Cell_Carcinoma_Spindle_Cell\n",
            "   [Spindle cell squamous cell car] -> Skin::Neoplastic::Epidermal::Malignant::Squamous_Cell_Carcinoma_Spindle_Cell\n",
            "   [Clear cell squamous cell carci] -> Skin::Neoplastic::Epidermal::Malignant::Squamous_Cell_Carcinoma_Invasive_NOS\n",
            "   [Lymphoepithelial carcinoma    ] -> Skin::Neoplastic::Epidermal::Malignant::Lymphoepithelioma_Like_Carcinoma\n",
            "   [Acantholytic squamous cell car] -> Skin::Neoplastic::Epidermal::Malignant::Squamous_Cell_Carcinoma_Acantholytic\n",
            "   [Verrucous squamous cell carcin] -> Skin::Neoplastic::Epidermal::Malignant::Squamous_Cell_Carcinoma_Verrucous\n",
            "   [Squamous cell carcinomas      ] -> Skin::Neoplastic::Epidermal::Malignant::Squamous_Cell_Carcinoma_Invasive_NOS\n",
            "   [Fibroepithelial basal cell car] -> Skin::Neoplastic::Epidermal::Malignant::Fibroepithelioma_Of_Pinkus\n",
            "   [Basal cell carcinoma with adne] -> Skin::Neoplastic::Epidermal::Malignant::Basal_Cell_Carcinoma_Overview_NOS\n",
            "   [Basal cell carcinoma with sarc] -> Skin::Neoplastic::Epidermal::Malignant::Basal_Cell_Carcinoma_Overview_NOS\n",
            "   [Basosquamous carcinoma        ] -> Skin::Neoplastic::Epidermal::Malignant::Basosquamous_Carcinoma\n",
            "   [Sclerosing/morphoeic basal cel] -> Skin::Neoplastic::Epidermal::Malignant::Basal_Cell_Carcinoma_Morpheaform\n",
            "   [Infiltrating basal cell carcin] -> Skin::Neoplastic::Epidermal::Malignant::Basal_Cell_Carcinoma_Infiltrative\n",
            "   [Micronodular basal cell carcin] -> Skin::Neoplastic::Epidermal::Malignant::Basal_Cell_Carcinoma_Micronodular\n",
            "   [Nodular basal cell carcinoma  ] -> Skin::Neoplastic::Epidermal::Malignant::Basal_Cell_Carcinoma_Nodular\n",
            "   [Superficial basal cell carcino] -> Skin::Neoplastic::Epidermal::Malignant::Basal_Cell_Carcinoma_Superficial\n",
            "   [Basal cell carcinomas         ] -> Skin::Neoplastic::Epidermal::Malignant::Basal_Cell_Carcinoma_Overview_NOS\n",
            "   [Melanoma metastatic to the ski] -> Skin::Neoplastic::Melanocytic::Malignant::Melanoma_Metastatic_To_Skin\n",
            "   [Dermal melanoma               ] -> Skin::Neoplastic::Melanocytic::Malignant::Melanoma_Primary_Dermal\n",
            "   [Naevoid melanoma              ] -> Skin::Neoplastic::Melanocytic::Malignant::Melanoma_Nevoid\n",
            "   [Nodular melanoma              ] -> Skin::Neoplastic::Melanocytic::Malignant::Melanoma_Nodular\n",
            "   [Diffuse meningeal melanocytic ] -> Skin::Neoplastic::Melanocytic::Benign::Dermal_Melanocytoses_Overview\n",
            "   [Diffuse meningeal melanocytic ] -> Skin::Neoplastic::Melanocytic::Benign::Dermal_Melanocytoses_Overview\n",
            "   [Uveal melanocytoma            ] -> Skin::Neoplastic::Melanocytic::Benign::Common_Acquired_Nevus_NOS\n",
            "   [Conjunctival melanoma         ] -> Skin::Neoplastic::Melanocytic::Malignant::Melanoma_Conjunctival\n",
            "   [Conjunctival naevus           ] -> Skin::Neoplastic::Melanocytic::Benign::Common_Acquired_Nevus_NOS\n",
            "   [Melanoma arising in giant cong] -> Skin::Neoplastic::Melanocytic::Malignant::Melanoma_Invasive_Overview_NOS\n",
            "   [Proliferative nodules in conge] -> Skin::Neoplastic::Melanocytic::Benign::Congenital_Nevus\n",
            "   [Congenital melanocytic naevus ] -> Skin::Neoplastic::Melanocytic::Benign::Congenital_Nevus\n",
            "   [Melanoma arising in blue naevu] -> Skin::Neoplastic::Melanocytic::Malignant::Blue_Nevus_Malignant_Cellular\n",
            "   [Congenital dermal melanocytosi] -> Skin::Neoplastic::Melanocytic::Benign::Mongolian_Spot\n",
            "   [Naevus of Ito and naevus of Ot] -> Skin::Neoplastic::Melanocytic::Benign::Nevus_of_Ota_Ito\n",
            "   [Mucosal melanoma              ] -> Skin::Neoplastic::Melanocytic::Malignant::Melanoma_Mucosal_Genital\n",
            "   [Genital naevus                ] -> Skin::Neoplastic::Melanocytic::Benign::Special_Site_Nevus\n",
            "   [Melanosis                     ] -> Skin::Neoplastic::Melanocytic::Benign::Melanotic_Macule_Labial_Oral\n",
            "   [Acral naevus                  ] -> Skin::Neoplastic::Melanocytic::Benign::Acral_Nevus\n",
            "   [Spitz melanoma                ] -> Skin::Neoplastic::Melanocytic::Malignant::Melanoma_Spitzoid\n",
            "   [Spitz melanocytoma (atypical S] -> Skin::Neoplastic::Melanocytic::InterMedia::Atypical_Spitz_Tumor\n",
            "   [Pigmented spindle cell naevus ] -> Skin::Neoplastic::Melanocytic::Benign::Pigmented_Spindle_Cell_Nevus_Reed\n",
            "   [MITF pathway-activated melanoc] -> Skin::Neoplastic::Melanocytic::InterMedia::Melanocytoma_PEM\n",
            "   [BAP1-inactivated melanocytoma ] -> Skin::Neoplastic::Melanocytic::Benign::BAP1_Inactivated_Melanocytoma\n",
            "   [Pigmented epithelioid melanocy] -> Skin::Neoplastic::Melanocytic::InterMedia::Melanocytoma_PEM\n",
            "   [Combined naevus               ] -> Skin::Neoplastic::Melanocytic::Benign::Combined_Nevus\n",
            "   [Recurrent naevus              ] -> Skin::Neoplastic::Melanocytic::Benign::Recurrent_Nevus_Pseudomelanoma\n",
            "   [Meyerson naevus               ] -> Skin::Neoplastic::Melanocytic::Benign::Meyerson_Nevus\n",
            "   [Halo naevus                   ] -> Skin::Neoplastic::Melanocytic::Benign::Halo_Nevus\n",
            "   [Special-site naevi (of the bre] -> Skin::Neoplastic::Melanocytic::Benign::Special_Site_Nevus\n",
            "   [Naevus spilus                 ] -> Skin::Neoplastic::Melanocytic::Benign::Common_Acquired_Nevus_NOS\n",
            "   [Simple lentigo and lentiginous] -> Skin::Neoplastic::Melanocytic::Benign::Lentigo_Simplex\n",
            "   [Adenocarcinoma of mammary glan] -> Skin::Neoplastic::Adnexal::Apocrine::Apocrine_Carcinoma\n",
            "   [Extramammary Paget disease    ] -> Skin::Neoplastic::Adnexal::Paget::Extramammary_Paget_Disease\n",
            "   [Mammary Paget disease         ] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Fibroadenoma and phyllodes tum] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Hidradenoma papilliferum      ] -> Skin::Neoplastic::Adnexal::Apocrine::Hidradenoma_Papilliferum\n",
            "   [Sebaceous carcinoma           ] -> Skin::Neoplastic::Adnexal::Sebaceous::Sebaceous_Carcinoma\n",
            "   [Sebaceous adenoma             ] -> Skin::Neoplastic::Adnexal::Sebaceous::Sebaceous_Adenoma\n",
            "   [Sebaceoma                     ] -> Skin::Neoplastic::Adnexal::Sebaceous::Sebaceoma\n",
            "   [Trichilemmal carcinoma        ] -> Skin::Neoplastic::Adnexal::Follicular::Trichilemmal_Carcinoma\n",
            "   [Trichoblastic carcinoma/carcin] -> Skin::Neoplastic::Adnexal::Follicular::Trichoblastoma_and_Trichoblastic_Carcinoma\n",
            "   [Pilomatrical carcinoma        ] -> Skin::Neoplastic::Adnexal::Follicular::Pilomatrical_Carcinoma\n",
            "   [Proliferating trichilemmal tum] -> Skin::Congenital_Structural::Cyst::Proliferating_Pilar_Cyst\n",
            "   [Trichodiscoma and fibrofollicu] -> Skin::Neoplastic::Adnexal::Follicular::Fibrofolliculoma_and_Trichodiscoma\n",
            "   [Tumour of the follicular infun] -> Skin::Neoplastic::Adnexal::Follicular::Tumor_Of_Follicular_Infundibulum\n",
            "   [Pilar sheath acanthoma        ] -> Skin::Neoplastic::Adnexal::Follicular::Pilar_Sheath_Acanthoma\n",
            "   [Trichofolliculoma             ] -> Skin::Neoplastic::Adnexal::Follicular::Trichofolliculoma\n",
            "   [Trichoadenoma                 ] -> Skin::Neoplastic::Adnexal::Follicular::Trichoadenoma\n",
            "   [Trichilemmoma                 ] -> Skin::Neoplastic::Adnexal::Follicular::Trichilemmoma\n",
            "   [Melanocytic matricoma         ] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Pilomatricoma                 ] -> Skin::Neoplastic::Adnexal::Follicular::Pilomatricoma\n",
            "   [Signet-ring cell / histiocytoi] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Secretory carcinoma           ] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Syringocystadenocarcinoma papi] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Squamoid eccrine ductal carcin] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Apocrine carcinoma            ] -> Skin::Neoplastic::Adnexal::Apocrine::Apocrine_Carcinoma\n",
            "   [Adenoid cystic carcinoma      ] -> Skin::Neoplastic::Adnexal::Eccrine::Adenoid_Cystic_Carcinoma_Cutaneous\n",
            "   [Digital papillary adenocarcino] -> Skin::Neoplastic::Adnexal::Eccrine::Digital_Papillary_Adenocarcinoma\n",
            "   [Mucinous carcinoma            ] -> Skin::Neoplastic::Adnexal::Eccrine::Mucinous_Carcinoma_Primary_Cutaneous\n",
            "   [Endocrine mucin-producing swea] -> Skin::Neoplastic::Adnexal::Eccrine::Endocrine_Mucin_Producing_Sweat_Gland_Carcinoma\n",
            "   [Hidradenocarcinoma            ] -> Skin::Neoplastic::Adnexal::Eccrine::Hidradenocarcinoma\n",
            "   [Malignant mixed tumour        ] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Malignant neoplasms arising fr] -> Skin::Neoplastic::Adnexal::Eccrine::Spiradenocarcinoma\n",
            "   [NUT carcinoma                 ] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Porocarcinoma                 ] -> Skin::Neoplastic::Adnexal::Eccrine::Porocarcinoma\n",
            "   [Cribriform tumour (previously ] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Microcystic adnexal carcinoma ] -> Skin::Neoplastic::Adnexal::Eccrine::Microcystic_Adnexal_Carcinoma\n",
            "   [Adnexal adenocarcinoma NOS    ] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Myoepithelial tumour          ] -> Skin::Neoplastic::Uncertain::Myoepithelioma_of_Soft_Tissue\n",
            "   [Mixed tumour                  ] -> Skin::Neoplastic::Adnexal::Eccrine::Mixed_Tumor_Chondroid_Syringoma\n",
            "   [Syringocystadenoma papilliferu] -> Skin::Neoplastic::Adnexal::Apocrine::Syringocystadenoma_Papilliferum\n",
            "   [Tubular adenoma               ] -> Skin::Neoplastic::Adnexal::Apocrine::Tubular_Apocrine_Adenoma\n",
            "   [Cylindroma                    ] -> Skin::Neoplastic::Adnexal::Eccrine::Cylindroma\n",
            "   [Spiradenoma                   ] -> Skin::Neoplastic::Adnexal::Eccrine::Spiradenoma\n",
            "   [Hidradenoma                   ] -> Skin::Neoplastic::Adnexal::Eccrine::Nodular_Hidradenoma\n",
            "   [Syringofibroadenoma           ] -> Skin::Neoplastic::Adnexal::Eccrine::Syringofibroadenoma_of_Mascaro\n",
            "   [Poroma                        ] -> Skin::Neoplastic::Adnexal::Eccrine::Poroma_Eccrine\n",
            "   [Syringoma                     ] -> Skin::Neoplastic::Adnexal::Eccrine::Syringoma\n",
            "   [Hidrocystoma/cystadenoma      ] -> Skin::Congenital_Structural::Cyst::Hidrocystoma_Eccrine_Apocrine\n",
            "   [Subungual keratoacanthoma     ] -> Skin::Neoplastic::Epidermal::Malignant::Keratoacanthoma\n",
            "   [Onychocytic matricoma         ] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Ungual fibrokeratoma          ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Acquired_Digital_Fibrokeratoma\n",
            "   [Onychopapilloma               ] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [Onychomatricoma               ] -> Skin::Neoplastic::Adnexal::Adnexal_Neoplasm_Overview_NOS\n",
            "   [EBV-positive mucocutaneous ulc] -> Skin::Neoplastic::Hematolymphoid::Cutaneous_B_Cell_Lymphoma\n",
            "   [T-lymphoblastic leukaemia/lymp] -> Skin::Neoplastic::Hematolymphoid::Leukemia_Cutis\n",
            "   [T-prolymphocytic leukaemia    ] -> Skin::Neoplastic::Hematolymphoid::Leukemia_Cutis\n",
            "   [Anaplastic large cell lymphoma] -> Skin::Neoplastic::Hematolymphoid::Anaplastic_Large_Cell_Lymphoma_Cutaneous\n",
            "   [Severe mosquito bite allergy  ] -> Skin::Neoplastic::Hematolymphoid::Systemic_T_Cell_and_NK_Cell_Lymphomas\n",
            "   [Primary cutaneous peripheral T] -> Skin::Neoplastic::Hematolymphoid::Cutaneous_Lymphoma_Overview_NOS\n",
            "   [Primary cutaneous CD8-positive] -> Skin::Neoplastic::Hematolymphoid::Aggressive_Epidermotropic_CD8_Positive_T_Cell_Lymphoma\n",
            "   [Primary cutaneous gamma/delta ] -> Skin::Neoplastic::Hematolymphoid::Cutaneous_Gamma_Delta_T_Cell_Lymphoma\n",
            "   [Subcutaneous panniculitis-like] -> Skin::Neoplastic::Hematolymphoid::Subcutaneous_Panniculitis_Like_T_Cell_Lymphoma\n",
            "   [Primary cutaneous CD30-positiv] -> Skin::Neoplastic::Hematolymphoid::Primary_Cutaneous_Anaplastic_Large_Cell_Lymphoma\n",
            "   [Primary cutaneous CD4-positive] -> Skin::Neoplastic::Hematolymphoid::Primary_Cutaneous_CD4_Positive_Small_Medium_T_Cell_Lymphoproliferative_Disorder\n",
            "   [B-lymphoblastic leukaemia/lymp] -> Skin::Neoplastic::Hematolymphoid::Leukemia_Cutis\n",
            "   [Chronic lymphocytic leukaemia ] -> Skin::Neoplastic::Hematolymphoid::Leukemia_Cutis\n",
            "   [Burkitt lymphoma              ] -> Skin::Neoplastic::Hematolymphoid::Cutaneous_B_Cell_Lymphoma\n",
            "   [Mantle cell lymphoma          ] -> Skin::Neoplastic::Hematolymphoid::Cutaneous_B_Cell_Lymphoma\n",
            "   [Intravascular large B-cell lym] -> Skin::Neoplastic::Hematolymphoid::Intravascular_Large_B_Cell_Lymphoma\n",
            "   [Reactive B-cell-rich lymphoid ] -> Skin::Neoplastic::Hematolymphoid::Pseudolymphoma_Cutaneous_Lymphoid_Hyperplasia\n",
            "   [Indeterminate dendritic cell t] -> Skin::Neoplastic::Hematolymphoid::Indeterminate_Cell_Histiocytosis\n",
            "   [Histiocytic sarcoma           ] -> Skin::Neoplastic::Hematolymphoid::Histiocytic_Sarcoma\n",
            "   [ALK-positive histiocytosis    ] -> Skin::Neoplastic::Hematolymphoid::Non_Langerhans_Cell_Histiocytoses\n",
            "   [Erdheim-Chester disease       ] -> Skin::Neoplastic::Hematolymphoid::Non_Langerhans_Cell_Histiocytoses\n",
            "   [Xanthelasma                   ] -> Skin::Depositional_Metabolic::Xanthoma::Xanthelasma\n",
            "   [Blastic plasmacytoid dendritic] -> Skin::Neoplastic::Hematolymphoid::Blastic_Plasmacytoid_Dendritic_Cell_Neoplasm\n",
            "   [Mature plasmacytoid dendritic ] -> Skin::Neoplastic::Hematolymphoid::Leukemia_Cutis\n",
            "   [Mast cell sarcoma             ] -> Skin::Neoplastic::Hematolymphoid::Mastocytosis_Urticaria_Pigmentosa\n",
            "   [Muir-Torre syndrome           ] -> Skin::Congenital_Structural::Genodermatosis::Genodermatosis_Overview_NOS\n",
            "   [Carney complex                ] -> Skin::Congenital_Structural::Genodermatosis::Genodermatosis_Overview_NOS\n",
            "   [Naevoid basal cell carcinoma s] -> Skin::Congenital_Structural::Genodermatosis::Genodermatosis_Overview_NOS\n",
            "   [Metastases to skin            ] -> Skin::Neoplastic::Metastatic::Metastatic_Skin_Tumors_Overview\n",
            "   [Ewing sarcoma                 ] -> Skin::Neoplastic::Uncertain::Ewing_Sarcoma_Cutaneous\n",
            "   [Dermal clear cell sarcoma     ] -> Skin::Neoplastic::Uncertain::Clear_Cell_Sarcoma_Soft_Tissue\n",
            "   [CRTC1::TRIM11 cutaneous tumour] -> Skin::Neoplastic::Uncertain::Distinctive_Dermal_Clear_Cell_Mesenchymal_Tumor\n",
            "   [Epithelioid sarcoma           ] -> Skin::Neoplastic::Uncertain::Epithelioid_Sarcoma\n",
            "   [Pleomorphic dermal sarcoma    ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Pleomorphic_Dermal_Sarcoma\n",
            "   [Superficial CD34-positive fibr] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Pseudosarcomatous_Fibroblastic_Proliferations\n",
            "   [Atypical fibroxanthoma        ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Atypical_Fibroxanthoma\n",
            "   [NTRK-rearranged spindle cell n] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Pseudosarcomatous_Fibroblastic_Proliferations\n",
            "   [Angiomatoid fibrous histiocyto] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Pseudosarcomatous_Fibroblastic_Proliferations\n",
            "   [PEComa                        ] -> Skin::Neoplastic::Uncertain::PEComa\n",
            "   [Non-neural granular cell tumou] -> Skin::Neoplastic::Soft_Tissue::Neural::Granular_Cell_Tumor\n",
            "   [Epithelioid fibrous histiocyto] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Epithelioid_Cell_Histiocytoma\n",
            "   [Cellular neurothekeoma        ] -> Skin::Neoplastic::Uncertain::Cellular_Neurothekeoma\n",
            "   [Malignant peripheral nerve she] -> Skin::Neoplastic::Soft_Tissue::Neural::Malignant_Peripheral_Nerve_Sheath_Tumor\n",
            "   [Hybrid nerve sheath tumours   ] -> Skin::Neoplastic::Soft_Tissue::Neural::Neuromas_Overview\n",
            "   [Schwannoma                    ] -> Skin::Neoplastic::Soft_Tissue::Neural::Schwannoma\n",
            "   [Neurofibroma                  ] -> Skin::Neoplastic::Soft_Tissue::Neural::Neurofibroma\n",
            "   [Perineurioma                  ] -> Skin::Neoplastic::Soft_Tissue::Neural::Perineurioma\n",
            "   [Dermal nerve sheath myxoma    ] -> Skin::Neoplastic::Soft_Tissue::Neural::Dermal_Nerve_Sheath_Myxoma\n",
            "   [Solitary circumscribed neuroma] -> Skin::Neoplastic::Soft_Tissue::Neural::Palisaded_Encapsulated_Neuroma\n",
            "   [Dermal hyperneury / epithelial] -> Skin::Neoplastic::Soft_Tissue::Neural::Neuromas_Overview\n",
            "   [Atypical intradermal smooth mu] -> Skin::Neoplastic::Soft_Tissue::Muscular::Leiomyosarcoma_Cutaneous\n",
            "   [Cutaneous leiomyomas          ] -> Skin::Neoplastic::Soft_Tissue::Muscular::Cutaneous_Leiomyoma_Pilar\n",
            "   [Smooth muscle hamartoma       ] -> Skin::Neoplastic::Soft_Tissue::Muscular::Smooth_Muscle_Hamartoma\n",
            "   [Angioleiomyoma                ] -> Skin::Neoplastic::Soft_Tissue::Muscular::Angioleiomyoma\n",
            "   [Myofibroma and myofibromatosis] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Myofibroma_and_Myofibromatosis\n",
            "   [Myopericytoma                 ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Myopericytoma\n",
            "   [Glomus tumour                 ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Glomus_Tumor\n",
            "   [Composite haemangioendotheliom] -> Skin::Neoplastic::Soft_Tissue::Vascular::Composite_Hemangioendothelioma\n",
            "   [Hobnail haemangioendotheliomas] -> Skin::Neoplastic::Soft_Tissue::Vascular::Dabska_Tumor_Papillary_Intralymphatic_Angioendothelioma\n",
            "   [Epithelioid haemangioendotheli] -> Skin::Neoplastic::Soft_Tissue::Vascular::Epithelioid_Hemangioendothelioma\n",
            "   [Pseudomyogenic haemangioendoth] -> Skin::Neoplastic::Soft_Tissue::Vascular::Angiosarcoma_Cutaneous\n",
            "   [Postradiation atypical vascula] -> Skin::Neoplastic::Soft_Tissue::Vascular::Atypical_Vascular_Lesion\n",
            "   [Cutaneous epithelioid angiomat] -> Skin::Neoplastic::Soft_Tissue::Vascular::Epithelioid_Hemangioma\n",
            "   [Arteriovenous malformation    ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Arteriovenous_Malformation\n",
            "   [Verrucous venous malformation ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Verrucous_Hemangioma\n",
            "   [Acquired elastotic haemangioma] -> Skin::Neoplastic::Soft_Tissue::Vascular::Acquired_Elastotic_Hemangioma\n",
            "   [Poikilodermatous plaque-like h] -> Skin::Neoplastic::Soft_Tissue::Vascular::Vascular_Malformations\n",
            "   [Lobular capillary haemangioma ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Lobular_Capillary_Hemangioma_Pyogenic_Granuloma\n",
            "   [Congenital non-progressive hae] -> Skin::Neoplastic::Soft_Tissue::Vascular::Infantile_Juvenile_Hemangioma\n",
            "   [Infantile haemangioma         ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Infantile_Juvenile_Hemangioma\n",
            "   [Angiokeratoma                 ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Angiokeratoma\n",
            "   [Tufted haemangioma            ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Tufted_Hemangioma\n",
            "   [Epithelioid haemangioma       ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Epithelioid_Hemangioma\n",
            "   [Spindle cell haemangioma      ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Spindle_Cell_Hemangioma\n",
            "   [Papillary haemangioma         ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Vascular_Malformations\n",
            "   [Glomeruloid haemangioma       ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Glomeruloid_Hemangioma\n",
            "   [Hobnail haemangioma           ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Hobnail_Hemangioma\n",
            "   [Microvenular haemangioma      ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Microvenular_Hemangioma\n",
            "   [Sinusoidal haemangioma        ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Sinusoidal_Hemangioma\n",
            "   [Cherry haemangioma            ] -> Skin::Neoplastic::Soft_Tissue::Vascular::Cherry_Hemangioma\n",
            "   [Myxofibrosarcoma              ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Myxofibrosarcoma\n",
            "   [Myxoinflammatory fibroblastic ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Myxoinflammatory_Fibroblastic_Sarcoma\n",
            "   [EWSR1::SMAD3-rearranged fibrob] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Pseudosarcomatous_Fibroblastic_Proliferations\n",
            "   [Nodular fasciitis             ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Nodular_Fasciitis\n",
            "   [Plaque-like CD34-positive derm] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Dermatomyofibroma\n",
            "   [Multinucleate cell angiohistio] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Multinucleate_Cell_Angiohistiocytoma\n",
            "   [Dermatomyofibroma             ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Dermatomyofibroma\n",
            "   [Cutaneous myxoma (superficial ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Cutaneous_Myxoma_and_Angiomyxoma\n",
            "   [Superficial acral fibromyxoma ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Superficial_Acral_Fibromyxoma\n",
            "   [Plexiform fibrohistiocytic tum] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Plexiform_Fibrohistiocytic_Tumor\n",
            "   [Inclusion body fibromatosis   ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Digital_Fibromatosis_Infantile\n",
            "   [Superficial fibromatosis      ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Superficial_Fibromatosis\n",
            "   [Fibro-osseous tumour of digits] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Pseudosarcomatous_Fibroblastic_Proliferations\n",
            "   [Fibroblastic connective tissue] -> Skin::Congenital_Structural::Hamartoma::Connective_Tissue_Nevus_Collagenoma\n",
            "   [Fibrous papule                ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Angiofibroma_Fibrous_Papule\n",
            "   [Desmoplastic fibroblastoma    ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Desmoplastic_Fibroblastoma\n",
            "   [Elastofibroma                 ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Elastofibroma_Dorsi\n",
            "   [Pleomorphic fibroma           ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Pleomorphic_Fibroma\n",
            "   [Gardner fibroma               ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Gardner_Fibroma\n",
            "   [Nuchal-type fibroma           ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Nuchal_Type_Fibroma\n",
            "   [Calcifying aponeurotic fibroma] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Fibromatosis\n",
            "   [Sclerotic fibroma             ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Sclerotic_Fibroma\n",
            "   [Fibroma of tendon sheath      ] -> Skin::Neoplastic::Soft_Tissue::Fibrous::Fibroma_of_Tendon_Sheath\n",
            "   [Atypical lipomatous tumour    ] -> Skin::Neoplastic::Soft_Tissue::Adipocytic::Atypical_Lipomatous_Tumor\n",
            "   [Pleomorphic liposarcoma       ] -> Skin::Neoplastic::Soft_Tissue::Adipocytic::Pleomorphic_Liposarcoma\n",
            "   [Spindle cell / pleomorphic lip] -> Skin::Neoplastic::Soft_Tissue::Adipocytic::Spindle_Cell_Lipoma\n",
            "   [Lipoma                        ] -> Skin::Neoplastic::Soft_Tissue::Adipocytic::Lipoma\n",
            "   [Naevus lipomatosus superficial] -> Skin::Neoplastic::Soft_Tissue::Adipocytic::Nevus_Lipomatosus_Superficialis\n",
            "\n",
            "üíæ Success! Saved to: SKIN_Cleaned_Gemini3.json\n",
            "   (Download this file from the left sidebar)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF BLOCK 2: TEXTBOOK ARCHITECT (High-Fidelity Monolith)\n"
      ],
      "metadata": {
        "id": "k0P6QtEh3vos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy6oq1WV3s_g",
        "outputId": "e6cb6b27-fbea-43c5-c806-ac4723a78e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SELECT TAG LIST ---\n",
            "[1] BST_Tags.txt\n",
            "[2] Breast_Tags.txt\n",
            "[3] Endo_Tags.txt\n",
            "[4] GI_Tags.txt\n",
            "[5] GYN_Tags.txt\n",
            "[6] Skin_Tags.txt\n",
            "Choice: 6\n",
            "\n",
            "--- SELECT TEXTBOOK ---\n",
            "[1] BST_Horvai_CONTENT.json\n",
            "[2] Bone_Atlas_CONTENT.json\n",
            "[3] Bone_Dorfman_CONTENT.json\n",
            "[4] Bone_Pattern_CONTENT.json\n",
            "[5] Breast_Atlas_CONTENT.json\n",
            "[6] Breast_Biopsy_CONTENT.json\n",
            "[7] Breast_FAQ_CONTENT.json\n",
            "[8] Breast_Pattern_CONTENT.json\n",
            "[9] Cyto_Breast_Yokohama_CONTENT.json\n",
            "[10] Cyto_Cibas_CONTENT.json\n",
            "[11] Cyto_Comprehensive_Part_One_CONTENT.json\n",
            "[12] Cyto_Comprehensive_Part_Two_CONTENT.json\n",
            "[13] Cyto_GU_Paris_CONTENT.json\n",
            "[14] Cyto_Gyn_Bethesda_CONTENT.json\n",
            "[15] Cyto_Milan_CONTENT.json\n",
            "[16] Cyto_PSC_Lung_CONTENT.json\n",
            "[17] Cyto_Pattern_CONTENT.json\n",
            "[18] Cyto_Serous_Fluids_CONTENT.json\n",
            "[19] Cyto_Thyroid_Bethesda_CONTENT.json\n",
            "[20] Derm_Barnhill_CONTENT.json\n",
            "[21] Derm_Elston_CONTENT.json\n",
            "[22] Derm_Levers_CONTENT.json\n",
            "[23] Derm_McKeeHY_CONTENT.json\n",
            "[24] Derm_McKee_CONTENT.json\n",
            "[25] Derm_McKee_High_Yield_CONTENT.json\n",
            "[26] Derm_Patterson_CONTENT.json\n",
            "[27] Derm_Weedon_CONTENT.json\n",
            "[28] Endo_Atlas_CONTENT.json\n",
            "[29] GI_Atlas_CONTENT.json\n",
            "[30] GI_Biopsy_Interpretation_(Neoplastic)_CONTENT.json\n",
            "[31] GI_Biopsy_Interpretation_(Non_Neoplastic)_CONTENT.json\n",
            "[32] GI_Intestinal_Atlas1_CONTENT.json\n",
            "[33] GI_Liver_Macsween_CONTENT.json\n",
            "[34] GI_Non-Neoplastic_Zhang_CONTENT.json\n",
            "[35] GU_Biopsy_Interpretation_(Prostate)_CONTENT.json\n",
            "[36] Gyn_Atlas_Part_One_CONTENT.json\n",
            "[37] Gyn_Atlas_Part_Two_CONTENT.json\n",
            "[38] Gyn_Essentials_CONTENT.json\n",
            "[39] HN_Thompson_CONTENT.json\n",
            "[40] Peds_Course_review_CONTENT.json\n",
            "[41] Skin_Elston_CONTENT.json\n",
            "[42] Skin_Levers_CONTENT.json\n",
            "[43] SoftTissue_Enzinger_CONTENT.json\n",
            "[44] SoftTissue_Pattern_CONTENT.json\n",
            "Choice: 23\n",
            "\n",
            "üöÄ Processing: Derm_McKeeHY\n",
            "   Model: gemini-3-pro-preview (High Fidelity)\n",
            "\n",
            "‚ö†Ô∏è Existing MASTER file found for Derm_McKeeHY.\n",
            "Type 'RESUME' to continue or 'RESTART' to overwrite (Recommended if 0 entities): //asc5\n",
            "   -> Starting fresh. Overwriting previous file.\n",
            "üì¶ Total Chunks: 13 (~50 pages each)\n",
            "\n",
            "--- Processing Batch 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 2: TEXTBOOK ARCHITECT (FINAL ROBUST VERSION)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import difflib\n",
        "import random\n",
        "import os\n",
        "from typing import List, Dict, Set, Any\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# We use 1.5 Pro because it is currently the most stable \"Deep Reasoning\" model for large context.\n",
        "# You can swap this to 'gemini-3-pro-preview' if you have specific access.\n",
        "MODEL_NAME = \"gemini-3-pro-preview\"\n",
        "CONCURRENCY_LIMIT = 2           # Low concurrency is required for Pro models (Rate Limits)\n",
        "PAGES_PER_CHUNK = 50            # Process 50 pages at once (The \"Super-Chunk\")\n",
        "PAGE_OVERLAP = 2                # Context overlap\n",
        "MAX_RETRIES = 10                # Aggressive retries for 429 errors\n",
        "BATCH_SIZE = 5                  # Save to disk every 5 chunks\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_read_text(blob_path: str) -> str:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return blob.download_as_string().decode('utf-8') if blob.exists() else \"\"\n",
        "\n",
        "def gcs_load_json(blob_path: str) -> List:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data: Any, blob_path: str):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def validate_tag(tag_input: Any, valid_set: Set[str]) -> str:\n",
        "    \"\"\"Safely validates tags, handling None/Empty/Lists.\"\"\"\n",
        "    # 1. Handle non-string inputs (e.g. None or list)\n",
        "    if not tag_input:\n",
        "        return \"Skin::Unclassified\"\n",
        "\n",
        "    if isinstance(tag_input, list):\n",
        "        tag_str = tag_input[0] if len(tag_input) > 0 else \"Skin::Unclassified\"\n",
        "    else:\n",
        "        tag_str = str(tag_input)\n",
        "\n",
        "    clean = tag_str.strip()\n",
        "\n",
        "    # 2. Exact Match\n",
        "    if clean in valid_set:\n",
        "        return clean\n",
        "\n",
        "    # 3. Fuzzy Match\n",
        "    matches = difflib.get_close_matches(clean, list(valid_set), n=1, cutoff=0.7)\n",
        "    return matches[0] if matches else clean\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. PROMPT ENGINEERING\n",
        "# ------------------------------------------------------------------------------\n",
        "def construct_textbook_prompt(text_content, figure_context, valid_tags_list):\n",
        "    return f\"\"\"\n",
        "Role: You are a Senior Dermatopathologist and Data Engineer.\n",
        "Objective: Convert the provided TEXTBOOK CONTENT (Text + Figures) into a standardized Knowledge Base.\n",
        "\n",
        "INPUT CONTEXT:\n",
        "This input represents a ~{PAGES_PER_CHUNK} page section of a medical textbook.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. **Identify Entities:** Extract every distinct disease/pathology entity discussed.\n",
        "   - Ignore \"Preface\", \"Contributors\", \"Index\", or generic \"Introduction\" unless they contain specific disease data.\n",
        "2. **High-Fidelity Extraction (CRITICAL):**\n",
        "   - **Microscopic:** Do not summarize. Extract specific architectural features (e.g., \"saw-toothing\", \"max-joseph spaces\") and cytological features.\n",
        "   - **Ancillary Studies:** You MUST list every specific stain mentioned (e.g., \"CD45+\", \"S100-\", \"CK20 dot-like\"). If genetics are mentioned (e.g., \"t(11;22)\"), include them.\n",
        "3. **Figure Linking:**\n",
        "   - Link relevant figures from the provided list to the correct entity.\n",
        "   - Use the `gcs_path` provided.\n",
        "\n",
        "REQUIRED JSON SCHEMA (List of Objects):\n",
        "[\n",
        "  {{\n",
        "    \"entity_name\": \"Disease Name\",\n",
        "    \"definition\": \"...\",\n",
        "    \"tags\": [\"Exact_Tag_From_List\"],\n",
        "    \"html_gcs_path\": null,\n",
        "\n",
        "    \"localization\": \"...\",\n",
        "    \"clinical\": \"...\",\n",
        "    \"pathogenesis\": \"...\",\n",
        "    \"macroscopic\": \"...\",\n",
        "    \"microscopic\": \"Detailed histology...\",\n",
        "    \"cytology\": \"...\",\n",
        "    \"ancillary_studies\": \"List specific IHC stains and molecular findings.\",\n",
        "    \"differential_diagnosis\": \"...\",\n",
        "    \"staging\": \"...\",\n",
        "    \"prognosis_and_prediction\": \"...\",\n",
        "\n",
        "    \"related_figures\": [\n",
        "        {{\n",
        "            \"id\": \"Use the ID from input\",\n",
        "            \"src\": \"gs://...\",\n",
        "            \"gcs_path\": \"gs://...\",\n",
        "            \"diagnosis\": \"Disease Name\",\n",
        "            \"legend\": \"Full caption from text + visual description.\"\n",
        "        }}\n",
        "    ]\n",
        "  }}\n",
        "]\n",
        "\n",
        "REFERENCE TAGS:\n",
        "{valid_tags_list}\n",
        "\n",
        "--- FIGURES AVAILABLE IN THIS SECTION ---\n",
        "{figure_context}\n",
        "\n",
        "--- TEXTBOOK CONTENT ---\n",
        "{text_content}\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. CHUNK PROCESSOR\n",
        "# ------------------------------------------------------------------------------\n",
        "async def process_textbook_chunk(session, chunk_data, figures_in_chunk, valid_tags_text, valid_tags_set, sem):\n",
        "    async with sem:\n",
        "        full_text = \"\\n\\n\".join([f\"--- Page {p['page_number']} ---\\n{p['content']}\" for p in chunk_data])\n",
        "\n",
        "        fig_desc_list = []\n",
        "        for f in figures_in_chunk:\n",
        "            fig_desc_list.append(\n",
        "                f\"ID: {f.get('figure_id', 'Unknown')}\\n\"\n",
        "                f\"Page: {f['source_page']}\\n\"\n",
        "                f\"GCS Path: {f['gcs_path']}\\n\"\n",
        "                f\"Caption: {f.get('description', '')}\\n\"\n",
        "            )\n",
        "        fig_context = \"\\n\".join(fig_desc_list)\n",
        "\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "        payload = {\"contents\": [{\"parts\": [{\"text\": construct_textbook_prompt(full_text, fig_context, valid_tags_text)}]}]}\n",
        "\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                # Long timeout for Pro models reading large chunks\n",
        "                async with session.post(url, json=payload, timeout=600) as response:\n",
        "                    if response.status == 200:\n",
        "                        data = await response.json()\n",
        "                        raw_txt = data['candidates'][0]['content']['parts'][0]['text']\n",
        "                        raw_txt = raw_txt.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "                        match = re.search(r'\\[.*\\]', raw_txt, re.DOTALL)\n",
        "                        if match:\n",
        "                            entities = json.loads(match.group(0))\n",
        "                            valid_entities = []\n",
        "                            for ent in entities:\n",
        "                                if not ent.get('entity_name'): continue\n",
        "\n",
        "                                # Robust Tag Validation\n",
        "                                raw_tags = ent.get('tags', [])\n",
        "                                if isinstance(raw_tags, str): raw_tags = [raw_tags] # Handle AI returning string instead of list\n",
        "                                ent['tags'] = [validate_tag(t, valid_tags_set) for t in raw_tags]\n",
        "\n",
        "                                # Enforce Nulls for missing keys\n",
        "                                required_keys = [\n",
        "                                    \"clinical\", \"microscopic\", \"ancillary_studies\",\n",
        "                                    \"differential_diagnosis\", \"pathogenesis\", \"staging\",\n",
        "                                    \"prognosis_and_prediction\", \"cytology\", \"macroscopic\"\n",
        "                                ]\n",
        "                                for k in required_keys:\n",
        "                                    if k not in ent: ent[k] = None\n",
        "                                ent['html_gcs_path'] = None\n",
        "\n",
        "                                valid_entities.append(ent)\n",
        "                            return valid_entities\n",
        "                        return []\n",
        "                    elif response.status == 429:\n",
        "                        wait = (2 ** attempt) + random.uniform(5, 15)\n",
        "                        print(f\"  ‚è≥ Rate Limit (Chunk {chunk_data[0]['page_number']})... Waiting {wait:.1f}s\")\n",
        "                        await asyncio.sleep(wait)\n",
        "                        continue\n",
        "                    else:\n",
        "                        print(f\"‚ùå Error {response.status} on chunk starting page {chunk_data[0]['page_number']}\")\n",
        "                        return []\n",
        "            except Exception as e:\n",
        "                # print(f\"‚ùå Exception: {e}\")\n",
        "                await asyncio.sleep(15)\n",
        "        return []\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MAIN WORKFLOW\n",
        "# ------------------------------------------------------------------------------\n",
        "async def main_textbook_robust():\n",
        "    # 1. Select Tags\n",
        "    tag_files = [b.name for b in bucket.list_blobs(prefix=\"Tags/\") if b.name.endswith('.txt')]\n",
        "    print(\"\\n--- SELECT TAG LIST ---\")\n",
        "    for i, f in enumerate(tag_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    t_idx = int(input(\"Choice: \")) - 1\n",
        "    tags_text = gcs_read_text(tag_files[t_idx])\n",
        "    tags_set = set(l.strip() for l in tags_text.splitlines() if l.strip())\n",
        "\n",
        "    # 2. Select Textbook\n",
        "    content_files = [b.name for b in bucket.list_blobs(prefix=PATHS['gcs_content_textbooks']) if \"_CONTENT.json\" in b.name]\n",
        "    if not content_files: print(\"‚ùå No CONTENT files found.\"); return\n",
        "\n",
        "    print(\"\\n--- SELECT TEXTBOOK ---\")\n",
        "    for i, f in enumerate(content_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    c_idx = int(input(\"Choice: \")) - 1\n",
        "\n",
        "    content_path = content_files[c_idx]\n",
        "    book_base = content_path.split('/')[-1].replace(\"_CONTENT.json\", \"\")\n",
        "    fig_path = content_path.replace(\"_CONTENT.json\", \"_FIGURES.json\")\n",
        "    final_path = f\"{PATHS['gcs_content_textbooks']}/{book_base}_MASTER.json\"\n",
        "\n",
        "    print(f\"\\nüöÄ Processing: {book_base}\")\n",
        "    print(f\"   Model: {MODEL_NAME} (High Fidelity)\")\n",
        "\n",
        "    # 3. Resume vs Restart Logic\n",
        "    master_kb = []\n",
        "    if bucket.blob(final_path).exists():\n",
        "        print(f\"\\n‚ö†Ô∏è Existing MASTER file found for {book_base}.\")\n",
        "        choice = input(\"Type 'RESUME' to continue or 'RESTART' to overwrite (Recommended if 0 entities): \").strip().upper()\n",
        "        if choice == 'RESUME':\n",
        "            master_kb = gcs_load_json(final_path)\n",
        "            print(f\"   -> Resuming with {len(master_kb)} existing entities.\")\n",
        "        else:\n",
        "            print(\"   -> Starting fresh. Overwriting previous file.\")\n",
        "            master_kb = []\n",
        "\n",
        "    # 4. Load Data & Chunk\n",
        "    raw_content = gcs_load_json(content_path)\n",
        "    raw_figures = gcs_load_json(fig_path)\n",
        "    raw_content.sort(key=lambda x: x['page_number'])\n",
        "\n",
        "    chunks = []\n",
        "    total_pages = len(raw_content)\n",
        "    for i in range(0, total_pages, PAGES_PER_CHUNK):\n",
        "        end_idx = min(i + PAGES_PER_CHUNK + PAGE_OVERLAP, total_pages)\n",
        "        chunks.append(raw_content[i : end_idx])\n",
        "\n",
        "    print(f\"üì¶ Total Chunks: {len(chunks)} (~{PAGES_PER_CHUNK} pages each)\")\n",
        "\n",
        "    # 5. Process Batches\n",
        "    # Logic: If resuming, calculate where we left off based on entity count (approx)\n",
        "    # Actually, simpler to just re-process if we suspect issues, OR skip chunks based on index.\n",
        "    # For now, let's just process. The Dedupe handles overlaps.\n",
        "\n",
        "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
        "\n",
        "    for i in range(0, len(chunks), BATCH_SIZE):\n",
        "        batch_chunks = chunks[i : i + BATCH_SIZE]\n",
        "        print(f\"\\n--- Processing Batch {i//BATCH_SIZE + 1}/{(len(chunks)//BATCH_SIZE) + 1} ---\")\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = []\n",
        "            for chunk in batch_chunks:\n",
        "                page_nums = {p['page_number'] for p in chunk}\n",
        "                chunk_figs = [f for f in raw_figures if f['source_page'] in page_nums]\n",
        "                tasks.append(process_textbook_chunk(session, chunk, chunk_figs, tags_text, tags_set, sem))\n",
        "\n",
        "            results = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "            new_in_batch = 0\n",
        "            for res_list in results:\n",
        "                master_kb.extend(res_list)\n",
        "                new_in_batch += len(res_list)\n",
        "\n",
        "            # Incremental Save\n",
        "            if new_in_batch > 0:\n",
        "                gcs_upload_json(master_kb, final_path)\n",
        "                print(f\"üíæ Saved progress... (+{new_in_batch} entities)\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è No entities found in this batch (or API error). Continuing...\")\n",
        "\n",
        "    # 6. Final Deduplication (Crash-Proof)\n",
        "    print(\"\\nüßπ Final Deduplication...\")\n",
        "    unique_kb = []\n",
        "    seen_keys = set()\n",
        "    for ent in master_kb:\n",
        "        # Robust Key Gen: Handle None definition\n",
        "        def_text = ent.get('definition')\n",
        "        if def_text is None: def_text = \"\"\n",
        "\n",
        "        key = (ent.get('entity_name'), def_text[:50])\n",
        "\n",
        "        if key not in seen_keys:\n",
        "            unique_kb.append(ent)\n",
        "            seen_keys.add(key)\n",
        "\n",
        "    gcs_upload_json(unique_kb, final_path)\n",
        "    print(f\"\\n‚úÖ COMPLETE: gs://{GCS_BUCKET_NAME}/{final_path}\")\n",
        "    print(f\"üìä Final Count: {len(unique_kb)} Entities\")\n",
        "\n",
        "await main_textbook_robust()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 2.5: THE TAG REFINER (Crash-Proof Version)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import difflib\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gemini-3-flash-preview\"\n",
        "CONCURRENCY_LIMIT = 30\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_read_text(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return blob.download_as_string().decode('utf-8') if blob.exists() else \"\"\n",
        "\n",
        "def gcs_load_json(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "# --- AI WORKER ---\n",
        "async def retag_entity(session, entity, valid_tags_text, valid_tags_set, sem):\n",
        "    # 1. Safe Tag Retrieval (The Fix)\n",
        "    current_tags = entity.get('tags')\n",
        "    if current_tags and isinstance(current_tags, list) and len(current_tags) > 0:\n",
        "        current_tag = current_tags[0]\n",
        "    else:\n",
        "        current_tag = \"Unknown/Unclassified\"\n",
        "\n",
        "    # 2. Quick Check: Is it already valid?\n",
        "    if current_tag in valid_tags_set:\n",
        "        return entity\n",
        "\n",
        "    # 3. AI Fix\n",
        "    async with sem:\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Role: Pathology Taxonomist.\n",
        "        Task: Map this entity to the BEST matching tag from the Authorized List.\n",
        "\n",
        "        ENTITY: {entity.get('entity_name', 'Unknown')}\n",
        "        DEFINITION: {str(entity.get('definition', ''))[:300]}\n",
        "        CURRENT TAG: {current_tag}\n",
        "\n",
        "        AUTHORIZED LIST:\n",
        "        {valid_tags_text}\n",
        "\n",
        "        INSTRUCTION: Return ONLY the exact string from the Authorized List.\n",
        "        \"\"\"\n",
        "\n",
        "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "\n",
        "        try:\n",
        "            async with session.post(url, json=payload) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "                    new_tag = data['candidates'][0]['content']['parts'][0]['text'].strip()\n",
        "\n",
        "                    # Fuzzy validation\n",
        "                    if new_tag in valid_tags_set:\n",
        "                        entity['tags'] = [new_tag]\n",
        "                    else:\n",
        "                        # Try fuzzy match if AI made a typo\n",
        "                        matches = difflib.get_close_matches(new_tag, list(valid_tags_set), n=1, cutoff=0.7)\n",
        "                        if matches:\n",
        "                            entity['tags'] = [matches[0]]\n",
        "                        # If still no match, keep original (or Unknown)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return entity\n",
        "\n",
        "# --- MAIN ---\n",
        "async def main_retagger():\n",
        "    # 1. Select Tags\n",
        "    tag_files = [b.name for b in bucket.list_blobs(prefix=\"Tags/\") if b.name.endswith('.txt')]\n",
        "    print(\"\\n--- SELECT NEW TAG LIST ---\")\n",
        "    for i, f in enumerate(tag_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    t_idx = int(input(\"Choice: \")) - 1\n",
        "    tags_text = gcs_read_text(tag_files[t_idx])\n",
        "    tags_set = set(l.strip() for l in tags_text.splitlines() if l.strip())\n",
        "\n",
        "    # 2. Select Master File\n",
        "    content_files = [b.name for b in bucket.list_blobs(prefix=PATHS['gcs_content_textbooks']) if \"_MASTER.json\" in b.name]\n",
        "    print(\"\\n--- SELECT MASTER FILE TO UPDATE ---\")\n",
        "    for i, f in enumerate(content_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    c_idx = int(input(\"Choice: \")) - 1\n",
        "\n",
        "    master_path = content_files[c_idx]\n",
        "    entities = gcs_load_json(master_path)\n",
        "    print(f\"\\nüöÄ Refining Tags for {len(entities)} entities...\")\n",
        "\n",
        "    # 3. Process\n",
        "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
        "    updated_entities = []\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [retag_entity(session, ent, tags_text, tags_set, sem) for ent in entities]\n",
        "        updated_entities = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "    # 4. Save\n",
        "    gcs_upload_json(updated_entities, master_path)\n",
        "    print(f\"\\n‚úÖ UPDATED: {master_path}\")\n",
        "    print(\"   All entities have been re-aligned to the tag list.\")\n",
        "\n",
        "await main_retagger()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2G0Hk-xBK5b",
        "outputId": "22ceba3b-397a-4705-babb-a75dc6512d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SELECT NEW TAG LIST ---\n",
            "[1] BST_Tags.txt\n",
            "[2] Breast_Tags.txt\n",
            "[3] Endo_Tags.txt\n",
            "[4] GI_Tags.txt\n",
            "[5] GYN_Tags.txt\n",
            "[6] Skin_Tags.txt\n",
            "Choice: 6\n",
            "\n",
            "--- SELECT MASTER FILE TO UPDATE ---\n",
            "[1] Derm_Elston_MASTER.json\n",
            "[2] Derm_McKee_MASTER.json\n",
            "[3] Derm_Patterson_MASTER.json\n",
            "[4] Skin_Elston_MASTER.json\n",
            "Choice: 1\n",
            "\n",
            "üöÄ Refining Tags for 1078 entities...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1078/1078 [01:31<00:00, 11.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ UPDATED: _content_library/textbooks/Derm_Elston_MASTER.json\n",
            "   All entities have been re-aligned to the tag list.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF BLOCK 2.1: THE CONSOLIDATOR (Map-Reduce Merge)"
      ],
      "metadata": {
        "id": "GKMkiyqh4CM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 3: THE CONSOLIDATOR (Map-Reduce Merge)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Any  # <--- Added missing imports\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Flash is perfect for merging text. It is fast and respects the data.\n",
        "MODEL_NAME = \"gemini-3-flash-preview\"\n",
        "CONCURRENCY_LIMIT = 15\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_load_json(blob_path: str) -> List:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data: Any, blob_path: str):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "# --- PROMPT ---\n",
        "def construct_merge_prompt(entity_name, fragments):\n",
        "    return f\"\"\"\n",
        "Role: Medical Data Editor.\n",
        "Task: Merge these fragmented records for \"{entity_name}\" into ONE comprehensive entry.\n",
        "\n",
        "INPUT FRAGMENTS (from different chapters):\n",
        "{json.dumps(fragments, indent=2)}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. **Consolidate Text:** Combine 'clinical', 'microscopic', 'definition', etc. Do not lose details.\n",
        "   - If Fragment A has \"Clinical: Itchy\" and Fragment B has \"Clinical: Purple papules\", the result must be \"Itchy, purple papules.\"\n",
        "   - **Crucial:** Preserve all specific stains (CD45+, S100) and genetic findings.\n",
        "2. **Merge Figures:** Combine all 'related_figures' into one list. Remove duplicates if exact same ID.\n",
        "3. **Preserve Tags:** Use the most specific tag available.\n",
        "4. **Output:** A single JSON object.\n",
        "\n",
        "REQUIRED SCHEMA:\n",
        "{{\n",
        "    \"entity_name\": \"{entity_name}\",\n",
        "    \"definition\": \"Merged...\",\n",
        "    \"tags\": [\"...\"],\n",
        "    \"html_gcs_path\": null,\n",
        "    \"clinical\": \"...\",\n",
        "    \"pathogenesis\": \"...\",\n",
        "    \"macroscopic\": \"...\",\n",
        "    \"microscopic\": \"...\",\n",
        "    \"cytology\": \"...\",\n",
        "    \"ancillary_studies\": \"...\",\n",
        "    \"differential_diagnosis\": \"...\",\n",
        "    \"staging\": \"...\",\n",
        "    \"prognosis_and_prediction\": \"...\",\n",
        "    \"related_figures\": [...]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# --- AI WORKER ---\n",
        "async def merge_entity_group(session, tag, group, sem):\n",
        "    async with sem:\n",
        "        # If only 1 entry, no merge needed\n",
        "        if len(group) == 1:\n",
        "            return group[0]\n",
        "\n",
        "        # Construct Prompt\n",
        "        entity_name = group[0]['entity_name']\n",
        "        prompt = construct_merge_prompt(entity_name, group)\n",
        "\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "\n",
        "        try:\n",
        "            async with session.post(url, json=payload) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "                    raw = data['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
        "                    if match:\n",
        "                        return json.loads(match.group(0))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Fallback: Just return the first one if AI fails (prevents data loss)\n",
        "        print(f\"‚ö†Ô∏è Merge failed for {entity_name}, keeping fragments.\")\n",
        "        return group[0]\n",
        "\n",
        "# --- MAIN ---\n",
        "async def main_consolidator():\n",
        "    # 1. Select Content\n",
        "    content_files = [b.name for b in bucket.list_blobs(prefix=PATHS['gcs_content_textbooks']) if \"_MASTER.json\" in b.name and \"_CONSOLIDATED\" not in b.name]\n",
        "    if not content_files: print(\"‚ùå No MASTER files found. Run Block 2 first.\"); return\n",
        "\n",
        "    print(\"\\n--- SELECT MASTER FILE TO CONSOLIDATE ---\")\n",
        "    for i, f in enumerate(content_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    c_idx = int(input(\"Choice: \")) - 1\n",
        "\n",
        "    master_path = content_files[c_idx]\n",
        "    raw_entities = gcs_load_json(master_path)\n",
        "\n",
        "    print(f\"\\nüöÄ Consolidating {len(raw_entities)} entities...\")\n",
        "\n",
        "    # 2. Group by Tag (or Name if Tag is generic)\n",
        "    groups = defaultdict(list)\n",
        "    for ent in raw_entities:\n",
        "        # Key strategy: Use the first Tag as the primary key.\n",
        "        # If tag is missing/generic, fallback to Entity Name.\n",
        "        tag_list = ent.get('tags', [])\n",
        "        key = tag_list[0] if tag_list else ent.get('entity_name', 'Unknown')\n",
        "        groups[key].append(ent)\n",
        "\n",
        "    print(f\"   -> Found {len(groups)} unique topics (Tags/Names).\")\n",
        "\n",
        "    # 3. Process Groups\n",
        "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
        "    final_kb = []\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for key, group in groups.items():\n",
        "            tasks.append(merge_entity_group(session, key, group, sem))\n",
        "\n",
        "        results = await tqdm_asyncio.gather(*tasks)\n",
        "        final_kb = results\n",
        "\n",
        "    # 4. Save\n",
        "    out_path = master_path.replace(\"_MASTER.json\", \"_CONSOLIDATED.json\")\n",
        "    gcs_upload_json(final_kb, out_path)\n",
        "    print(f\"\\n‚úÖ CONSOLIDATED MASTER SAVED: gs://{GCS_BUCKET_NAME}/{out_path}\")\n",
        "    print(f\"üìä Reduced {len(raw_entities)} fragments -> {len(final_kb)} unique entities.\")\n",
        "\n",
        "await main_consolidator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZAVfDpP3_DI",
        "outputId": "0a95ff22-e4a5-4761-e02c-86955eb08eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SELECT MASTER FILE TO CONSOLIDATE ---\n",
            "[1] Derm_Elston_MASTER.json\n",
            "[2] Derm_Levers_MASTER.json\n",
            "[3] Derm_McKee_MASTER.json\n",
            "[4] Derm_Patterson_MASTER.json\n",
            "[5] Skin_Elston_MASTER.json\n",
            "Choice: 2\n",
            "\n",
            "üöÄ Consolidating 494 entities...\n",
            "   -> Found 349 unique topics (Tags/Names).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 349/349 [02:06<00:00,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ CONSOLIDATED MASTER SAVED: gs://pathology-hub-0/_content_library/textbooks/Derm_Levers_CONSOLIDATED.json\n",
            "üìä Reduced 494 fragments -> 349 unique entities.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIDEO"
      ],
      "metadata": {
        "id": "HVCDbcXT5MqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 1: LECTURE EXTRACTOR (Whisper + Gemini 3 Flash)"
      ],
      "metadata": {
        "id": "vpmaF2wAsch_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 1: LECTURE EXTRACTOR (Whisper + Gemini 3 Flash)\n",
        "# ==============================================================================\n",
        "import shutil, cv2, whisper, json, os, io, base64, re, asyncio, aiohttp\n",
        "import logging\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from google.cloud import storage\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "API_CONCURRENCY_LIMIT = 20\n",
        "VISION_MODEL = \"gemini-3-pro-preview\" # Fast & Cheap for per-slide analysis\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_upload_file(local_path, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_filename(local_path)\n",
        "\n",
        "def gcs_upload_json(data, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def gcs_exists(blob_path):\n",
        "    return bucket.blob(blob_path).exists()\n",
        "\n",
        "def get_comparison_frame(frame):\n",
        "    h, w = frame.shape[:2]\n",
        "    new_w = 200\n",
        "    new_h = int(h * (new_w / w))\n",
        "    small = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "    gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)\n",
        "    return cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# --- AI ANALYST ---\n",
        "async def analyze_slide_async(session, slide_data, local_img_path, sem):\n",
        "    async with sem:\n",
        "        if not os.path.exists(local_img_path): return slide_data\n",
        "\n",
        "        try:\n",
        "            # Prepare Image\n",
        "            with Image.open(local_img_path) as img:\n",
        "                buf = io.BytesIO()\n",
        "                img.convert(\"RGB\").save(buf, format=\"JPEG\")\n",
        "                b64_img = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "            url = f\"https://generativelanguage.googleapis.com/v1beta/models/{VISION_MODEL}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "            # Prompt: Extract raw visual data. We don't need deep reasoning yet, just \"What is on this slide?\"\n",
        "            prompt = (\n",
        "                f\"Transcript Context: \\\"{slide_data['raw_transcript'][:1000]}...\\\"\\n\\n\"\n",
        "                \"TASK: Analyze this slide image. \\n\"\n",
        "                \"1. Extract the Title.\\n\"\n",
        "                \"2. Extract text labels verbatim (e.g. 'CD45', 'H&E', '40x').\\n\"\n",
        "                \"3. Summarize the visual content (e.g., 'Histology showing...').\\n\"\n",
        "                \"Return JSON: {\\\"slide_title\\\": \\\"...\\\", \\\"key_points\\\": [\\\"...\\\"], \\\"visual_desc\\\": \\\"...\\\"}\"\n",
        "            )\n",
        "\n",
        "            payload = {\"contents\": [{\"parts\": [{\"text\": prompt}, {\"inline_data\": {\"mime_type\": \"image/jpeg\", \"data\": b64_img}}]}]}\n",
        "\n",
        "            async with session.post(url, json=payload) as res:\n",
        "                if res.status == 200:\n",
        "                    dat = await res.json()\n",
        "                    txt = dat['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', txt, re.DOTALL)\n",
        "                    if match:\n",
        "                        slide_data.update(json.loads(match.group(0)))\n",
        "        except Exception as e:\n",
        "            pass # Skip frame if AI fails\n",
        "\n",
        "        return slide_data\n",
        "\n",
        "# --- PIPELINE ---\n",
        "async def process_video(video_path, counter, total):\n",
        "    fname = os.path.basename(video_path)\n",
        "    lecture_name = os.path.splitext(fname)[0].replace(\" \", \"_\")\n",
        "\n",
        "    # GCS Paths\n",
        "    asset_base = f\"{PATHS['gcs_asset_lectures']}/{lecture_name}\"\n",
        "    raw_json_path = f\"{PATHS['gcs_content_lectures']}/{lecture_name}_RAW.json\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\\nüé• PROCESSING {counter}/{total}: {lecture_name}\\n{'='*60}\")\n",
        "\n",
        "    if gcs_exists(raw_json_path):\n",
        "        print(\"‚úÖ Already processed in GCS. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # 1. WHISPER TRANSCRIPTION\n",
        "    print(\"üéôÔ∏è Step 1: Whisper Transcription...\")\n",
        "    model = whisper.load_model(\"base\") # Use 'small' if you have GPU RAM, 'base' is fast\n",
        "    result = model.transcribe(video_path, fp16=False)\n",
        "\n",
        "    # 2. FRAME EXTRACTION & MERGING\n",
        "    print(\"üéûÔ∏è Step 2: Extracting Slides...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    slides = []\n",
        "    curr_slide = None\n",
        "    prev_cmp = None\n",
        "\n",
        "    # We use TQDM to track progress through the audio segments\n",
        "    for seg in tqdm(result['segments'], desc=\"Scanning\", unit=\"seg\"):\n",
        "        cap.set(cv2.CAP_PROP_POS_MSEC, seg['start'] * 1000)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: continue\n",
        "\n",
        "        curr_cmp = get_comparison_frame(frame)\n",
        "\n",
        "        if curr_slide is None:\n",
        "            curr_slide = {**seg, 'frame': frame}\n",
        "            prev_cmp = curr_cmp\n",
        "            continue\n",
        "\n",
        "        # SSIM Check (Merge if > 85% similar)\n",
        "        if ssim(prev_cmp, curr_cmp, data_range=255) >= 0.85:\n",
        "            curr_slide['text'] += \" \" + seg['text']\n",
        "            curr_slide['end'] = seg['end']\n",
        "        else:\n",
        "            slides.append(curr_slide)\n",
        "            curr_slide = {**seg, 'frame': frame}\n",
        "            prev_cmp = curr_cmp\n",
        "\n",
        "    if curr_slide: slides.append(curr_slide)\n",
        "    cap.release()\n",
        "    print(f\"   -> Consolidated into {len(slides)} unique slides.\")\n",
        "\n",
        "    # 3. UPLOAD & PREPARE\n",
        "    print(\"‚òÅÔ∏è Step 3: Uploading Images...\")\n",
        "    final_data = []\n",
        "    local_imgs = {} # Map id -> local path for AI step\n",
        "\n",
        "    for i, slide in enumerate(slides):\n",
        "        img_name = f\"{lecture_name}_slide_{i+1:04d}.jpg\"\n",
        "        local_p = f\"/tmp/{img_name}\"\n",
        "        gcs_p = f\"{asset_base}/{img_name}\"\n",
        "        full_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_p}\"\n",
        "\n",
        "        cv2.imwrite(local_p, slide['frame'])\n",
        "\n",
        "        if not gcs_exists(gcs_p):\n",
        "            gcs_upload_file(local_p, gcs_p)\n",
        "\n",
        "        local_imgs[i] = local_p\n",
        "\n",
        "        final_data.append({\n",
        "            \"id\": i,\n",
        "            \"timestamp_start\": slide['start'],\n",
        "            \"timestamp_end\": slide['end'],\n",
        "            \"raw_transcript\": slide['text'].strip(),\n",
        "            \"image_path\": full_uri,\n",
        "            \"gcs_path\": full_uri, # Important for Block 2\n",
        "            \"slide_title\": \"\",\n",
        "            \"key_points\": [],\n",
        "            \"visual_desc\": \"\"\n",
        "        })\n",
        "\n",
        "    # 4. GEMINI ENHANCEMENT\n",
        "    print(\"üß† Step 4: Gemini Vision Analysis...\")\n",
        "    sem = asyncio.Semaphore(API_CONCURRENCY_LIMIT)\n",
        "    async with aiohttp.ClientSession() as sess:\n",
        "        tasks = [analyze_slide_async(sess, d, local_imgs[d['id']], sem) for d in final_data]\n",
        "        enhanced_data = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "    # 5. SAVE RAW JSON\n",
        "    gcs_upload_json(enhanced_data, raw_json_path)\n",
        "    print(f\"‚úÖ Saved RAW data: {raw_json_path}\")\n",
        "\n",
        "    # Cleanup\n",
        "    for p in local_imgs.values():\n",
        "        if os.path.exists(p): os.remove(p)\n",
        "\n",
        "# --- RUNNER ---\n",
        "async def main_lectures():\n",
        "    vid_files = sorted([f for f in os.listdir(PATHS['source_videos']) if f.lower().endswith(('.mp4', '.mov'))])\n",
        "    if not vid_files: print(\"‚ùå No videos found.\"); return\n",
        "\n",
        "    print(\"\\n--- AVAILABLE LECTURES ---\")\n",
        "    for i, v in enumerate(vid_files): print(f\"[{i+1}] {v}\")\n",
        "\n",
        "    sel = input(\"\\nSelect (e.g. 1, 3-5, or 'all'): \")\n",
        "    indices = set()\n",
        "    if sel == 'all': indices = range(len(vid_files))\n",
        "    else:\n",
        "        for part in sel.split(','):\n",
        "            if '-' in part:\n",
        "                s, e = map(int, part.split('-'))\n",
        "                indices.update(range(s-1, e))\n",
        "            elif part.strip().isdigit():\n",
        "                indices.add(int(part)-1)\n",
        "\n",
        "    for idx in sorted(list(indices)):\n",
        "        if 0 <= idx < len(vid_files):\n",
        "            await process_video(os.path.join(PATHS['source_videos'], vid_files[idx]), idx+1, len(indices))\n",
        "\n",
        "await main_lectures()"
      ],
      "metadata": {
        "id": "ZzGRiY1z5XBT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d7703daafeaf4527bb55ec4de96224a3",
            "b24646f3a73d445ba3f8fb4ad8744247",
            "98080b61b681460e9fe17756f9539293",
            "40ba9e21a64c41539d8ea79b68b473e8",
            "a4b541244fdc4c468b259a268f629e99",
            "689e7919a4b34958b26f3c39ad914ddc",
            "9c57adc0432540119c53a463628f45f7",
            "4bbfdfd9791040bf8b929c59d51be14f",
            "aebd159cdba4421b9c648f6d9f395fa8",
            "76df98f6ede24a7b8f33b1ca3311c2aa",
            "450cfa75048e4a8cb60c64731cdab258"
          ]
        },
        "outputId": "8d377b19-723a-4086-f770-6851001ba4d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- AVAILABLE LECTURES ---\n",
            "[1] BST_Lecture_1_Grossing.mp4\n",
            "[2] BST_Lecture_2_SoftTissue1.mp4\n",
            "[3] BST_Lecture_3_SofTissue2.mp4\n",
            "[4] BST_Lecture_4_SoftTissue3.mp4\n",
            "[5] BST_Lecture_5_Bone1.mp4\n",
            "[6] BST_Lecture_6_Bone2.mp4\n",
            "[7] Breast_Lecture_Epithelial Part 1_Chen.mp4\n",
            "[8] Breast_Lecture_Fibroepithelial.mp4\n",
            "[9] Breast_Lecture_Grossing.mp4\n",
            "[10] Breast_Lecture_IHC.mp4\n",
            "[11] Breast_Lecture_Invasive.mp4\n",
            "[12] Breast_Lecture_Lobular.mp4\n",
            "[13] Breast_Lecture_Normal.mp4\n",
            "[14] Breast_Lecture_Papillary.mp4\n",
            "[15] Breast_Lecture_Prognostics.mp4\n",
            "[16] Breast_Lecture_Rad-Path.mp4\n",
            "[17] Breast_Lecture_SpindleCell.mp4\n",
            "[18] Breast_Lecture_Treated.mp4\n",
            "[19] Derm_Lecture_10_Folliculitis_SLIDE_SESSION.mp4\n",
            "[20] Derm_Lecture_11_Granulomatous_dermatitis with Jeff North.mp4\n",
            "[21] Derm_Lecture_12_Histiocytoses.mp4\n",
            "[22] Derm_Lecture_13_pigment_disorders.mp4\n",
            "[23] Derm_Lecture_14_inpatient_SLIDE_SESSION.mp4\n",
            "[24] Derm_Lecture_15_interface_dermatitis.mp4\n",
            "[25] Derm_Lecture_16_intraepidermal_vesicular_dermatitis.mp4\n",
            "[26] Derm_Lecture_17_intro_to_dermpath.mp4\n",
            "[27] Derm_Lecture_18_normal_skin.mp4\n",
            "[28] Derm_Lecture_19_intro_to_inflammatory_patterns_SLIDE_SESSION.mp4\n",
            "[29] Derm_Lecture_1_Adnexal_SLIDE_SESSION.mp4\n",
            "[30] Derm_Lecture_20_margins_and_levels.mp4\n",
            "[31] Derm_Lecture_21_medication_reactions.mp4\n",
            "[32] Derm_Lecture_22_melanoma.mp4\n",
            "[33] Derm_Lecture_23_molecular_melanocytic.mp4\n",
            "[34] Derm_Lecture_24_molecular_melanocytic.mp4\n",
            "[35] Derm_Lecture_25_nail.mp4\n",
            "[36] Derm_Lecture_26_nevi2.mp4\n",
            "[37] Derm_Lecture_27_nevi1.mp4\n",
            "[38] Derm_Lecture_28_oral.mp4\n",
            "[39] Derm_Lecture_29_panniculitis_SLIDE_SESSION.mp4\n",
            "[40] Derm_Lecture_2_Adnexal.mp4\n",
            "[41] Derm_Lecture_30_miscellaneous.mp4\n",
            "[42] Derm_Lecture_31_psoriasiform_dermatitis.mp4\n",
            "[43] Derm_Lecture_32_spongiotic_dermatitis.mp4\n",
            "[44] Derm_Lecture_33_subepidermal_vesicular_dermatitis.mp4\n",
            "[45] Derm_Lecture_34_AK_and_SCC.mp4\n",
            "[46] Derm_Lecture_35_vascular2.mp4\n",
            "[47] Derm_Lecture_36_vasculitis_vasculopathy.mp4\n",
            "[48] Derm_Lecture_37_vascular1.mp4\n",
            "[49] Derm_Lecture_38_fat_muscle_bone_SLIDE_SESSION.mp4\n",
            "[50] Derm_Lecture_3_Cutaneous lymphomas.mp4\n",
            "[51] Derm_Lecture_4_cutaneous lymphomas.mp4\n",
            "[52] Derm_Lecture_5_IHC_Deposits._SLIDE_SESSION.mp4\n",
            "[53] Derm_Lecture_6_Dermatitis_without_epidermal_change_SLIDE_SESSION.mp4\n",
            "[54] Derm_Lecture_7_cutaneous_infections.mp4\n",
            "[55] Derm_Lecture_8_Direct_IF.mp4\n",
            "[56] Derm_Lecture_8_alopecia_SLIDE_SESSION.mp4\n",
            "[57] GI_Lecture_0_Gross_Liver.mp4\n",
            "[58] GI_Lecture_10_Colon.mp4\n",
            "[59] GI_Lecture_11_Esophagus.mp4\n",
            "[60] GI_Lecture_12_SmallIntestine.mp4\n",
            "[61] GI_Lecture_13_Stomach2.mp4\n",
            "[62] GI_Lecture_1_Liver1.mp4\n",
            "[63] GI_Lecture_2_Liver2.mp4\n",
            "[64] GI_Lecture_3_Liver3.mp4\n",
            "[65] GI_Lecture_4_Peds1.mp4\n",
            "[66] GI_Lecture_5_Peds2.mp4\n",
            "[67] GI_Lecture_7_IBD.mp4\n",
            "[68] GI_Lecture_8_Pancreas.mp4\n",
            "[69] GI_Lecture_9_Stomach1.mp4\n",
            "[70] GU_Lecture_0_BladderTumors.mp4\n",
            "[71] GU_Lecture_0_Papillary_Urothelial_CA.mp4\n",
            "[72] GU_Lecture_0_ProstateGrading.mp4\n",
            "[73] GU_Lecture_0_ProstateIntraductal.mp4\n",
            "[74] GU_Lecture_10_Prostate1.mp4\n",
            "[75] GU_Lecture_11_Prostate2.mp4\n",
            "[76] GU_Lecture_12_Prostate3.mp4\n",
            "[77] GU_Lecture_13_Prostate4.mp4\n",
            "[78] GU_Lecture_1_Kidney1.mp4\n",
            "[79] GU_Lecture_2_Kidney2.mp4\n",
            "[80] GU_Lecture_3_Kidney3.mp4\n",
            "[81] GU_Lecture_4_Kidney4.mp4\n",
            "[82] GU_Lecture_5_Testis.mp4\n",
            "[83] GU_Lecture_6_Bladder1.mp4\n",
            "[84] GU_Lecture_7_Bladder2.mp4\n",
            "[85] GU_Lecture_8_Bladder3.mp4\n",
            "[86] GU_Lecture_9_Bladder4.mp4\n",
            "[87] Gyn_Lecture_10_Ovary4.mp4\n",
            "[88] Gyn_Lecture_12_Uterine_Mesenchymal1.mp4\n",
            "[89] Gyn_Lecture_13_Uterine_Mesenchymal2.mp4\n",
            "[90] Gyn_Lecture_1_Grossing1.mp4\n",
            "[91] Gyn_Lecture_2_Grossing2.mp4\n",
            "[92] Gyn_Lecture_3_Cervix_Glandular.mp4\n",
            "[93] Gyn_Lecture_4_Cervix_Squamous.mp4\n",
            "[94] Gyn_Lecture_5_Endometrium1.mp4\n",
            "[95] Gyn_Lecture_6_Gestational.mp4\n",
            "[96] Gyn_Lecture_7_Ovary1.mp4\n",
            "[97] Gyn_Lecture_8_Ovary2.mp4\n",
            "[98] Gyn_Lecture_9_Ovary3.mp4\n",
            "[99] HN_Lecture_0_Grossing1.mp4\n",
            "[100] HN_Lecture_0_Grossing2.mp4\n",
            "[101] HN_Lecture_1_Salivary1.mp4\n",
            "[102] HN_Lecture_2_Salivary2.mp4\n",
            "[103] HN_Lecture_3_Thyroid1.mp4\n",
            "[104] HN_Lecture_4_Thyroid2.mp4\n",
            "[105] HN_Lecture_5_Odontogenic.mp4\n",
            "[106] HN_Lecture_6_Oral.mp4\n",
            "[107] HN_Lecture_7_HPV.mp4\n",
            "[108] Other_Heme_Lecture_AML Plasma BMFailure BMSystemic.mp4\n",
            "[109] Other_Heme_Lecture_AML.mp4\n",
            "[110] Other_Heme_Lecture_Bone Marrow Failure Syndromes.mp4\n",
            "[111] Other_Heme_Lecture_Bone Marrow Manifestations of Systemic Disease.mp4\n",
            "[112] Other_Heme_Lecture_Eo MDS MPN.mp4\n",
            "[113] Other_Heme_Lecture_Histiocytic.mp4\n",
            "[114] Other_Heme_Lecture_MDS MPN Eo last half.mp4\n",
            "[115] Other_Heme_Lecture_Myelodysplasia.mp4\n",
            "[116] Other_Heme_Lecture_Myeloid Intro.mp4\n",
            "[117] Other_Heme_Lecture_NK and TCL.mp4\n",
            "[118] Other_Heme_Lecture_NLP PTLD.mp4\n",
            "[119] Other_Heme_Lecture_Plasma Cell Neoplasms.mp4\n",
            "[120] Other_Heme_Lecture_Spleen.mp4\n",
            "[121] Other_Heme_Lecture_Systemic last half MDS MPN Eo first hald.mp4\n",
            "[122] Other_Heme_Lecture_aggressive b cell lymphomas.mp4\n",
            "[123] Other_Heme_Lecture_small b cells part 1.mp4\n",
            "[124] Other_Heme_Lecture_small b cells part 2.mp4\n",
            "[125] Other_Heme_Reactive_Hodgkin_IHC.mp4\n",
            "[126] Other_Skin_Lecture_10_Blisters.mp4\n",
            "[127] Other_Skin_Lecture_11_Granulomatous.mp4\n",
            "[128] Other_Skin_Lecture_12_Vasculitis.mp4\n",
            "[129] Other_Skin_Lecture_14_15_Alopecia_Depositional_Metabolic.mp4\n",
            "[130] Other_Skin_Lecture_16_17_Panniculitis_Fibrous.mp4\n",
            "[131] Other_Skin_Lecture_18_19_Bacteria.Mycobacteria_Fungal.mp4\n",
            "[132] Other_Skin_Lecture_1_NormalAnat_Intro.mp4\n",
            "[133] Other_Skin_Lecture_20_21_Neural_Vascular.mp4\n",
            "[134] Other_Skin_Lecture_22_23_Fat.Muscle.Bone_Mets.mp4\n",
            "[135] Other_Skin_Lecture_24_Viral,Parasite.mp4\n",
            "[136] Other_Skin_Lecture_25_Lymphomas_Genoderms.mp4\n",
            "[137] Other_Skin_Lecture_2_Cysts_BenignEpidermal.mp4\n",
            "[138] Other_Skin_Lecture_3_MalignantEpiTumors.mp4\n",
            "[139] Other_Skin_Lecture_4_Follicular_Sebaceous.mp4\n",
            "[140] Other_Skin_Lecture_5_SweatTumors.mp4\n",
            "[141] Other_Skin_Lecture_6_BenignMelanocytic.mp4\n",
            "[142] Other_Skin_Lecture_7_MalignantMelanocytic.mp4\n",
            "[143] Other_Skin_Lecture_8_Interface_SupDeep.mp4\n",
            "[144] Other_Skin_Lecture_9_PsoriasiformSpongiotic.mp4\n",
            "[145] Other_Skin_Lecture_BasicExam_Review.mp4\n",
            "[146] Thoracic_Lecture_0_Caridac_Gross.mp4\n",
            "[147] Thoracic_Lecture_0_Lung_Gross.mp4\n",
            "[148] Thoracic_Lecture_1_Lung_Non_Neoplastic1.mp4\n",
            "[149] Thoracic_Lecture_2_Lung_Non_Neoplastic2.mp4\n",
            "[150] Thoracic_Lecture_3_Lung_ARS.mp4\n",
            "[151] Thoracic_Lecture_4_Lung_ILD.mp4\n",
            "[152] Thoracic_Lecture_5_Lung_Neoplastic.mp4\n",
            "[153] Thoracic_Lecture_6_Lung_Molecular1.mp4\n",
            "[154] Thoracic_Lecture_8_Lung_Molecular2.mp4\n",
            "[155] Thoracic_Lecture_9_Thymus.mp4\n",
            "[156] YT_BST_Bone_Tumors1_Garcia.mp4\n",
            "[157] YT_BST_Bone_Tumors2_Garcia.mp4\n",
            "[158] YT_BST_Cartilage_Tumors.mp4\n",
            "[159] YT_BST_Spindle_Cell_Sarcomas_Fritchie.mp4\n",
            "[160] YT_Cyto_CSF_Sobek.mp4\n",
            "[161] YT_Cyto_Cervical1_Banet.mp4\n",
            "[162] YT_Cyto_Cervical2_Banet.mp4\n",
            "[163] YT_Cyto_Cervical_Goulart.mp4\n",
            "[164] YT_Cyto_Cervix_Giovanni.mp4\n",
            "[165] YT_Cyto_Lung_Lozano.mp4\n",
            "[166] YT_Cyto_Lung_Schmitt.mp4\n",
            "[167] YT_Cyto_Lymph_Kloboves.mp4\n",
            "[168] YT_Cyto_Lymph_WHO_Field.mp4\n",
            "[169] YT_Cyto_Pancreas_Cyst_Pitman.mp4\n",
            "[170] YT_Cyto_Pancreas_Jhala.mp4\n",
            "[171] YT_Cyto_Pancreas_Reid.mp4\n",
            "[172] YT_Cyto_Pancreas_WHO.mp4\n",
            "[173] YT_Cyto_Pancres_WHO_Pitman.mp4\n",
            "[174] YT_Cyto_Paris_Bubendorf.mp4\n",
            "[175] YT_Cyto_Peds_Barroca.mp4\n",
            "[176] YT_Cyto_Salivary_Chen.mp4\n",
            "[177] YT_Cyto_Salivary_Faquin.mp4\n",
            "[178] YT_Cyto_Serous_Chandra.mp4\n",
            "[179] YT_Cyto_Thyroid_Bongiovanni.mp4\n",
            "[180] YT_Cyto_Thyroid_Nishino.mp4\n",
            "[181] YT_Cyto_Thyroid_Rossi.mp4\n",
            "[182] YT_Derm_Adnexal_Skupsk1.mp4\n",
            "[183] YT_Derm_Adnexal_Skupsky2.mp4\n",
            "[184] YT_GI_Anal_Darragh.mp4\n",
            "[185] YT_GI_Barretts_Esophagus_Goldblum.mp4\n",
            "[186] YT_GI_Colon_Cancer_Dhall.mp4\n",
            "[187] YT_GI_Colorectal_Polyp_Pai.mp4\n",
            "[188] YT_GI_Colorectal_Polyps_Pai.mp4\n",
            "[189] YT_GI_Gastric_Polyps_Montgomery.mp4\n",
            "[190] YT_GI_IBD_Gonzalez.mp4\n",
            "[191] YT_GI_IBD_Related_Dysplasia.mp4\n",
            "[192] YT_GI_Iatrogenic_Montgomery.mp4\n",
            "[193] YT_GI_Luminal_Biopsy_Furth.mp4\n",
            "[194] YT_GI_Lymphomas_Cruise.mp4\n",
            "[195] YT_GI_Mesenchymal_Montgomery.mp4\n",
            "[196] YT_GI_Non_IBD_Colitis_Allende_SLIDE_SESSION.mp4\n",
            "[197] YT_GI_Pancreas_Neoplastic_Hruben.mp4\n",
            "[198] YT_GI_Pancreas_Solid_Basturk.mp4\n",
            "[199] YT_GI_Vascular_Allende.mp4\n",
            "[200] YT_HN_Thyroid_Thompson.mp4\n",
            "[201] YT_HN_WHO_Classification_Thompson.mp4\n",
            "[202] YT_Micro_Bacteria1_Morgan.mp4\n",
            "[203] YT_Micro_Bacteria2_Morgan.mp4\n",
            "[204] YT_Micro_Mycobacteria_Morgan.mp4\n",
            "[205] YT_Micro_Mycology1_Morgan.mp4\n",
            "[206] YT_Micro_Mycology2_Morgan.mp4\n",
            "[207] YT_Micro_Parasites_Morgan.mp4\n",
            "[208] YT_Skin_B_Cell_Lymphoma_Cassarino.mp4\n",
            "[209] YT_Skin_Blistering_Fung.mp4\n",
            "[210] YT_Skin_Common_Disorders_Fung.mp4\n",
            "[211] YT_Skin_Granulomatous_Dermatitis_Fung.mp4\n",
            "[212] YT_Skin_IHC_Fung.mp4\n",
            "[213] YT_Skin_Interface_Dermatitis_Fung.mp4\n",
            "[214] YT_Skin_Intro_Fung.mp4\n",
            "[215] YT_Skin_Melanocytic_Fung.mp4\n",
            "[216] YT_Skin_Melanoma_LeBoit.mp4\n",
            "[217] YT_Skin_Metabolic_and_Connective_Tissue_Junkins.mp4\n",
            "[218] YT_Skin_Papulosquamous_and_Spongiotic_Dermatitis_Ko.mp4\n",
            "[219] YT_Skin_Perforating_Fung.mp4\n",
            "[220] YT_Skin_Perivascular_Dermatitis_Fung.mp4\n",
            "[221] YT_Skin_Psoriasiform_Dermatitis_Fung.mp4\n",
            "[222] YT_Skin_Soft_Tissue_Fung.mp4\n",
            "[223] YT_Skin_Spongiotic_Dermatitis_Fung.mp4\n",
            "[224] YT_Skin_T_Cell_Lymphoma_Cassarino.mp4\n",
            "[225] YT_Skin_Vacuolar_and_Lichenoid_Interface_Dermatitis_Junkins.mp4\n",
            "[226] YT_Skin_Vascular_Fung.mp4\n",
            "[227] YT_Skin_Vasculitis_Johnson.mp4\n",
            "[228] YT_Skin_WHO_Classification_Scoyler.mp4\n",
            "\n",
            "============================================================\n",
            "üé• PROCESSING 54/1: Derm_Lecture_7_cutaneous_infections\n",
            "============================================================\n",
            "üéôÔ∏è Step 1: Whisper Transcription...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:00<00:00, 163MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéûÔ∏è Step 2: Extracting Slides...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Scanning:   0%|          | 0/226 [00:00<?, ?seg/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7703daafeaf4527bb55ec4de96224a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   -> Consolidated into 118 unique slides.\n",
            "‚òÅÔ∏è Step 3: Uploading Images...\n",
            "üß† Step 4: Gemini Vision Analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/118 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2937322267.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATHS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_videos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mmain_lectures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2937322267.py\u001b[0m in \u001b[0;36mmain_lectures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATHS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_videos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mmain_lectures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2937322267.py\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(video_path, counter, total)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0maiohttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClientSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_slide_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0menhanced_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtqdm_asyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m# 5. SAVE RAW JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/asyncio.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mifs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwrap_awaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         res = [await f for f in cls.as_completed(ifs, loop=loop, timeout=timeout,\n\u001b[0m\u001b[1;32m     80\u001b[0m                                                  total=total, **tqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m_wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;31m# Dummy value from _on_timeout().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mgetter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Just in case getter is not done yet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 2: BATCH LECTURE ARCHITECT (Flexible Input Support)"
      ],
      "metadata": {
        "id": "x0-3O_Ex5Sf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 2: BATCH LECTURE ARCHITECT (Flexible Input Support)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import difflib\n",
        "import random\n",
        "from typing import List, Dict, Set, Any\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gemini-3-pro-preview\"\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "# --- SETUP ---\n",
        "if 'PATHS' not in globals():\n",
        "    raise NameError(\"‚ùå PATHS not found. Please run Block 0.\")\n",
        "bucket = storage.Client().bucket(PATHS['gcs_bucket'])\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_read_text(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return blob.download_as_string().decode('utf-8') if blob.exists() else \"\"\n",
        "\n",
        "def gcs_load_json(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def validate_tag(tag_input, valid_set):\n",
        "    if not tag_input: return \"Skin::Unclassified\"\n",
        "    clean = str(tag_input[0]) if isinstance(tag_input, list) and tag_input else str(tag_input)\n",
        "    clean = clean.strip()\n",
        "    if clean in valid_set: return clean\n",
        "    matches = difflib.get_close_matches(clean, list(valid_set), n=1, cutoff=0.7)\n",
        "    return matches[0] if matches else \"Skin::Unclassified\"\n",
        "\n",
        "# --- LOGIC: THE ID SWAPPER ---\n",
        "def inject_real_paths(entities, slide_lookup_map):\n",
        "    for ent in entities:\n",
        "        if 'related_figures' in ent:\n",
        "            for fig in ent['related_figures']:\n",
        "                # Handle ID variations (int vs string)\n",
        "                slide_id = str(fig.get('id', '')).replace(\"Slide_\", \"\")\n",
        "\n",
        "                # Look up real data\n",
        "                real_data = None\n",
        "                # Try direct match or string match\n",
        "                if slide_id in slide_lookup_map:\n",
        "                    real_data = slide_lookup_map[slide_id]\n",
        "                else:\n",
        "                    # Fallback loop for mismatched types\n",
        "                    for k, v in slide_lookup_map.items():\n",
        "                        if str(k) == slide_id:\n",
        "                            real_data = v\n",
        "                            break\n",
        "\n",
        "                if real_data:\n",
        "                    fig['src'] = real_data.get('gcs_path') or real_data.get('image_path')\n",
        "                    fig['gcs_path'] = fig['src']\n",
        "\n",
        "                    # Add timestamp to legend\n",
        "                    ts = real_data.get('timestamp_start') or real_data.get('start_time')\n",
        "                    if ts is not None:\n",
        "                        time_str = f\"(Time: {float(ts):.0f}s)\"\n",
        "                        if time_str not in fig.get('legend', ''):\n",
        "                            fig['legend'] = f\"{fig.get('legend', '')} {time_str}\".strip()\n",
        "                else:\n",
        "                    fig['src'] = None\n",
        "                    fig['gcs_path'] = None\n",
        "    return entities\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. PROMPT\n",
        "# ------------------------------------------------------------------------------\n",
        "def construct_lecture_prompt(transcript_data, valid_tags_list):\n",
        "    return f\"\"\"\n",
        "Role: Senior Dermatopathologist.\n",
        "Objective: Convert the ENTIRE LECTURE provided below into a standardized Knowledge Base.\n",
        "\n",
        "INPUT DATA:\n",
        "- Chronological sequence of slides (ID, Visual Description, Transcript).\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. **Consolidate:** Merge discussion across multiple slides into single Disease Entities.\n",
        "2. **Detail Extraction (CRITICAL):**\n",
        "   - **Stains (IHC):** List every specific stain mentioned (e.g., \"CK20+\", \"TTF-1 negative\").\n",
        "   - **Genetics:** List specific mutations/translocations.\n",
        "3. **Tagging:** Select exactly ONE tag from the reference list.\n",
        "4. **Figure Linking (ID SWAP):**\n",
        "   - Select the BEST slides for 'related_figures'.\n",
        "   - **IMPORTANT:** Use the exact ID provided (e.g., \"Slide_5\"). Leave `src` and `gcs_path` as \"PLACEHOLDER\".\n",
        "\n",
        "REQUIRED JSON SCHEMA:\n",
        "[\n",
        "  {{\n",
        "    \"entity_name\": \"Disease Name\",\n",
        "    \"definition\": \"...\",\n",
        "    \"tags\": [\"Single_Exact_Tag\"],\n",
        "    \"html_gcs_path\": null,\n",
        "\n",
        "    \"clinical\": \"...\",\n",
        "    \"pathogenesis\": \"...\",\n",
        "    \"macroscopic\": \"...\",\n",
        "    \"microscopic\": \"...\",\n",
        "    \"ancillary_studies\": \"List ALL stains/molecular details.\",\n",
        "    \"differential_diagnosis\": \"...\",\n",
        "    \"staging\": \"...\",\n",
        "    \"prognosis_and_prediction\": \"...\",\n",
        "\n",
        "    \"related_figures\": [\n",
        "        {{\n",
        "            \"id\": \"Slide_X\",\n",
        "            \"src\": \"PLACEHOLDER\",\n",
        "            \"gcs_path\": \"PLACEHOLDER\",\n",
        "            \"diagnosis\": \"Disease Name\",\n",
        "            \"legend\": \"Specific description of this slide.\"\n",
        "        }}\n",
        "    ]\n",
        "  }}\n",
        "]\n",
        "\n",
        "REFERENCE TAGS:\n",
        "{valid_tags_list}\n",
        "\n",
        "LECTURE CONTENT:\n",
        "{transcript_data}\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. SINGLE LECTURE PROCESSOR\n",
        "# ------------------------------------------------------------------------------\n",
        "async def process_lecture_async(session, raw_path, tags_text, tags_set):\n",
        "    fname = raw_path.split('/')[-1]\n",
        "    # Clean name: remove _RAW and extension\n",
        "    lecture_name = fname.replace(\"_RAW.json\", \"\").replace(\".json\", \"\")\n",
        "\n",
        "    print(f\"\\nüöÄ Processing: {lecture_name}\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    slides = gcs_load_json(raw_path)\n",
        "    if not slides:\n",
        "        print(\"   ‚ö†Ô∏è Empty file. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # 2. Build Monolith Input\n",
        "    formatted_input = \"\"\n",
        "    slide_map = {}\n",
        "\n",
        "    for s in slides:\n",
        "        # Handle variations in key names (Block 1 versions differ)\n",
        "        sid = s.get('id') if 'id' in s else s.get('segment_id')\n",
        "        transcript = s.get('raw_transcript', '')\n",
        "        visual = s.get('visual_desc') or s.get('slide_title', '')\n",
        "        ts = s.get('timestamp_start') or s.get('start_time', 0)\n",
        "\n",
        "        slide_map[str(sid)] = s\n",
        "\n",
        "        formatted_input += f\"\\n--- ID: Slide_{sid} (Time: {float(ts):.0f}s) ---\\n\"\n",
        "        formatted_input += f\"VISUAL: {visual}\\n\"\n",
        "        formatted_input += f\"TRANSCRIPT: {transcript}\\n\"\n",
        "\n",
        "    # 3. Call AI\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "    payload = {\"contents\": [{\"parts\": [{\"text\": construct_lecture_prompt(formatted_input, tags_text)}]}]}\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            async with session.post(url, json=payload, timeout=600) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "                    raw_txt = data['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\[.*\\]', raw_txt.replace(\"```json\", \"\").replace(\"```\", \"\"), re.DOTALL)\n",
        "\n",
        "                    if match:\n",
        "                        entities = json.loads(match.group(0))\n",
        "\n",
        "                        # ID Swap & Validation\n",
        "                        entities = inject_real_paths(entities, slide_map)\n",
        "\n",
        "                        valid_entities = []\n",
        "                        for ent in entities:\n",
        "                            ent['tags'] = [validate_tag(ent.get('tags'), tags_set)]\n",
        "                            ent['html_gcs_path'] = None\n",
        "                            for k in [\"microscopic\", \"ancillary_studies\", \"differential_diagnosis\", \"clinical\", \"pathogenesis\"]:\n",
        "                                if k not in ent: ent[k] = None\n",
        "                            valid_entities.append(ent)\n",
        "\n",
        "                        # Save\n",
        "                        out_path = f\"{PATHS['gcs_content_lectures']}/{lecture_name}_MASTER.json\"\n",
        "                        gcs_upload_json(valid_entities, out_path)\n",
        "                        print(f\"   ‚úÖ Saved: {out_path} ({len(valid_entities)} entities)\")\n",
        "                        return\n",
        "\n",
        "                elif response.status == 429:\n",
        "                    print(f\"   ‚è≥ Rate Limit... Waiting 30s\")\n",
        "                    await asyncio.sleep(30)\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"   ‚ùå API Error: {response.status}\")\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Exception: {e}\")\n",
        "            await asyncio.sleep(5)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MAIN BATCH RUNNER\n",
        "# ------------------------------------------------------------------------------\n",
        "async def main_batch_flexible():\n",
        "    # 1. Tags\n",
        "    tag_files = [b.name for b in bucket.list_blobs(prefix=\"Tags/\") if b.name.endswith('.txt')]\n",
        "    print(\"\\n--- SELECT TAG LIST ---\")\n",
        "    for i, f in enumerate(tag_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    t_idx = int(input(\"Choice: \")) - 1\n",
        "    tags_text = gcs_read_text(tag_files[t_idx])\n",
        "    tags_set = set(l.strip() for l in tags_text.splitlines() if l.strip())\n",
        "\n",
        "    # 2. Select Files (Flexible)\n",
        "    blobs = list(bucket.list_blobs(prefix=PATHS['gcs_content_lectures']))\n",
        "    candidates = []\n",
        "\n",
        "    for b in blobs:\n",
        "        if not b.name.endswith(\".json\"): continue\n",
        "        # EXCLUDE Master/Consolidated files\n",
        "        if \"_MASTER\" in b.name or \"_CONSOLIDATED\" in b.name or \"_APP_READY\" in b.name:\n",
        "            continue\n",
        "        candidates.append(b.name)\n",
        "\n",
        "    if not candidates: print(\"‚ùå No input JSONs found.\"); return\n",
        "\n",
        "    print(\"\\n--- AVAILABLE LECTURES ---\")\n",
        "    for i, f in enumerate(candidates): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "\n",
        "    sel = input(\"\\nSelect (e.g. 1, 3-5, all): \")\n",
        "    indices = set()\n",
        "    if sel == 'all': indices = range(len(candidates))\n",
        "    else:\n",
        "        for part in sel.split(','):\n",
        "            if '-' in part:\n",
        "                s, e = map(int, part.split('-'))\n",
        "                indices.update(range(s-1, e))\n",
        "            elif part.strip().isdigit():\n",
        "                indices.add(int(part)-1)\n",
        "\n",
        "    print(f\"\\nüöÄ Queued {len(indices)} lectures...\")\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for idx in sorted(list(indices)):\n",
        "            if 0 <= idx < len(candidates):\n",
        "                await process_lecture_async(session, candidates[idx], tags_text, tags_set)\n",
        "\n",
        "    print(\"\\nüéâ Batch Complete.\")\n",
        "\n",
        "await main_batch_flexible()"
      ],
      "metadata": {
        "id": "GgyXTjmV5f3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97bf2646-c98a-4129-b332-a120f8e1c588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SELECT TAG LIST ---\n",
            "[1] BST_Tags.txt\n",
            "[2] Breast_Tags.txt\n",
            "[3] Endo_Tags.txt\n",
            "[4] GI_Tags.txt\n",
            "[5] GYN_Tags.txt\n",
            "[6] Skin_Tags.txt\n",
            "Choice: 6\n",
            "\n",
            "--- AVAILABLE LECTURES ---\n",
            "[1] BST_Lecture_1_Grossing.json\n",
            "[2] final_ENHANCED_data.json\n",
            "[3] final_structured_data.json\n",
            "[4] BST_Lecture_2_SoftTissue1.json\n",
            "[5] final_ENHANCED_data.json\n",
            "[6] BST_Lecture_3_SofTissue2.json\n",
            "[7] final_ENHANCED_data.json\n",
            "[8] BST_Lecture_4_SoftTissue3.json\n",
            "[9] final_ENHANCED_data.json\n",
            "[10] BST_Lecture_5_Bone1.json\n",
            "[11] final_ENHANCED_data.json\n",
            "[12] BST_Lecture_6_Bone2.json\n",
            "[13] final_ENHANCED_data.json\n",
            "[14] Breast_Lecture_Epithelial.json\n",
            "[15] final_ENHANCED_data.json\n",
            "[16] Breast_Lecture_Fibroepithelial.json\n",
            "[17] final_ENHANCED_data.json\n",
            "[18] Breast_Lecture_Grossing.json\n",
            "[19] final_ENHANCED_data.json\n",
            "[20] Breast_Lecture_IHC.json\n",
            "[21] final_ENHANCED_data.json\n",
            "[22] Breast_Lecture_Invasive.json\n",
            "[23] final_ENHANCED_data.json\n",
            "[24] Breast_Lecture_Lobular.json\n",
            "[25] final_ENHANCED_data.json\n",
            "[26] Breast_Lecture_Normal.json\n",
            "[27] final_ENHANCED_data.json\n",
            "[28] Breast_Lecture_Papillary.json\n",
            "[29] final_ENHANCED_data.json\n",
            "[30] Breast_Lecture_Prognostics.json\n",
            "[31] final_ENHANCED_data.json\n",
            "[32] Breast_Lecture_Rad-Path.json\n",
            "[33] final_ENHANCED_data.json\n",
            "[34] Breast_Lecture_SpindleCell.json\n",
            "[35] final_ENHANCED_data.json\n",
            "[36] Breast_Lecture_Treated.json\n",
            "[37] final_ENHANCED_data.json\n",
            "[38] Derm_Lecture_10_Folliculitis_SLIDE_SESSION.json\n",
            "[39] Derm_Lecture_11_Granulomatous_dermatitis with Jeff North.json\n",
            "[40] final_ENHANCED_data.json\n",
            "[41] final_structured_data.json\n",
            "[42] Derm_Lecture_12_Histiocytoses.json\n",
            "[43] final_structured_data.json\n",
            "[44] Derm_Lecture_13_pigment_disorders.json\n",
            "[45] final_ENHANCED_data.json\n",
            "[46] final_structured_data.json\n",
            "[47] Derm_Lecture_15_interface_dermatitis.json\n",
            "[48] final_ENHANCED_data.json\n",
            "[49] final_structured_data.json\n",
            "[50] Derm_Lecture_16_intraepidermal_vesicular_dermatitis.json\n",
            "[51] final_structured_data.json\n",
            "[52] Derm_Lecture_17_intro_to_dermpath.json\n",
            "[53] final_structured_data.json\n",
            "[54] Derm_Lecture_18_normal_skin.json\n",
            "[55] final_structured_data.json\n",
            "[56] Derm_Lecture_20_margins_and_levels.json\n",
            "[57] final_structured_data.json\n",
            "[58] Derm_Lecture_21_medication_reactions.json\n",
            "[59] final_structured_data.json\n",
            "[60] Derm_Lecture_22_melanoma.json\n",
            "[61] final_ENHANCED_data.json\n",
            "[62] final_structured_data.json\n",
            "[63] Derm_Lecture_23_molecular_melanocytic.json\n",
            "[64] final_ENHANCED_data.json\n",
            "[65] final_structured_data.json\n",
            "[66] Derm_Lecture_24_molecular_melanocytic.json\n",
            "[67] final_ENHANCED_data.json\n",
            "[68] final_structured_data.json\n",
            "[69] Derm_Lecture_25_nail.json\n",
            "[70] final_structured_data.json\n",
            "[71] Derm_Lecture_26_nevi2.json\n",
            "[72] Derm_Lecture_27_nevi1.json\n",
            "[73] final_ENHANCED_data.json\n",
            "[74] final_structured_data.json\n",
            "[75] Derm_Lecture_28_oral.json\n",
            "[76] final_structured_data.json\n",
            "[77] Derm_Lecture_2_Adnexal.json\n",
            "[78] final_ENHANCED_data.json\n",
            "[79] final_structured_data.json\n",
            "[80] Derm_Lecture_30_miscellaneous.json\n",
            "[81] Derm_Lecture_30_miscellaneous_RAW.json\n",
            "[82] Derm_Lecture_31_psoriasiform_dermatitis.json\n",
            "[83] Derm_Lecture_32_spongiotic_dermatitis.json\n",
            "[84] final_structured_data.json\n",
            "[85] Derm_Lecture_33_subepidermal_vesicular_dermatitis.json\n",
            "[86] final_structured_data.json\n",
            "[87] Derm_Lecture_34_AK_and_SCC.json\n",
            "[88] final_structured_data.json\n",
            "[89] Derm_Lecture_35_vascular2.json\n",
            "[90] final_structured_data.json\n",
            "[91] Derm_Lecture_36_vasculitis_vasculopathy.json\n",
            "[92] final_structured_data.json\n",
            "[93] Derm_Lecture_37_vascular1.json\n",
            "[94] final_structured_data.json\n",
            "[95] Derm_Lecture_3_Cutaneous lymphomas.json\n",
            "[96] final_structured_data.json\n",
            "[97] Derm_Lecture_4_cutaneous lymphomas.json\n",
            "[98] final_structured_data.json\n",
            "[99] Derm_Lecture_5_IHC_Deposits._SLIDE_SESSION.json\n",
            "[100] final_structured_data.json\n",
            "[101] Derm_Lecture_7_cutaneous_infections.json\n",
            "[102] final_structured_data.json\n",
            "[103] Derm_Lecture_8_Direct_IF.json\n",
            "[104] final_structured_data.json\n",
            "[105] GI_Lecture_0_Gross_Liver.json\n",
            "[106] final_ENHANCED_data.json\n",
            "[107] final_structured_data.json\n",
            "[108] GI_Lecture_10_Colon.json\n",
            "[109] final_ENHANCED_data.json\n",
            "[110] final_structured_data.json\n",
            "[111] GI_Lecture_11_Esophagus.json\n",
            "[112] final_ENHANCED_data.json\n",
            "[113] final_structured_data.json\n",
            "[114] GI_Lecture_12_SmallIntestine.json\n",
            "[115] final_ENHANCED_data.json\n",
            "[116] final_structured_data.json\n",
            "[117] GI_Lecture_13_Stomach2.json\n",
            "[118] final_ENHANCED_data.json\n",
            "[119] final_structured_data.json\n",
            "[120] GI_Lecture_1_Liver1.json\n",
            "[121] final_ENHANCED_data.json\n",
            "[122] final_structured_data.json\n",
            "[123] GI_Lecture_2_Liver2.json\n",
            "[124] final_ENHANCED_data.json\n",
            "[125] final_structured_data.json\n",
            "[126] GI_Lecture_3_Liver3.json\n",
            "[127] final_ENHANCED_data.json\n",
            "[128] final_structured_data.json\n",
            "[129] GI_Lecture_4_Peds1.json\n",
            "[130] final_ENHANCED_data.json\n",
            "[131] final_structured_data.json\n",
            "[132] GI_Lecture_5_Peds2.json\n",
            "[133] final_ENHANCED_data.json\n",
            "[134] final_structured_data.json\n",
            "[135] GI_Lecture_7_IBD.json\n",
            "[136] final_ENHANCED_data.json\n",
            "[137] final_structured_data.json\n",
            "[138] GI_Lecture_8_Pancreas.json\n",
            "[139] final_ENHANCED_data.json\n",
            "[140] final_structured_data.json\n",
            "[141] GI_Lecture_9_Stomach1.json\n",
            "[142] final_ENHANCED_data.json\n",
            "[143] final_structured_data.json\n",
            "[144] GU_Lecture_0_BladderTumors.json\n",
            "[145] GU_Lecture_0_Papillary_Urothelial_CA.json\n",
            "[146] GU_Lecture_0_ProstateGrading.json\n",
            "[147] GU_Lecture_0_ProstateIntraductal.json\n",
            "[148] GU_Lecture_10_Prostate1.json\n",
            "[149] final_ENHANCED_data.json\n",
            "[150] final_structured_data.json\n",
            "[151] GU_Lecture_11_Prostate2.json\n",
            "[152] final_ENHANCED_data.json\n",
            "[153] final_structured_data.json\n",
            "[154] GU_Lecture_12_Prostate3.json\n",
            "[155] final_ENHANCED_data.json\n",
            "[156] final_structured_data.json\n",
            "[157] GU_Lecture_13_Prostate4.json\n",
            "[158] final_ENHANCED_data.json\n",
            "[159] final_structured_data.json\n",
            "[160] GU_Lecture_1_Kidney1.json\n",
            "[161] final_ENHANCED_data.json\n",
            "[162] final_structured_data.json\n",
            "[163] GU_Lecture_2_Kidney2.json\n",
            "[164] final_ENHANCED_data.json\n",
            "[165] final_structured_data.json\n",
            "[166] GU_Lecture_3_Kidney3.json\n",
            "[167] final_ENHANCED_data.json\n",
            "[168] final_structured_data.json\n",
            "[169] GU_Lecture_4_Kidney4.json\n",
            "[170] final_ENHANCED_data.json\n",
            "[171] final_structured_data.json\n",
            "[172] GU_Lecture_5_Testis.json\n",
            "[173] final_ENHANCED_data.json\n",
            "[174] final_structured_data.json\n",
            "[175] GU_Lecture_6_Bladder1.json\n",
            "[176] final_ENHANCED_data.json\n",
            "[177] final_structured_data.json\n",
            "[178] GU_Lecture_7_Bladder2.json\n",
            "[179] final_ENHANCED_data.json\n",
            "[180] final_structured_data.json\n",
            "[181] GU_Lecture_8_Bladder3.json\n",
            "[182] final_ENHANCED_data.json\n",
            "[183] final_structured_data.json\n",
            "[184] GU_Lecture_9_Bladder4.json\n",
            "[185] final_ENHANCED_data.json\n",
            "[186] final_structured_data.json\n",
            "[187] Gyn_Lecture_10_Ovary4.json\n",
            "[188] final_ENHANCED_data.json\n",
            "[189] Gyn_Lecture_12_Uterine_Mesenchymal1.json\n",
            "[190] final_ENHANCED_data.json\n",
            "[191] Gyn_Lecture_13_Uterine_Mesenchymal2.json\n",
            "[192] final_ENHANCED_data.json\n",
            "[193] Gyn_Lecture_1_Grossing1.json\n",
            "[194] final_ENHANCED_data.json\n",
            "[195] Gyn_Lecture_2_Grossing2.json\n",
            "[196] final_ENHANCED_data.json\n",
            "[197] Gyn_Lecture_3_Cervix_Glandular.json\n",
            "[198] final_ENHANCED_data.json\n",
            "[199] Gyn_Lecture_4_Cervix_Squamous.json\n",
            "[200] final_ENHANCED_data.json\n",
            "[201] Gyn_Lecture_5_Endometrium1.json\n",
            "[202] final_ENHANCED_data.json\n",
            "[203] Gyn_Lecture_6_Gestational.json\n",
            "[204] final_ENHANCED_data.json\n",
            "[205] Gyn_Lecture_7_Ovary1.json\n",
            "[206] final_ENHANCED_data.json\n",
            "[207] Gyn_Lecture_8_Ovary2.json\n",
            "[208] final_ENHANCED_data.json\n",
            "[209] Gyn_Lecture_9_Ovary3.json\n",
            "[210] final_ENHANCED_data.json\n",
            "[211] HN_Lecture_0_Grossing1.json\n",
            "[212] final_ENHANCED_data.json\n",
            "[213] final_structured_data.json\n",
            "[214] HN_Lecture_0_Grossing2.json\n",
            "[215] final_ENHANCED_data.json\n",
            "[216] final_structured_data.json\n",
            "[217] HN_Lecture_1_Salivary1.json\n",
            "[218] final_ENHANCED_data.json\n",
            "[219] final_structured_data.json\n",
            "[220] HN_Lecture_2_Salivary2.json\n",
            "[221] final_ENHANCED_data.json\n",
            "[222] final_structured_data.json\n",
            "[223] HN_Lecture_3_Thyroid1.json\n",
            "[224] final_ENHANCED_data.json\n",
            "[225] final_structured_data.json\n",
            "[226] HN_Lecture_4_Thyroid2.json\n",
            "[227] final_structured_data.json\n",
            "[228] HN_Lecture_5_Odontogenic.json\n",
            "[229] final_ENHANCED_data.json\n",
            "[230] final_structured_data.json\n",
            "[231] HN_Lecture_6_Oral.json\n",
            "[232] final_ENHANCED_data.json\n",
            "[233] final_structured_data.json\n",
            "[234] HN_Lecture_7_HPV.json\n",
            "[235] final_ENHANCED_data.json\n",
            "[236] final_structured_data.json\n",
            "[237] Other_Heme_Lecture_AML Plasma BMFailure BMSystemic.json\n",
            "[238] final_ENHANCED_data.json\n",
            "[239] final_structured_data.json\n",
            "[240] Other_Heme_Lecture_AML.json\n",
            "[241] Other_Heme_Lecture_Bone Marrow Failure Syndromes.json\n",
            "[242] Other_Heme_Lecture_Bone Marrow Manifestations of Systemic Disease.json\n",
            "[243] Other_Heme_Lecture_Eo MDS MPN.json\n",
            "[244] Other_Heme_Lecture_Histiocytic.json\n",
            "[245] Other_Heme_Lecture_MDS MPN Eo last half.json\n",
            "[246] Other_Heme_Lecture_Myelodysplasia.json\n",
            "[247] Other_Heme_Lecture_Myeloid Intro.json\n",
            "[248] Other_Heme_Lecture_NK and TCL.json\n",
            "[249] Other_Heme_Lecture_NLP PTLD.json\n",
            "[250] Other_Heme_Lecture_Plasma Cell Neoplasms.json\n",
            "[251] Other_Heme_Lecture_Spleen.json\n",
            "[252] Other_Heme_Lecture_Systemic last half MDS MPN Eo first hald.json\n",
            "[253] Other_Heme_Lecture_aggressive b cell lymphomas.json\n",
            "[254] Other_Heme_Lecture_small b cells part 1.json\n",
            "[255] Other_Heme_Lecture_small b cells part 2.json\n",
            "[256] Other_Heme_Reactive_Hodgkin_IHC.json\n",
            "[257] Other_Skin_Lecture_10_Blisters.json\n",
            "[258] final_ENHANCED_data.json\n",
            "[259] final_structured_data.json\n",
            "[260] Other_Skin_Lecture_11_Granulomatous.json\n",
            "[261] Other_Skin_Lecture_12_Vasculitis.json\n",
            "[262] Other_Skin_Lecture_14_15_Alopecia_Depositional_Metabolic.json\n",
            "[263] Other_Skin_Lecture_16_17_Panniculitis_Fibrous.json\n",
            "[264] Other_Skin_Lecture_18_19_Bacteria.Mycobacteria_Fungal.json\n",
            "[265] Other_Skin_Lecture_1_NormalAnat_Intro.json\n",
            "[266] Other_Skin_Lecture_20_21_Neural_Vascular.json\n",
            "[267] Other_Skin_Lecture_22_23_Fat.Muscle.Bone_Mets.json\n",
            "[268] Other_Skin_Lecture_24_Viral,Parasite.json\n",
            "[269] Other_Skin_Lecture_25_Lymphomas_Genoderms.json\n",
            "[270] Other_Skin_Lecture_2_Cysts_BenignEpidermal.json\n",
            "[271] Other_Skin_Lecture_3_MalignantEpiTumors.json\n",
            "[272] Other_Skin_Lecture_6_BenignMelanocytic.json\n",
            "[273] Other_Skin_Lecture_7_MalignantMelanocytic.json\n",
            "[274] Other_Skin_Lecture_8_Interface_SupDeep.json\n",
            "[275] Other_Skin_Lecture_9_PsoriasiformSpongiotic.json\n",
            "[276] Other_Skin_Lecture_BasicExam_Review.json\n",
            "[277] Thoracic_Lecture_0_Caridac_Gross.json\n",
            "[278] Thoracic_Lecture_0_Lung_Gross.json\n",
            "[279] Thoracic_Lecture_1_Lung_Non_Neoplastic1.json\n",
            "[280] final_ENHANCED_data.json\n",
            "[281] final_structured_data.json\n",
            "[282] Thoracic_Lecture_2_Lung_Non_Neoplastic2.json\n",
            "[283] Thoracic_Lecture_3_Lung_ARS.json\n",
            "[284] Thoracic_Lecture_4_Lung_ILD.json\n",
            "[285] Thoracic_Lecture_5_Lung_Neoplastic.json\n",
            "[286] Thoracic_Lecture_6_Lung_Molecular1.json\n",
            "[287] Thoracic_Lecture_8_Lung_Molecular2.json\n",
            "[288] Thoracic_Lecture_9_Thymus.json\n",
            "[289] YT_BST_Bone_Tumors1_Garcia.json\n",
            "[290] YT_BST_Bone_Tumors2_Garcia.json\n",
            "[291] YT_BST_Cartilage_Tumors.json\n",
            "[292] YT_BST_Spindle_Cell_Sarcomas_Fritchie.json\n",
            "[293] YT_Cyto_CSF_Sobek.json\n",
            "[294] final_ENHANCED_data.json\n",
            "[295] final_structured_data.json\n",
            "[296] YT_Cyto_Cervical1_Banet.json\n",
            "[297] final_structured_data.json\n",
            "[298] YT_Cyto_Cervical2_Banet.json\n",
            "[299] final_structured_data.json\n",
            "[300] YT_Cyto_Cervical_Goulart.json\n",
            "[301] final_ENHANCED_data.json\n",
            "[302] final_structured_data.json\n",
            "[303] YT_Cyto_Cervix_Giovanni.json\n",
            "[304] final_ENHANCED_data.json\n",
            "[305] final_structured_data.json\n",
            "[306] YT_Cyto_Lung_Lozano.json\n",
            "[307] final_structured_data.json\n",
            "[308] YT_Cyto_Lung_Schmitt.json\n",
            "[309] final_structured_data.json\n",
            "[310] YT_Cyto_Lymph_Kloboves.json\n",
            "[311] final_structured_data.json\n",
            "[312] YT_Cyto_Lymph_WHO_Field.json\n",
            "[313] final_ENHANCED_data.json\n",
            "[314] final_structured_data.json\n",
            "[315] YT_Cyto_Pancreas_Cyst_Pitman.json\n",
            "[316] final_structured_data.json\n",
            "[317] YT_Cyto_Pancreas_Jhala.json\n",
            "[318] YT_Cyto_Pancreas_Reid.json\n",
            "[319] final_ENHANCED_data.json\n",
            "[320] final_structured_data.json\n",
            "[321] YT_Cyto_Pancreas_WHO.json\n",
            "[322] YT_Cyto_Pancres_WHO_Pitman.json\n",
            "[323] YT_Cyto_Paris_Bubendorf.json\n",
            "[324] final_ENHANCED_data.json\n",
            "[325] final_structured_data.json\n",
            "[326] YT_Cyto_Peds_Barroca.json\n",
            "[327] final_ENHANCED_data.json\n",
            "[328] final_structured_data.json\n",
            "[329] YT_Cyto_Salivary_Chen.json\n",
            "[330] final_ENHANCED_data.json\n",
            "[331] final_structured_data.json\n",
            "[332] YT_Cyto_Salivary_Faquin.json\n",
            "[333] final_ENHANCED_data.json\n",
            "[334] final_structured_data.json\n",
            "[335] YT_Cyto_Serous_Chandra.json\n",
            "[336] final_ENHANCED_data.json\n",
            "[337] final_structured_data.json\n",
            "[338] YT_Cyto_Thyroid_Bongiovanni.json\n",
            "[339] final_ENHANCED_data.json\n",
            "[340] final_structured_data.json\n",
            "[341] YT_Cyto_Thyroid_Nishino.json\n",
            "[342] final_ENHANCED_data.json\n",
            "[343] final_structured_data.json\n",
            "[344] YT_Cyto_Thyroid_Rossi.json\n",
            "[345] final_ENHANCED_data.json\n",
            "[346] final_structured_data.json\n",
            "[347] YT_GI_Anal_Darragh.json\n",
            "[348] final_structured_data.json\n",
            "[349] YT_GI_Barretts_Esophagus_Goldblum.json\n",
            "[350] final_structured_data.json\n",
            "[351] YT_GI_Colon_Cancer_Dhall.json\n",
            "[352] final_structured_data.json\n",
            "[353] YT_GI_Colorectal_Polyp_Pai.json\n",
            "[354] final_structured_data.json\n",
            "[355] YT_GI_Colorectal_Polyps_Pai.json\n",
            "[356] final_structured_data.json\n",
            "[357] YT_GI_Gastric_Polyps_Montgomery.json\n",
            "[358] final_structured_data.json\n",
            "[359] YT_GI_IBD_Gonzalez.json\n",
            "[360] final_structured_data.json\n",
            "[361] YT_GI_IBD_Related_Dysplasia.json\n",
            "[362] final_structured_data.json\n",
            "[363] YT_GI_Iatrogenic_Montgomery.json\n",
            "[364] final_structured_data.json\n",
            "[365] YT_GI_Luminal_Biopsy_Furth.json\n",
            "[366] final_structured_data.json\n",
            "[367] YT_GI_Lymphomas_Cruise.json\n",
            "[368] final_structured_data.json\n",
            "[369] YT_GI_Mesenchymal_Montgomery.json\n",
            "[370] final_structured_data.json\n",
            "[371] YT_GI_Non_IBD_Colitis_Allende_SLIDE_SESSION.json\n",
            "[372] final_structured_data.json\n",
            "[373] YT_GI_Pancreas_Neoplastic_Hruben.json\n",
            "[374] final_structured_data.json\n",
            "[375] YT_GI_Pancreas_Solid_Basturk.json\n",
            "[376] final_structured_data.json\n",
            "[377] YT_GI_Vascular_Allende.json\n",
            "[378] final_structured_data.json\n",
            "[379] YT_HN_Thyroid_Thompson.json\n",
            "[380] YT_HN_WHO_Classification_Thompson.json\n",
            "[381] YT_Micro_Bacteria1_Morgan.json\n",
            "[382] YT_Micro_Bacteria2_Morgan.json\n",
            "[383] YT_Micro_Mycobacteria_Morgan.json\n",
            "[384] YT_Micro_Mycology1_Morgan.json\n",
            "[385] YT_Micro_Mycology2_Morgan.json\n",
            "[386] YT_Micro_Parasites_Morgan.json\n",
            "[387] YT_Skin_B_Cell_Lymphoma_Cassarino.json\n",
            "[388] YT_Skin_Blistering_Fung.json\n",
            "[389] final_ENHANCED_data.json\n",
            "[390] final_structured_data.json\n",
            "[391] YT_Skin_Common_Disorders_Fung.json\n",
            "[392] final_ENHANCED_data.json\n",
            "[393] final_structured_data.json\n",
            "[394] YT_Skin_Granulomatous_Dermatitis_Fung.json\n",
            "[395] final_ENHANCED_data.json\n",
            "[396] final_structured_data.json\n",
            "[397] YT_Skin_IHC_Fung.json\n",
            "[398] final_ENHANCED_data.json\n",
            "[399] final_structured_data.json\n",
            "[400] YT_Skin_Interface_Dermatitis_Fung.json\n",
            "[401] final_ENHANCED_data.json\n",
            "[402] final_structured_data.json\n",
            "[403] YT_Skin_Intro_Fung.json\n",
            "[404] final_ENHANCED_data.json\n",
            "[405] final_structured_data.json\n",
            "[406] YT_Skin_Melanocytic_Fung.json\n",
            "[407] final_ENHANCED_data.json\n",
            "[408] final_structured_data.json\n",
            "[409] YT_Skin_Melanoma_LeBoit.json\n",
            "[410] YT_Skin_Metabolic_and_Connective_Tissue_Junkins.json\n",
            "[411] YT_Skin_Papulosquamous_and_Spongiotic_Dermatitis_Ko.json\n",
            "[412] YT_Skin_Perforating_Fung.json\n",
            "[413] final_ENHANCED_data.json\n",
            "[414] final_structured_data.json\n",
            "[415] YT_Skin_Perivascular_Dermatitis_Fung.json\n",
            "[416] final_ENHANCED_data.json\n",
            "[417] final_structured_data.json\n",
            "[418] YT_Skin_Psoriasiform_Dermatitis_Fung.json\n",
            "[419] final_ENHANCED_data.json\n",
            "[420] final_structured_data.json\n",
            "[421] YT_Skin_Soft_Tissue_Fung.json\n",
            "[422] final_ENHANCED_data.json\n",
            "[423] final_structured_data.json\n",
            "[424] YT_Skin_Spongiotic_Dermatitis_Fung.json\n",
            "[425] final_ENHANCED_data.json\n",
            "[426] final_structured_data.json\n",
            "[427] YT_Skin_T_Cell_Lymphoma_Cassarino.json\n",
            "[428] YT_Skin_Vacuolar_and_Lichenoid_Interface_Dermatitis_Junkins.json\n",
            "[429] YT_Skin_Vascular_Fung.json\n",
            "[430] final_ENHANCED_data.json\n",
            "[431] final_structured_data.json\n",
            "[432] YT_Skin_Vasculitis_Johnson.json\n",
            "[433] final_structured_data.json\n",
            "[434] YT_Skin_WHO_Classification_Scoyler.json\n",
            "[435] final_ENHANCED_data.json\n",
            "[436] final_structured_data.json\n",
            "\n",
            "Select (e.g. 1, 3-5, all): 388, 391, 394, 397, 400, 403, 406, 412, 415, 418, 421, 424, 429\n",
            "\n",
            "üöÄ Queued 13 lectures...\n",
            "\n",
            "üöÄ Processing: YT_Skin_Blistering_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Blistering_Fung_MASTER.json (19 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Common_Disorders_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Common_Disorders_Fung_MASTER.json (12 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Granulomatous_Dermatitis_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Granulomatous_Dermatitis_Fung_MASTER.json (11 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_IHC_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_IHC_Fung_MASTER.json (12 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Interface_Dermatitis_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Interface_Dermatitis_Fung_MASTER.json (16 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Intro_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Intro_Fung_MASTER.json (1 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Melanocytic_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Melanocytic_Fung_MASTER.json (26 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Perforating_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Perforating_Fung_MASTER.json (6 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Perivascular_Dermatitis_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Perivascular_Dermatitis_Fung_MASTER.json (22 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Psoriasiform_Dermatitis_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Psoriasiform_Dermatitis_Fung_MASTER.json (9 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Soft_Tissue_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Soft_Tissue_Fung_MASTER.json (26 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Spongiotic_Dermatitis_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Spongiotic_Dermatitis_Fung_MASTER.json (14 entities)\n",
            "\n",
            "üöÄ Processing: YT_Skin_Vascular_Fung\n",
            "   ‚úÖ Saved: _content_library/lectures/YT_Skin_Vascular_Fung_MASTER.json (21 entities)\n",
            "\n",
            "üéâ Batch Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLOCK 3: THE GRAND UNIFIER (Textbooks + Lectures + WSI)"
      ],
      "metadata": {
        "id": "9SOGk-37jeQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 3: THE GRAND UNIFIER (Textbooks + Lectures + WSI)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import os\n",
        "from google.cloud import storage\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OUTPUT_FILENAME = \"GLOBAL_KNOWLEDGE_BASE.json\"\n",
        "WSI_FOLDER_PREFIX = \"WSI_JSON/\"\n",
        "BLACKLIST = [\"Copyright\", \"Preface\", \"Contents\", \"Index\", \"Contributors\", \"Dedication\", \"Title Page\"]\n",
        "\n",
        "# --- SETUP ---\n",
        "if 'PATHS' not in globals():\n",
        "    raise NameError(\"‚ùå PATHS not found. Please run Block 0.\")\n",
        "bucket = storage.Client().bucket(PATHS['gcs_bucket'])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. TRANSFORMATION LOGIC (For Textbooks/Lectures)\n",
        "# ------------------------------------------------------------------------------\n",
        "def transform_raw_to_app_schema(entry, source_name, source_type):\n",
        "    \"\"\"\n",
        "    Converts raw pipeline output (related_figures) into App Schema (media).\n",
        "    \"\"\"\n",
        "    # 1. Filter Noise\n",
        "    if any(x in entry.get('entity_name', '') for x in BLACKLIST): return None\n",
        "\n",
        "    new_entry = entry.copy()\n",
        "\n",
        "    # 2. Build Media Array\n",
        "    media_list = []\n",
        "    if 'related_figures' in entry:\n",
        "        for fig in entry['related_figures']:\n",
        "            diag = fig.get('diagnosis', '').strip()\n",
        "            leg = fig.get('legend', '').strip()\n",
        "\n",
        "            # Combine for rich legend\n",
        "            if diag and leg and diag not in leg:\n",
        "                final_legend = f\"{diag}. {leg}\"\n",
        "            else:\n",
        "                final_legend = leg or diag\n",
        "\n",
        "            media_item = {\n",
        "                \"type\": \"figure\",\n",
        "                \"path\": fig.get('gcs_path') or fig.get('src'),\n",
        "                \"legend\": final_legend\n",
        "            }\n",
        "            media_list.append(media_item)\n",
        "        del new_entry['related_figures']\n",
        "\n",
        "    new_entry['media'] = media_list\n",
        "\n",
        "    # 3. Inject Metadata & Defaults\n",
        "    new_entry['_meta_source'] = source_name\n",
        "    new_entry['_meta_type'] = source_type\n",
        "\n",
        "    for k in [\"video\", \"html\", \"wsi\", \"definition\", \"clinical\", \"microscopic\"]:\n",
        "        if k not in new_entry: new_entry[k] = None\n",
        "\n",
        "    # 4. Clean Internal Keys\n",
        "    for k in ['html_gcs_path', 'gcs_origin', 'best_slide_id', 'source_document', 'source_type']:\n",
        "        if k in new_entry: del new_entry[k]\n",
        "\n",
        "    return new_entry\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. WSI LOADER (Already Formatted)\n",
        "# ------------------------------------------------------------------------------\n",
        "def load_app_ready_wsi(entry, source_name):\n",
        "    \"\"\"\n",
        "    Loads WSI entries that are ALREADY in App Schema.\n",
        "    Just injects metadata.\n",
        "    \"\"\"\n",
        "    # Just ensure metadata exists\n",
        "    entry['_meta_source'] = source_name\n",
        "    entry['_meta_type'] = \"WSI_Collection\"\n",
        "    return entry\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. FILE FINDERS\n",
        "# ------------------------------------------------------------------------------\n",
        "def get_best_pipeline_files(prefix):\n",
        "    \"\"\"Finds _CONSOLIDATED (preferred) or _MASTER files.\"\"\"\n",
        "    blobs = list(bucket.list_blobs(prefix=prefix))\n",
        "    files_map = {}\n",
        "\n",
        "    for b in blobs:\n",
        "        if not b.name.endswith(\".json\"): continue\n",
        "        fname = b.name.split('/')[-1]\n",
        "\n",
        "        if \"_MASTER\" not in fname and \"_CONSOLIDATED\" not in fname: continue\n",
        "\n",
        "        base = fname.replace(\"_MASTER.json\", \"\").replace(\"_CONSOLIDATED.json\", \"\")\n",
        "\n",
        "        # Priority: Consolidated > Master\n",
        "        if \"_CONSOLIDATED\" in fname:\n",
        "            files_map[base] = b.name\n",
        "        elif base not in files_map:\n",
        "            files_map[base] = b.name\n",
        "    return files_map\n",
        "\n",
        "def get_wsi_files():\n",
        "    \"\"\"Finds _APP_READY files in the WSI folder.\"\"\"\n",
        "    blobs = list(bucket.list_blobs(prefix=WSI_FOLDER_PREFIX))\n",
        "    return [b.name for b in blobs if b.name.endswith(\"_APP_READY.json\")]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MAIN EXECUTION\n",
        "# ------------------------------------------------------------------------------\n",
        "def main_unifier():\n",
        "    print(f\"üöÄ Starting Grand Unification...\")\n",
        "    global_kb = []\n",
        "\n",
        "    # --- A. TEXTBOOKS ---\n",
        "    print(\"üìö Scanning Textbooks...\")\n",
        "    books = get_best_pipeline_files(PATHS['gcs_content_textbooks'])\n",
        "\n",
        "    for name, path in tqdm(books.items(), desc=\"Textbooks\"):\n",
        "        try:\n",
        "            data = json.loads(bucket.blob(path).download_as_string())\n",
        "            for item in data:\n",
        "                res = transform_raw_to_app_schema(item, name, \"Textbook\")\n",
        "                if res: global_kb.append(res)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error in {name}: {e}\")\n",
        "\n",
        "    # --- B. LECTURES ---\n",
        "    print(\"üé• Scanning Lectures...\")\n",
        "    lectures = get_best_pipeline_files(PATHS['gcs_content_lectures'])\n",
        "\n",
        "    for name, path in tqdm(lectures.items(), desc=\"Lectures\"):\n",
        "        try:\n",
        "            data = json.loads(bucket.blob(path).download_as_string())\n",
        "            for item in data:\n",
        "                res = transform_raw_to_app_schema(item, name, \"Lecture\")\n",
        "                if res: global_kb.append(res)\n",
        "        except: pass\n",
        "\n",
        "    # --- C. WSI COLLECTIONS ---\n",
        "    print(\"üî¨ Scanning WSI Collections...\")\n",
        "    wsi_files = get_wsi_files()\n",
        "\n",
        "    for path in tqdm(wsi_files, desc=\"WSI\"):\n",
        "        try:\n",
        "            data = json.loads(bucket.blob(path).download_as_string())\n",
        "            name = path.split('/')[-1].replace(\"_APP_READY.json\", \"\")\n",
        "            for item in data:\n",
        "                # WSI files are already formatted, just add metadata\n",
        "                res = load_app_ready_wsi(item, name)\n",
        "                if res: global_kb.append(res)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error in {path}: {e}\")\n",
        "\n",
        "    # --- SAVE ---\n",
        "    print(f\"\\nüìä Total Entities: {len(global_kb)}\")\n",
        "    print(f\"üíæ Saving to gs://{PATHS['gcs_bucket']}/{OUTPUT_FILENAME}...\")\n",
        "\n",
        "    bucket.blob(OUTPUT_FILENAME).upload_from_string(\n",
        "        json.dumps(global_kb, indent=2),\n",
        "        content_type='application/json'\n",
        "    )\n",
        "    print(\"‚úÖ SUCCESS!\")\n",
        "\n",
        "main_unifier()"
      ],
      "metadata": {
        "id": "QmIgAoI8jbGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temp / Utility"
      ],
      "metadata": {
        "id": "VKxjvbPzuc5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLOCK 1: ROBUST WSI IMPORTER (Case-Insensitive & Spelling Aware)"
      ],
      "metadata": {
        "id": "kgh3Y71fkBHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 1: ROBUST WSI IMPORTER (Case-Insensitive & Spelling Aware)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import difflib\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gemini-3-flash-preview\"\n",
        "CONCURRENCY_LIMIT = 20\n",
        "SOURCE_FOLDER_PREFIX = \"WSI_JSON/\"\n",
        "\n",
        "# --- SETUP ---\n",
        "if 'PATHS' not in globals():\n",
        "    raise NameError(\"‚ùå PATHS not found. Please run Block 0.\")\n",
        "bucket = storage.Client().bucket(PATHS['gcs_bucket'])\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_load_json(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def gcs_read_text(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return blob.download_as_string().decode('utf-8') if blob.exists() else \"\"\n",
        "\n",
        "# --- KEY NORMALIZER (The Fix) ---\n",
        "def get_val_case_insensitive(data_dict, target_key):\n",
        "    \"\"\"Finds a key in a dict regardless of casing.\"\"\"\n",
        "    # Direct match\n",
        "    if target_key in data_dict: return data_dict[target_key]\n",
        "\n",
        "    # Lowercase match\n",
        "    target_lower = target_key.lower().replace(\" \", \"_\")\n",
        "    for k, v in data_dict.items():\n",
        "        k_norm = k.lower().replace(\" \", \"_\")\n",
        "        if k_norm == target_lower:\n",
        "            return v\n",
        "    return None\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. NORMALIZERS\n",
        "# ------------------------------------------------------------------------------\n",
        "def normalize_who_chapter(entry):\n",
        "    \"\"\"Handles WHO structure with case-insensitive key lookup.\"\"\"\n",
        "\n",
        "    # Map text fields\n",
        "    new_entry = {}\n",
        "    fields = [\n",
        "        \"entity_name\", \"definition\", \"clinical\", \"pathogenesis\",\n",
        "        \"macroscopic\", \"microscopic\", \"ancillary_studies\",\n",
        "        \"differential_diagnosis\", \"staging\", \"prognosis_and_prediction\",\n",
        "        \"cytology\", \"diagnostic_molecular_pathology\",\n",
        "        \"related_terminology\", \"subtypes\"\n",
        "    ]\n",
        "\n",
        "    for f in fields:\n",
        "        new_entry[f] = get_val_case_insensitive(entry, f)\n",
        "\n",
        "    # Process Figures\n",
        "    media_list = []\n",
        "    raw_figs = get_val_case_insensitive(entry, \"related_figures\") or []\n",
        "\n",
        "    for fig in raw_figs:\n",
        "        legend = get_val_case_insensitive(fig, \"legend\") or \"\"\n",
        "        diag = get_val_case_insensitive(fig, \"diagnosis\") or \"\"\n",
        "\n",
        "        if diag and diag not in legend:\n",
        "            legend = f\"{diag}. {legend}\"\n",
        "\n",
        "        media_item = {\"legend\": legend}\n",
        "\n",
        "        is_wsi = get_val_case_insensitive(fig, \"isWSI\")\n",
        "        fig_id = str(get_val_case_insensitive(fig, \"id\"))\n",
        "\n",
        "        if is_wsi is True:\n",
        "            media_item[\"type\"] = \"wsi\"\n",
        "            media_item[\"path\"] = f\"https://tumourclassification.iarc.who.int/static/dzi/{fig_id}_files/10/0_0.jpeg\"\n",
        "            media_item[\"url\"] = f\"https://tumourclassification.iarc.who.int/Viewer/Index2?fid={fig_id}\"\n",
        "        else:\n",
        "            media_item[\"type\"] = \"figure\"\n",
        "            src = get_val_case_insensitive(fig, \"src\")\n",
        "            gcs = get_val_case_insensitive(fig, \"gcs_path\")\n",
        "            media_item[\"path\"] = gcs or src\n",
        "            if not media_item[\"path\"]: continue\n",
        "\n",
        "        media_list.append(media_item)\n",
        "\n",
        "    new_entry['media'] = media_list\n",
        "    return new_entry\n",
        "\n",
        "def normalize_simple(entry, fmt):\n",
        "    \"\"\"Handles simple formats (Leeds, PP, etc).\"\"\"\n",
        "    # Try common keys for diagnosis\n",
        "    diag = get_val_case_insensitive(entry, \"Diagnosis\") or get_val_case_insensitive(entry, \"Title\")\n",
        "\n",
        "    # Try common keys for URL\n",
        "    url = get_val_case_insensitive(entry, \"URL\") or get_val_case_insensitive(entry, \"Link\")\n",
        "\n",
        "    # Try common keys for Thumbnail\n",
        "    thumb = get_val_case_insensitive(entry, \"Thumbnail\")\n",
        "    if fmt == \"PathPresenter\": thumb = None # Force null\n",
        "\n",
        "    return {\n",
        "        \"entity_name\": diag,\n",
        "        \"media\": [{\n",
        "            \"type\": \"wsi\",\n",
        "            \"path\": thumb,\n",
        "            \"url\": url,\n",
        "            \"legend\": diag\n",
        "        }]\n",
        "    }\n",
        "\n",
        "def detect_format(data_entry):\n",
        "    keys = [k.lower() for k in data_entry.keys()]\n",
        "    if \"related_figures\" in keys or \"microscopic\" in keys: return \"WHO\"\n",
        "    if \"diagnosis\" in keys or \"url\" in keys: return \"Simple\"\n",
        "    return \"Unknown\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. AI TAGGING\n",
        "# ------------------------------------------------------------------------------\n",
        "async def assign_tag_async(session, entity, valid_tags_text, valid_tags_set, sem):\n",
        "    async with sem:\n",
        "        diag = entity.get('entity_name')\n",
        "        if not diag:\n",
        "            entity['tags'] = [\"Skin::Unclassified\"]\n",
        "            return entity\n",
        "\n",
        "        # Check exact match first\n",
        "        if diag in valid_tags_set:\n",
        "            entity['tags'] = [diag]\n",
        "            return entity\n",
        "\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Role: Pathology Taxonomist.\n",
        "        Task: Map the diagnosis \"{diag}\" to the best tag from the list.\n",
        "\n",
        "        RULES:\n",
        "        1. \"Naevus\" = \"Nevus\".\n",
        "        2. Ignore \"(HE)\", \"(Actin)\", or case numbers.\n",
        "        3. Return ONLY the tag string from the list.\n",
        "\n",
        "        VALID TAGS:\n",
        "        {valid_tags_text}\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(3): # Retry logic\n",
        "            try:\n",
        "                async with session.post(url, json={\"contents\": [{\"parts\": [{\"text\": prompt}]}]}) as res:\n",
        "                    if res.status == 200:\n",
        "                        dat = await res.json()\n",
        "                        tag = dat['candidates'][0]['content']['parts'][0]['text'].strip()\n",
        "\n",
        "                        if tag in valid_tags_set:\n",
        "                            entity['tags'] = [tag]\n",
        "                        else:\n",
        "                            # Fallback fuzzy match\n",
        "                            matches = difflib.get_close_matches(tag, list(valid_tags_set), n=1, cutoff=0.7)\n",
        "                            entity['tags'] = [matches[0]] if matches else [\"Skin::Unclassified\"]\n",
        "                        return entity\n",
        "                    elif res.status == 429:\n",
        "                        await asyncio.sleep(2)\n",
        "            except: pass\n",
        "\n",
        "        entity['tags'] = [\"Skin::Unclassified\"]\n",
        "        return entity\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. EXECUTION\n",
        "# ------------------------------------------------------------------------------\n",
        "async def main_wsi_robust():\n",
        "    # 1. Tags\n",
        "    tag_files = [b.name for b in bucket.list_blobs(prefix=\"Tags/\") if b.name.endswith('.txt')]\n",
        "    print(\"\\n--- SELECT TAG LIST ---\")\n",
        "    for i, f in enumerate(tag_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    t_idx = int(input(\"Choice: \")) - 1\n",
        "    tags_text = gcs_read_text(tag_files[t_idx])\n",
        "    tags_set = set(l.strip() for l in tags_text.splitlines() if l.strip())\n",
        "\n",
        "    # 2. Files\n",
        "    print(f\"\\nüîç Scanning {SOURCE_FOLDER_PREFIX} ...\")\n",
        "    blobs = list(bucket.list_blobs(prefix=SOURCE_FOLDER_PREFIX))\n",
        "    candidates = [b.name for b in blobs if b.name.endswith(\".json\") and \"_APP_READY1\" not in b.name]\n",
        "\n",
        "    if not candidates: print(\"‚ùå No files found.\"); return\n",
        "\n",
        "    print(\"\\n--- AVAILABLE FILES ---\")\n",
        "    for i, f in enumerate(candidates): print(f\"[{i+1}] {f}\")\n",
        "\n",
        "    sel = input(\"\\nSelect files (e.g. 1, 3-5, all): \").strip().lower()\n",
        "    indices = set(range(len(candidates))) if sel == 'all' else {int(x)-1 for x in re.findall(r'\\d+', sel)}\n",
        "\n",
        "    # 3. Process\n",
        "    for idx in sorted(list(indices)):\n",
        "        if idx < 0 or idx >= len(candidates): continue\n",
        "        src_blob = candidates[idx]\n",
        "\n",
        "        print(f\"\\n{'='*50}\\nüöÄ Processing: {src_blob}\")\n",
        "        raw_data = gcs_load_json(src_blob)\n",
        "\n",
        "        if not raw_data: print(\"‚ö†Ô∏è Empty file.\"); continue\n",
        "\n",
        "        # Debug: Print keys of first entry\n",
        "        print(f\"   Sample Keys: {list(raw_data[0].keys())}\")\n",
        "\n",
        "        # Detect Format\n",
        "        fmt = detect_format(raw_data[0])\n",
        "        print(f\"   Detected Format: {fmt}\")\n",
        "\n",
        "        normalized_data = []\n",
        "        for item in raw_data:\n",
        "            if fmt == \"WHO\": norm = normalize_who_chapter(item)\n",
        "            else: norm = normalize_simple(item, fmt)\n",
        "\n",
        "            # Fill schema nulls\n",
        "            for k in [\"video\", \"html\", \"wsi\", \"definition\", \"clinical\", \"microscopic\"]:\n",
        "                if k not in norm: norm[k] = None\n",
        "            normalized_data.append(norm)\n",
        "\n",
        "        # Tagging\n",
        "        print(f\"üß† Tagging {len(normalized_data)} entities...\")\n",
        "        sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
        "        final_data = []\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = [assign_tag_async(session, ent, tags_text, tags_set, sem) for ent in normalized_data]\n",
        "            final_data = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "        # Save\n",
        "        out_name = f\"{src_blob.replace('.json', '_APP_READY.json')}\"\n",
        "        gcs_upload_json(final_data, out_name)\n",
        "\n",
        "        valid = sum(1 for e in final_data if \"Unclassified\" not in e['tags'][0])\n",
        "        print(f\"‚úÖ Saved: {out_name} (Tagged: {valid}/{len(final_data)})\")\n",
        "\n",
        "await main_wsi_robust()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBZdLjPqj6ra",
        "outputId": "738795bc-74f6-4d81-ca0f-50391a2ff325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SELECT TAG LIST ---\n",
            "[1] BST_Tags.txt\n",
            "[2] Breast_Tags.txt\n",
            "[3] Endo_Tags.txt\n",
            "[4] GI_Tags.txt\n",
            "[5] GYN_Tags.txt\n",
            "[6] Skin_Tags.txt\n",
            "Choice: 6\n",
            "\n",
            "üîç Scanning WSI_JSON/ ...\n",
            "\n",
            "--- AVAILABLE FILES ---\n",
            "[1] WSI_JSON/Leeds_WSI_Skin.json\n",
            "[2] WSI_JSON/PP_Skin_1-500.json\n",
            "[3] WSI_JSON/PP_Skin_1-500_APP_READY.json\n",
            "[4] WSI_JSON/PP_Skin_1001-1500.json\n",
            "[5] WSI_JSON/PP_Skin_1001-1500_APP_READY.json\n",
            "[6] WSI_JSON/PP_Skin_1501-2000.json\n",
            "[7] WSI_JSON/PP_Skin_1501-2000_APP_READY.json\n",
            "[8] WSI_JSON/PP_Skin_2001-2500.json\n",
            "[9] WSI_JSON/PP_Skin_2001-2500_APP_READY.json\n",
            "[10] WSI_JSON/PP_Skin_2501-3000.json\n",
            "[11] WSI_JSON/PP_Skin_2501-3000_APP_READY.json\n",
            "[12] WSI_JSON/PP_Skin_3001-3500.json\n",
            "[13] WSI_JSON/PP_Skin_3001-3500_APP_READY.json\n",
            "[14] WSI_JSON/PP_Skin_3501-4000.json\n",
            "[15] WSI_JSON/PP_Skin_3501-4000_APP_READY.json\n",
            "[16] WSI_JSON/PP_Skin_4001-4500.json\n",
            "[17] WSI_JSON/PP_Skin_4001-4500_APP_READY.json\n",
            "[18] WSI_JSON/PP_Skin_4501-5000.json\n",
            "[19] WSI_JSON/PP_Skin_4501-5000_APP_READY.json\n",
            "[20] WSI_JSON/PP_Skin_5001-6000.json\n",
            "[21] WSI_JSON/PP_Skin_5001-6000_APP_READY.json\n",
            "[22] WSI_JSON/PP_Skin_501-1000.json\n",
            "[23] WSI_JSON/PP_Skin_501-1000_APP_READY.json\n",
            "[24] WSI_JSON/Rosai_Skin_Links.json\n",
            "[25] WSI_JSON/Rosai_Skin_Links_APP_READY.json\n",
            "[26] WSI_JSON/SKIN.json\n",
            "[27] WSI_JSON/SKIN_APP_READY.json\n",
            "[28] WSI_JSON/SKIN_APP_READY_APP_READY.json\n",
            "[29] WSI_JSON/Skin_MGH_Links.json\n",
            "[30] WSI_JSON/Skin_MGH_Links_APP_READY.json\n",
            "\n",
            "Select files (e.g. 1, 3-5, all): 26\n",
            "\n",
            "==================================================\n",
            "üöÄ Processing: WSI_JSON/SKIN.json\n",
            "   Sample Keys: ['entity_name', 'definition', 'related_terminology', 'subtypes', 'localization', 'clinical', 'pathogenesis', 'macroscopic', 'microscopic', 'ancillary_studies', 'differential_diagnosis', 'cytology', 'diagnostic_molecular_pathology', 'essential_and_desirable_diagnostic_criteria', 'staging', 'prognosis_and_prediction', 'related_figures', 'tags', 'html_gcs_path']\n",
            "   Detected Format: WHO\n",
            "üß† Tagging 224 entities...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224/224 [01:44<00:00,  2.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved: WSI_JSON/SKIN_APP_READY.json (Tagged: 74/224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üõ†Ô∏è Block 1: Single Tag Enforcer (Local Input: Skin_Tags.txt)\n",
        "import json\n",
        "import re\n",
        "import difflib\n",
        "import os\n",
        "from typing import List, Any, Set\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "BUCKET_NAME = \"pathology-hub-0\"\n",
        "\n",
        "# 1. Source of VALID TAGS (Cloud)\n",
        "TAGS_GCS_PATH = \"Tags/Skin_Tags.txt\"\n",
        "\n",
        "# 2. Source of ENTITIES (Local File uploaded to Colab)\n",
        "INPUT_FILENAME = \"Skin_Tags.txt\"\n",
        "\n",
        "# 3. Output Destination (Cloud)\n",
        "GCS_OUTPUT_PATH = \"WHO/WHO_JSON_PROCESSED/SKIN.json\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DOWNLOAD VALID TAGS FROM GCS\n",
        "# ==============================================================================\n",
        "def get_tags_from_gcs(bucket_name: str, blob_path: str) -> Set[str]:\n",
        "    print(f\"‚òÅÔ∏è Downloading Master Tag List from: gs://{bucket_name}/{blob_path} ...\")\n",
        "    try:\n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_path)\n",
        "        content = blob.download_as_string().decode('utf-8')\n",
        "        # Build set of valid tags\n",
        "        valid_tags = set(line.strip() for line in content.splitlines() if line.strip())\n",
        "        return valid_tags\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error downloading tags: {e}\")\n",
        "        return set()\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. PARSE LOCAL ENTITIES (JSON)\n",
        "# ==============================================================================\n",
        "def parse_local_entities(filepath: str) -> List[dict]:\n",
        "    print(f\"üìÇ Reading local file: {filepath} ...\")\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Attempt to find JSON block if file has mixed content\n",
        "    json_match = re.search(\n",
        "        r'--- START OF FILE application/json ---\\n(.*)',\n",
        "        content,\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    if json_match:\n",
        "        json_text = json_match.group(1)\n",
        "        json_text = re.sub(r'\\n---.*', '', json_text) # Remove footer if present\n",
        "    else:\n",
        "        # Assume whole file is JSON\n",
        "        json_text = content\n",
        "\n",
        "    try:\n",
        "        data = json.loads(json_text)\n",
        "        return data\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå JSON Decode Error: {e}\")\n",
        "        # Soft fix for cut-off files\n",
        "        try:\n",
        "            print(\"   Attempting auto-fix...\")\n",
        "            data = json.loads(json_text + \"]\")\n",
        "            return data\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. TAGGING LOGIC (Force 1 Tag)\n",
        "# ==============================================================================\n",
        "def resolve_single_tag(tag_input: Any, valid_set: Set[str], entity_name: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Returns exactly ONE tag: [\"Category::Subcategory::Specific\"]\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "\n",
        "    # Normalize input to list of strings\n",
        "    if isinstance(tag_input, str):\n",
        "        candidates = [tag_input]\n",
        "    elif isinstance(tag_input, list):\n",
        "        candidates = [str(t) for t in tag_input]\n",
        "\n",
        "    # Strategy 1: Look for Exact Match in existing tags\n",
        "    for tag in candidates:\n",
        "        clean = tag.strip()\n",
        "        if clean in valid_set:\n",
        "            return [clean]\n",
        "\n",
        "    # Strategy 2: Look for Fuzzy Match in existing tags\n",
        "    for tag in candidates:\n",
        "        clean = tag.strip()\n",
        "        matches = difflib.get_close_matches(clean, list(valid_set), n=1, cutoff=0.85)\n",
        "        if matches:\n",
        "            return [matches[0]]\n",
        "\n",
        "    # Strategy 3: Match Entity Name to Tag List\n",
        "    # e.g., Entity: \"Basal cell carcinoma\" -> Tag: \"Skin::...::Basal_Cell_Carcinoma\"\n",
        "    if entity_name:\n",
        "        name_clean = entity_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
        "\n",
        "        # A. Containment match (Case insensitive)\n",
        "        # Find tags that contain the entity name\n",
        "        possible_matches = [t for t in valid_set if name_clean.lower() in t.lower()]\n",
        "        if possible_matches:\n",
        "            # Sort by length; usually the shortest containing tag is the specific category header\n",
        "            # or the longest might be the specific disease.\n",
        "            # We pick the one with best text overlap.\n",
        "            possible_matches.sort(key=lambda x: difflib.SequenceMatcher(None, name_clean.lower(), x.lower()).ratio(), reverse=True)\n",
        "            return [possible_matches[0]]\n",
        "\n",
        "        # B. Fuzzy match against all tags\n",
        "        matches = difflib.get_close_matches(name_clean, list(valid_set), n=1, cutoff=0.6)\n",
        "        if matches:\n",
        "            return [matches[0]]\n",
        "\n",
        "    return [\"Skin::Unclassified\"]\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MAIN\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    # 1. Auth\n",
        "    print(\"üîë Authenticating...\")\n",
        "    try:\n",
        "        auth.authenticate_user()\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Skipped auth\")\n",
        "\n",
        "    # 2. Get Tags from GCS\n",
        "    valid_tags = get_tags_from_gcs(BUCKET_NAME, TAGS_GCS_PATH)\n",
        "    if not valid_tags:\n",
        "        print(\"‚ùå ABORT: No tags found in GCS.\")\n",
        "        return\n",
        "    print(f\"   ‚úÖ Loaded {len(valid_tags)} valid tags.\")\n",
        "\n",
        "    # 3. Read Local JSON\n",
        "    if not os.path.exists(INPUT_FILENAME):\n",
        "        print(f\"‚ùå ABORT: Local file '{INPUT_FILENAME}' not found. Please upload it.\")\n",
        "        return\n",
        "\n",
        "    entities = parse_local_entities(INPUT_FILENAME)\n",
        "    print(f\"   ‚úÖ Loaded {len(entities)} entities.\")\n",
        "\n",
        "    # 4. Clean & Tag\n",
        "    cleaned_kb = []\n",
        "    keys_to_keep = [\n",
        "        \"entity_name\", \"definition\", \"localization\", \"clinical\",\n",
        "        \"pathogenesis\", \"macroscopic\", \"microscopic\", \"cytology\",\n",
        "        \"ancillary_studies\", \"differential_diagnosis\", \"staging\",\n",
        "        \"prognosis_and_prediction\", \"diagnostic_molecular_pathology\",\n",
        "        \"essential_and_desirable_diagnostic_criteria\", \"related_figures\"\n",
        "    ]\n",
        "\n",
        "    for ent in entities:\n",
        "        if not ent.get('entity_name'): continue\n",
        "\n",
        "        # Fix Tag\n",
        "        ent['tags'] = resolve_single_tag(\n",
        "            ent.get('tags', []),\n",
        "            valid_tags,\n",
        "            ent.get('entity_name', '')\n",
        "        )\n",
        "\n",
        "        # Enforce Keys\n",
        "        for k in keys_to_keep:\n",
        "            if k not in ent: ent[k] = None\n",
        "        ent['html_gcs_path'] = ent.get('html_gcs_path', None)\n",
        "\n",
        "        cleaned_kb.append(ent)\n",
        "\n",
        "    # 5. Upload Result\n",
        "    print(f\"\\nüöÄ Uploading {len(cleaned_kb)} cleaned entities to GCS...\")\n",
        "    try:\n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob(GCS_OUTPUT_PATH)\n",
        "        blob.upload_from_string(\n",
        "            json.dumps(cleaned_kb, indent=2, ensure_ascii=False),\n",
        "            content_type='application/json'\n",
        "        )\n",
        "        print(f\"‚úÖ DONE: gs://{BUCKET_NAME}/{GCS_OUTPUT_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Upload Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFWIpGuHuitI",
        "outputId": "ba69b066-9f37-48a1-bea8-1d3ac91b796f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Authenticating...\n",
            "‚òÅÔ∏è Downloading Master Tag List from: gs://pathology-hub-0/Tags/Skin_Tags.txt ...\n",
            "   ‚úÖ Loaded 723 valid tags.\n",
            "‚ùå ABORT: Local file 'Skin_Tags.txt' not found. Please upload it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üöÄ Block 1: Local File Processor (Gemini 3 Flash Preview)\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "from typing import List\n",
        "from google.colab import auth\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig, HarmCategory, HarmBlockThreshold\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "# ‚ö†Ô∏è Your Project ID\n",
        "PROJECT_ID = \"pathology-annotation-project\"\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "# ‚úÖ CORRECT MODEL ID (Released Dec 17, 2025)\n",
        "MODEL_ID = \"gemini-3-flash-preview\"\n",
        "\n",
        "# Local Files (Upload these to the \"Files\" folder on the left)\n",
        "LOCAL_TAGS_FILE = \"Skin_Tags.txt\"\n",
        "LOCAL_ENTITIES_FILE = \"SKIN.json\"\n",
        "LOCAL_OUTPUT_FILE = \"SKIN_Cleaned_Gemini3.json\"\n",
        "\n",
        "# ==============================================================================\n",
        "# AI PROCESSING LOGIC\n",
        "# ==============================================================================\n",
        "def map_tags_with_gemini_3(entities: List[dict], valid_tags_text: str):\n",
        "    print(f\"üîå Connecting to Vertex AI...\")\n",
        "    print(f\"   ‚Ä¢ Project: {PROJECT_ID}\")\n",
        "    print(f\"   ‚Ä¢ Region:  {LOCATION}\")\n",
        "    print(f\"   ‚Ä¢ Model:   {MODEL_ID}\")\n",
        "\n",
        "    try:\n",
        "        vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "        # Initialize Gemini 3\n",
        "        model = GenerativeModel(MODEL_ID)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå FATAL ERROR connecting to Vertex AI: {e}\")\n",
        "        return []\n",
        "\n",
        "    # Safety settings: BLOCK_NONE is critical for medical pathology terms\n",
        "    safety_settings = {\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    }\n",
        "\n",
        "    # Gemini 3 Flash has a massive context window (1M tokens).\n",
        "    # We can process large batches (e.g., 50 entities) safely.\n",
        "    BATCH_SIZE = 50\n",
        "    total = len(entities)\n",
        "    updated_entities = []\n",
        "\n",
        "    print(f\"üöÄ Processing {total} entities using {MODEL_ID}...\")\n",
        "\n",
        "    for i in range(0, total, BATCH_SIZE):\n",
        "        batch = entities[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Build prompt context\n",
        "        entities_str = \"\"\n",
        "        for idx, ent in enumerate(batch):\n",
        "            name = ent.get('entity_name', 'Unknown')\n",
        "            # Truncate definition slightly to save tokens, though Gemini 3 handles it easily\n",
        "            defin = ent.get('definition', '')[:500]\n",
        "            entities_str += f\"ITEM_{idx}: {name}\\nDEFINITION: {defin}\\n\\n\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Role: Senior Dermatopathologist & Data Steward.\n",
        "        Objective: Map specific skin disease entities to the standard WHO Classification Tags.\n",
        "\n",
        "        --- MASTER TAG LIST START ---\n",
        "        {valid_tags_text}\n",
        "        --- MASTER TAG LIST END ---\n",
        "\n",
        "        --- ENTITIES TO MAP ---\n",
        "        {entities_str}\n",
        "\n",
        "        INSTRUCTIONS:\n",
        "        1. For each entity, select the ONE most accurate tag from the Master List.\n",
        "        2. If the entity name implies a specific subtype (e.g., \"Sarcomatoid SCC\") and a specific tag exists (\"...Squamous_Cell_Carcinoma_Sarcomatoid\"), USE IT.\n",
        "        3. If no specific tag exists, find the closest parent tag.\n",
        "        4. Return a JSON list of strings (tags) in exactly the same order as the input items.\n",
        "\n",
        "        OUTPUT FORMAT:\n",
        "        [\"Tag_1\", \"Tag_2\", \"Tag_3\", ...]\n",
        "        \"\"\"\n",
        "\n",
        "        # Retry logic\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = model.generate_content(\n",
        "                    prompt,\n",
        "                    generation_config=GenerationConfig(\n",
        "                        temperature=0.0, # Deterministic for data cleaning\n",
        "                        response_mime_type=\"application/json\"\n",
        "                    ),\n",
        "                    safety_settings=safety_settings\n",
        "                )\n",
        "\n",
        "                tags_list = json.loads(response.text)\n",
        "\n",
        "                if len(tags_list) != len(batch):\n",
        "                    print(f\"‚ö†Ô∏è Batch {i}: Count mismatch ({len(tags_list)} tags vs {len(batch)} items). Retrying...\")\n",
        "                    raise ValueError(\"Count mismatch\")\n",
        "\n",
        "                # Apply tags\n",
        "                for ent, new_tag in zip(batch, tags_list):\n",
        "                    ent['tags'] = [new_tag]\n",
        "                    print(f\"   [{ent['entity_name'][:30]:<30}] -> {new_tag}\")\n",
        "\n",
        "                updated_entities.extend(batch)\n",
        "                break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error Batch {i} (Attempt {attempt+1}): {e}\")\n",
        "\n",
        "                # Check for the specific 403 that caused the previous issue\n",
        "                if \"403\" in str(e) or \"PERMISSION_DENIED\" in str(e):\n",
        "                    print(f\"\\nüõë PERMISSION ERROR DETECTED for {MODEL_ID}\")\n",
        "                    print(\"   1. Go to Google Cloud Console > Vertex AI > Model Garden\")\n",
        "                    print(\"   2. Search for 'Gemini 3 Flash'\")\n",
        "                    print(\"   3. Click 'ENABLE' to accept the Preview terms.\")\n",
        "                    # Stop script here so you can go fix it\n",
        "                    return updated_entities\n",
        "\n",
        "                time.sleep(2)\n",
        "\n",
        "    return updated_entities\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    # 1. Auth\n",
        "    print(\"üîë Authenticating...\")\n",
        "    try:\n",
        "        auth.authenticate_user()\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Authentication skipped (Local runtime?).\")\n",
        "\n",
        "    # 2. Check Files\n",
        "    if not os.path.exists(LOCAL_TAGS_FILE):\n",
        "        print(f\"‚ùå Error: '{LOCAL_TAGS_FILE}' not found. Please upload it.\")\n",
        "        return\n",
        "    if not os.path.exists(LOCAL_ENTITIES_FILE):\n",
        "        print(f\"‚ùå Error: '{LOCAL_ENTITIES_FILE}' not found. Please upload it.\")\n",
        "        return\n",
        "\n",
        "    # 3. Read Data\n",
        "    print(\"üìÇ Reading local input files...\")\n",
        "    with open(LOCAL_TAGS_FILE, 'r', encoding='utf-8') as f:\n",
        "        tags_text = f.read()\n",
        "\n",
        "    with open(LOCAL_ENTITIES_FILE, 'r', encoding='utf-8') as f:\n",
        "        # Robust load: handle if file has weird headers or just raw JSON\n",
        "        content = f.read()\n",
        "        try:\n",
        "            entities = json.loads(content)\n",
        "        except json.JSONDecodeError:\n",
        "            # Fallback: try to extract JSON if it was pasted with headers\n",
        "            import re\n",
        "            match = re.search(r'--- START OF FILE application/json ---\\n(.*)', content, re.DOTALL)\n",
        "            if match:\n",
        "                clean = re.sub(r'\\n---.*', '', match.group(1))\n",
        "                entities = json.loads(clean)\n",
        "            else:\n",
        "                print(\"‚ùå Failed to parse JSON file.\")\n",
        "                return\n",
        "\n",
        "    # 4. Run AI\n",
        "    final_data = map_tags_with_gemini_3(entities, tags_text)\n",
        "\n",
        "    # 5. Save Output\n",
        "    if final_data:\n",
        "        with open(LOCAL_OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
        "            json.dump(final_data, f, indent=2)\n",
        "        print(f\"\\nüíæ Success! Saved to: {LOCAL_OUTPUT_FILE}\")\n",
        "        print(\"   (Download this file from the left sidebar)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcqxB7Z9vjP6",
        "outputId": "dad1395f-2225-45eb-b7ba-5c8307ec1173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Authenticating...\n",
            "üìÇ Reading local input files...\n",
            "üîå Connecting to Vertex AI...\n",
            "   ‚Ä¢ Project: pathology-annotation-project\n",
            "   ‚Ä¢ Region:  us-central1\n",
            "   ‚Ä¢ Model:   gemini-3-flash-preview\n",
            "üöÄ Processing 210 entities using gemini-3-flash-preview...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚ùå Error Batch 0 (Attempt 1): 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/pathology-annotation-project/locations/us-central1/publishers/google/models/gemini-3-flash-preview' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"\n",
            "domain: \"aiplatform.googleapis.com\"\n",
            "metadata {\n",
            "  key: \"resource\"\n",
            "  value: \"projects/pathology-annotation-project/locations/us-central1/publishers/google/models/gemini-3-flash-preview\"\n",
            "}\n",
            "metadata {\n",
            "  key: \"permission\"\n",
            "  value: \"aiplatform.endpoints.predict\"\n",
            "}\n",
            "]\n",
            "\n",
            "üõë PERMISSION ERROR DETECTED for gemini-3-flash-preview\n",
            "   1. Go to Google Cloud Console > Vertex AI > Model Garden\n",
            "   2. Search for 'Gemini 3 Flash'\n",
            "   3. Click 'ENABLE' to accept the Preview terms.\n"
          ]
        }
      ]
    }
  ]
}