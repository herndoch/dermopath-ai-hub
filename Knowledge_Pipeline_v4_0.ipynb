{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b15272cc1eac4263bd2483ff224f309b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad2605415bc54648a036a3960216e73a",
              "IPY_MODEL_2964d4df97b4486d8ef92bbaa49e6509",
              "IPY_MODEL_49dbffde97e14a3d8524f4ba2b15c26b"
            ],
            "layout": "IPY_MODEL_6c3c4935f50a469088628949c710c1ee"
          }
        },
        "ad2605415bc54648a036a3960216e73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89cb58fbed1f40bebdb05b85e549d3ae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_74564169a8614db9b69759b7fec2faab",
            "value": "Textbooks:‚Äá100%"
          }
        },
        "2964d4df97b4486d8ef92bbaa49e6509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e25dc7146b64ffa9c760d76b648211a",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51fd7cbf60fb405cbaf0a3676535558f",
            "value": 6
          }
        },
        "49dbffde97e14a3d8524f4ba2b15c26b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f802a0cd5d32461e8a539316c4251991",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e1e67da2a0a444ebb588ba34fa8a475f",
            "value": "‚Äá6/6‚Äá[00:01&lt;00:00,‚Äá‚Äá4.98it/s]"
          }
        },
        "6c3c4935f50a469088628949c710c1ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89cb58fbed1f40bebdb05b85e549d3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74564169a8614db9b69759b7fec2faab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e25dc7146b64ffa9c760d76b648211a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51fd7cbf60fb405cbaf0a3676535558f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f802a0cd5d32461e8a539316c4251991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1e67da2a0a444ebb588ba34fa8a475f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e42abf5e76b4488285a620d622401837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5357d7ef1acf4020b76bcf664f3f904a",
              "IPY_MODEL_c3651d969d0e4a9e9f22a94ec8a08dcb",
              "IPY_MODEL_048db4e0d8914a5a83d9db9c6ee51e73"
            ],
            "layout": "IPY_MODEL_e4250cb715ba4a16ac7c63cf7079c8f1"
          }
        },
        "5357d7ef1acf4020b76bcf664f3f904a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8c5c92a699e4dfebc1316a59cddce73",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5711233e32c842f296f66b3660686f71",
            "value": "Lectures:‚Äá100%"
          }
        },
        "c3651d969d0e4a9e9f22a94ec8a08dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8de74449c1dd47ffa184ab64f3829bbb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edaf2e3b92dd4e3197586de503bdc060",
            "value": 1
          }
        },
        "048db4e0d8914a5a83d9db9c6ee51e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7162f37c70384ffab571d7bcd9a62ce2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4fd8168fbe3b47d39cab55ae8c54e369",
            "value": "‚Äá1/1‚Äá[00:00&lt;00:00,‚Äá‚Äá9.26it/s]"
          }
        },
        "e4250cb715ba4a16ac7c63cf7079c8f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8c5c92a699e4dfebc1316a59cddce73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5711233e32c842f296f66b3660686f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8de74449c1dd47ffa184ab64f3829bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edaf2e3b92dd4e3197586de503bdc060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7162f37c70384ffab571d7bcd9a62ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fd8168fbe3b47d39cab55ae8c54e369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/herndoch/dermopath-ai-hub/blob/main/Knowledge_Pipeline_v4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Me\n",
        "<details>\n",
        "<summary><strong>ü§ñ AI-to-AI Handover Protocol (Click to Expand)</strong></summary>\n",
        "\n",
        "# ü§ñ AI-to-AI Handover Protocol (Read First)\n",
        "\n",
        "**‚ö†Ô∏è CRITICAL SYSTEM INVARIANTS**\n",
        "*Do not modify these settings without explicit human authorization. These constraints exist to prevent known failure modes.*\n",
        "\n",
        "### 1. Model Hierarchy & Reasoning\n",
        "*   **The \"Monolith\" Rule:** Architect Blocks (PDF Block 2 & Video Block 2) **MUST** utilize `gemini-1.5-pro-002` (or `gemini-3-pro-preview` if available).\n",
        "    *   *Why:* We tested Flash; it hallucinates summaries and omits specific IHC stains (e.g., summarizing \"CD45+, S100-\" as just \"ruled out melanoma\"). Only Pro models maintain the high fidelity required for medical RAG.\n",
        "*   **The \"Flash\" Rule:** Extraction and Consolidation (Block 1 & 3) **MUST** use `gemini-3-flash-preview`.\n",
        "    *   *Why:* Pro models have strict Rate Limits (RPM). Using Pro for simple text merging or per-page extraction causes 429 loops and crashes.\n",
        "\n",
        "### 2. Data Integrity Constraints\n",
        "*   **The \"Menu\" Method (PDFs):** Never allow the AI to hallucinate image paths.\n",
        "    *   *Invariant:* PDF Block 2 prompts **MUST** utilize the pre-validated \"Figure Menu\" generated by Block 1. If an image isn't in the menu, it does not exist.\n",
        "*   **The \"Chain of Custody\" (Videos):**\n",
        "    *   *Invariant:* Video Block 1 generates the `gs://` link. Video Block 2 must be instructed to copy that specific link field, not invent a filename.\n",
        "*   **Zero-Loss Merging:** Block 3 (Consolidator) is purely additive.\n",
        "    *   *Invariant:* When merging fragmented entities (e.g., \"Lichen Planus\" from Page 40 and Page 400), the AI must **concatenate** facts, never overwrite or summarize them away.\n",
        "\n",
        "### 3. Operational Limits (The \"Sweet Spots\")\n",
        "*   **Textbook Chunk Size:** `40 pages` (with `2 page overlap`).\n",
        "    *   *Why:* >50 pages triggers HTTP timeouts. <30 pages loses context. 40 is the empirically tested maximum for stability.\n",
        "*   **Pro Concurrency:** `Limit = 2`.\n",
        "    *   *Why:* `gemini-1.5-pro` allows fewer concurrent requests than Flash. Increasing this >2 results in immediate 429 throttling.\n",
        "*   **Flash Concurrency:** `Limit = 15-20`.\n",
        "    *   *Why:* Safe zone for high-throughput image extraction.\n",
        "\n",
        "### 4. File Format Logic\n",
        "*   **Textbooks = PNG:** Lossless quality is required for OCR to correctly read tiny font sizes in medical diagrams.\n",
        "*   **Lectures = JPG:** Compression is required to handle the volume (100+ slides per hour) without exploding storage costs.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Recovery Playbook (If Execution Fails)\n",
        "*   **IF `429 Resource Exhausted`:** Do not restart. The script has auto-resume logic. Wait 60s and re-run.\n",
        "*   **IF `Content-Generation Timeout`:** The Chunk Size is too large for the current model latency. Reduce `PAGES_PER_CHUNK` from 40 to 30.\n",
        "*   **IF `KeyError: 'gcs_content_textbooks'`:** The environment is fresh. Run **Block 0** to re-initialize the `PATHS` map.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>üß¨ Pathology Knowledge Base Pipeline (SOP) (Click to Expand)</strong></summary>\n",
        "\n",
        "**System Version:** v5.0 (High-Fidelity / Monolithic Architecture)\n",
        "**Engine:** Google Gemini (1.5 Pro / 3 Flash)\n",
        "**Infrastructure:** Google Colab $\\leftrightarrow$ Google Cloud Storage (GCS)\n",
        "\n",
        "## üìã Overview\n",
        "This pipeline converts unstructured medical data (Textbooks and Video Lectures) into a strictly standardized, ontology-tagged JSON Knowledge Base. It uses a **\"Monolith\"** approach for reasoning (processing large contexts at once) and a **\"Map-Reduce\"** approach for consolidation.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Block 0: Universal Setup\n",
        "**Status:** ‚úÖ Mandatory (Run once per session)\n",
        "\n",
        "This block installs dependencies, authenticates with Google Cloud, and establishes the global directory map (`PATHS`) to prevent file-not-found errors.\n",
        "\n",
        "*   **Inputs:** None (requires Google Drive mount).\n",
        "*   **Actions:**\n",
        "    *   Installs `PyMuPDF` (PDFs), `openai-whisper` (Audio), `opencv` (Video), `aiohttp` (Async API).\n",
        "    *   Authenticates via Colab Secrets (`GEMINI_API_KEY`).\n",
        "    *   Sets up GCS Bucket paths.\n",
        "*   **Key Variable:** `PATHS` dictionary (Routes data for both Textbooks and Lectures).\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Workflow A: Textbooks (PDF)\n",
        "\n",
        "### Block 1: The Extractor\n",
        "**Goal:** Raw Data Acquisition & Normalization.\n",
        "*   **Model:** `gemini-3-flash-preview` (Speed & Cost).\n",
        "*   **Inputs:** Raw PDF files from Google Drive.\n",
        "*   **Logic:**\n",
        "    1.  **Text:** Cleans OCR errors page-by-page.\n",
        "    2.  **Images:** Extracts images >5KB (saved as **PNG**).\n",
        "    3.  **Panel-Aware Vision:** Detects if an image is \"Figure 2.1 (A)\" vs \"(B)\" and splits captions accordingly.\n",
        "    4.  **Golden Links:** Generates permanent `gs://` links for every image.\n",
        "*   **Outputs:** `_CONTENT.json` (Text), `_FIGURES.json` (Image Metadata).\n",
        "\n",
        "### Block 2: The Architect (High-Fidelity)\n",
        "**Goal:** Medical Reasoning & Schema Enforcement.\n",
        "*   **Model:** `gemini-1.5-pro-002` or `gemini-3-pro-preview` (Deep Reasoning).\n",
        "*   **Inputs:** `_CONTENT.json` + `_FIGURES.json` + `_Tags.txt`.\n",
        "*   **Logic:**\n",
        "    1.  **The Monolith:** Processes **40 pages** in a single context window to capture full disease descriptions.\n",
        "    2.  **The Menu:** Forces AI to pick images from a pre-validated list of `gs://` links (prevents broken links).\n",
        "    3.  **Strict Extraction:** Explicitly instructed to list **stains (CD45+)** and **genetics** without summarizing.\n",
        "    4.  **Safety:** Auto-saves every 5 chunks; resumes if interrupted.\n",
        "*   **Outputs:** `_MASTER.json` (High quality, but potentially fragmented entities).\n",
        "\n",
        "### Block 3: The Consolidator\n",
        "**Goal:** Map-Reduce / De-fragmentation.\n",
        "*   **Model:** `gemini-3-flash-preview` (Logistics & Merging).\n",
        "*   **Inputs:** `_MASTER.json`.\n",
        "*   **Logic:**\n",
        "    1.  **Map:** Groups entries by Tag (e.g., finds 3 separate entries for \"Lichen Planus\" from different chapters).\n",
        "    2.  **Reduce:** Merges text, combines figure lists, and deduplicates data into one Super-Entry.\n",
        "*   **Outputs:** `_CONSOLIDATED.json` (Final Database-Ready File).\n",
        "\n",
        "---\n",
        "\n",
        "## üé• Workflow B: Lectures (Video)\n",
        "\n",
        "### Block 1: The Extractor\n",
        "**Goal:** Audio Transcription & Slide Extraction.\n",
        "*   **Model:** `whisper` (Audio) + `gemini-3-flash-preview` (Visuals).\n",
        "*   **Inputs:** MP4/MOV files from Google Drive.\n",
        "*   **Logic:**\n",
        "    1.  **Audio:** Generates timestamped transcript.\n",
        "    2.  **Visuals:** Extracts frames using **SSIM (Structural Similarity)** to deduplicate static slides (only 1 image per slide change). Saved as **JPG**.\n",
        "    3.  **Analysis:** Vision model extracts text/titles visible on the slide.\n",
        "*   **Outputs:** `_RAW.json` (List of slides with transcripts and GCS paths).\n",
        "\n",
        "### Block 2: The Architect (The Monolith)\n",
        "**Goal:** Synthesis & SOP Compliance.\n",
        "*   **Model:** `gemini-1.5-pro-002` or `gemini-3-pro-preview`.\n",
        "*   **Inputs:** `_RAW.json` + `_Tags.txt`.\n",
        "*   **Logic:**\n",
        "    1.  **Single Shot:** Feeds the **Entire Lecture** (Transcript + All Slide Images) in one massive request.\n",
        "    2.  **Visual Fidelity:** Prompt forces extraction of text labels seen on slides (e.g., \"TTF-1+\", \"CK20+\") rather than just summarizing the diagnosis.\n",
        "    3.  **Schema:** Maps the spoken lecture into the strict 18-field SOP (Clinical, Microscopic, etc.).\n",
        "*   **Outputs:** `_MASTER.json`. *(Note: Lectures rarely need Block 3 consolidation as they usually discuss a topic linearly).*\n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ Data Structure (Google Cloud)\n",
        "\n",
        "```text\n",
        "gs://pathology-hub-0/\n",
        "‚îú‚îÄ‚îÄ Tags/                        # Source of Truth (Ontology)\n",
        "‚îú‚îÄ‚îÄ _asset_library/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ textbooks/\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [Book_Name]/\n",
        "‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ figure_images/   # Saved PNGs (Lossless)\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ lectures/\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ [Video_Name]/        # Saved JPGs (Compressed)\n",
        "‚îî‚îÄ‚îÄ _content_library/\n",
        "    ‚îú‚îÄ‚îÄ textbooks/\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ [Book]_CONTENT.json\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ [Book]_FIGURES.json\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ [Book]_MASTER.json\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ [Book]_CONSOLIDATED.json    # <--- FINAL PDF RESULT\n",
        "    ‚îî‚îÄ‚îÄ lectures/\n",
        "        ‚îú‚îÄ‚îÄ [Video]_RAW.json\n",
        "        ‚îî‚îÄ‚îÄ [Video]_MASTER.json         # <--- FINAL VIDEO RESULT</details>"
      ],
      "metadata": {
        "id": "Pr4xHAKSzJLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 0"
      ],
      "metadata": {
        "id": "BpbB--6v4yXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 0: UNIVERSAL SETUP (Textbooks + Lectures)\n",
        "# ==============================================================================\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive, userdata, auth\n",
        "from google.cloud import storage\n",
        "import google.generativeai as genai\n",
        "\n",
        "print(\"--- STEP 0: INITIALIZATION ---\")\n",
        "\n",
        "# 1. Install & Configure System (Textbooks + Whisper/Video tools)\n",
        "print(\"üì¶ Installing dependencies (PDF, Video, AI)...\")\n",
        "!sudo apt-get update -qq && sudo apt-get install -y ffmpeg > /dev/null 2>&1\n",
        "!pip install -q -U google-generativeai PyMuPDF scikit-image aiohttp tqdm openai-whisper opencv-python-headless\n",
        "\n",
        "# 2. Authentication\n",
        "print(\"üîë Authenticating with Google Cloud...\")\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "except Exception as e:\n",
        "    raise SystemExit(f\"‚ùå Authentication Failed: {e}\")\n",
        "\n",
        "# 3. Mount Drive (Source Storage)\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# 4. Universal Configuration\n",
        "GCS_BUCKET_NAME = 'pathology-hub-0'\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/1-Projects/Knowledge_Pipeline'\n",
        "\n",
        "# Initialize GCS Client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "\n",
        "# --- THE MASTER PATH MAP ---\n",
        "# This dictionary handles routing for BOTH workflows.\n",
        "PATHS = {\n",
        "    # --- SOURCES (Local Google Drive) ---\n",
        "    \"source_pdfs\":      os.path.join(DRIVE_ROOT, '_source_materials', 'pdfs'),\n",
        "    \"source_videos\":    os.path.join(DRIVE_ROOT, '_source_materials', 'videos'),\n",
        "\n",
        "    # --- DESTINATIONS (GCS Bucket Paths) ---\n",
        "    \"gcs_bucket\":       GCS_BUCKET_NAME,\n",
        "    \"gcs_tags\":         \"Tags\",  # Where your _Tags.txt files live\n",
        "\n",
        "    # Textbook Pipeline\n",
        "    \"gcs_asset_textbooks\":   \"_asset_library/textbooks\",\n",
        "    \"gcs_content_textbooks\": \"_content_library/textbooks\",\n",
        "\n",
        "    # Lecture Pipeline\n",
        "    \"gcs_asset_lectures\":    \"_asset_library/lectures\",\n",
        "    \"gcs_content_lectures\":  \"_content_library/lectures\"\n",
        "}\n",
        "\n",
        "# 5. Verification\n",
        "print(f\"\\n‚úÖ Connected to Bucket: gs://{GCS_BUCKET_NAME}\")\n",
        "print(f\"‚úÖ Source PDFs:   {PATHS['source_pdfs']}\")\n",
        "print(f\"‚úÖ Source Videos: {PATHS['source_videos']}\")\n",
        "print(\"\\nüöÄ SYSTEM READY. You can now run Block 1 (Textbook) or Block 1 (Lecture).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBl8jRWY41vc",
        "outputId": "71e42dca-7200-41dc-a814-b3a94e66f5b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STEP 0: INITIALIZATION ---\n",
            "üì¶ Installing dependencies (PDF, Video, AI)...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "üîë Authenticating with Google Cloud...\n",
            "Mounted at /content/drive\n",
            "\n",
            "‚úÖ Connected to Bucket: gs://pathology-hub-0\n",
            "‚úÖ Source PDFs:   /content/drive/MyDrive/1-Projects/Knowledge_Pipeline/_source_materials/pdfs\n",
            "‚úÖ Source Videos: /content/drive/MyDrive/1-Projects/Knowledge_Pipeline/_source_materials/videos\n",
            "\n",
            "üöÄ SYSTEM READY. You can now run Block 1 (Textbook) or Block 1 (Lecture).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF BLOCK 1: TEXTBOOK EXTRACTOR (Text + Figures)"
      ],
      "metadata": {
        "id": "qp8ILGTF4otI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 1: TEXTBOOK EXTRACTOR (Gemini 3 Flash - Panel Aware)\n",
        "# ==============================================================================\n",
        "import base64\n",
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import os\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from google.cloud import storage\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TEXT_CONCURRENCY = 20\n",
        "VISION_CONCURRENCY = 20\n",
        "TEXT_MODEL_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent?key={GEMINI_API_KEY}\"\n",
        "VISION_MODEL_NAME = \"gemini-3-flash-preview\"\n",
        "\n",
        "# --- HELPER: GCS UTILS ---\n",
        "def gcs_exists(blob_path):\n",
        "    return bucket.blob(blob_path).exists()\n",
        "\n",
        "def gcs_upload_bytes(data, blob_path, content_type):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(data, content_type=content_type)\n",
        "\n",
        "def gcs_upload_json(data, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def gcs_load_json(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    if blob.exists():\n",
        "        return json.loads(blob.download_as_string())\n",
        "    return []\n",
        "\n",
        "# --- AI HELPERS ---\n",
        "async def clean_text_async(session, text, page_num, sem):\n",
        "    async with sem:\n",
        "        if not text.strip(): return page_num, \"\"\n",
        "        prompt = (\n",
        "            \"Clean this medical text. Fix OCR errors. Keep structure. \"\n",
        "            \"Preserve Figure Captions exactly. Return JSON: {\\\"markdown\\\": \\\"...\\\"}\"\n",
        "            f\"\\n\\nRAW TEXT:\\n{text}\"\n",
        "        )\n",
        "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "        try:\n",
        "            async with session.post(TEXT_MODEL_URL, json=payload) as res:\n",
        "                if res.status == 200:\n",
        "                    dat = await res.json()\n",
        "                    raw = dat['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
        "                    if match: return page_num, json.loads(match.group(0)).get(\"markdown\", text)\n",
        "                return page_num, text\n",
        "        except: return page_num, text\n",
        "\n",
        "async def analyze_figure_async(session, b64_img, context, sem):\n",
        "    \"\"\"\n",
        "    Panel-Aware Vision Analysis.\n",
        "    Tries to distinguish if the image is just Panel A or Panel B of a multipart figure.\n",
        "    \"\"\"\n",
        "    async with sem:\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{VISION_MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        PAGE CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        TASK: Analyze the image below.\n",
        "        1. Identify the Figure ID (e.g. \"Fig 2.1\") from the context that matches this image.\n",
        "        2. **MULTI-PANEL CHECK:**\n",
        "           - Does the caption describe multiple parts (e.g. \"(A) ... (B) ...\")?\n",
        "           - If yes, determine if THIS specific image is Panel A, Panel B, etc.\n",
        "           - If this image is ONLY Panel A, try to extract ONLY the caption text for (A).\n",
        "           - If you cannot split the text, return the full caption but add \"(Panel A)\" to the ID.\n",
        "\n",
        "        Return JSON: {{\"figure_id\": \"Fig X.X (Panel A)\", \"matched_caption\": \"Specific caption...\"}} or null.\n",
        "        \"\"\"\n",
        "\n",
        "        parts = [\n",
        "            {\"text\": prompt},\n",
        "            {\"inline_data\": {\"mime_type\": \"image/png\", \"data\": b64_img}}\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            async with session.post(url, json={\"contents\": [{\"parts\": parts}]}) as res:\n",
        "                if res.status == 200:\n",
        "                    dat = await res.json()\n",
        "                    raw = dat['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
        "                    if match: return json.loads(match.group(0))\n",
        "        except: return None\n",
        "        return None\n",
        "\n",
        "# --- MAIN PROCESSOR ---\n",
        "async def process_textbook(pdf_path, start_p=1, end_p=None):\n",
        "    fname = os.path.basename(pdf_path)\n",
        "    book_name = os.path.splitext(fname)[0].replace(' ', '_')\n",
        "\n",
        "    base_asset = f\"{PATHS['gcs_asset_textbooks']}/{book_name}\"\n",
        "    path_fig_imgs = f\"{base_asset}/figure_images\"\n",
        "    path_content = f\"{PATHS['gcs_content_textbooks']}/{book_name}_CONTENT.json\"\n",
        "    path_figures = f\"{PATHS['gcs_content_textbooks']}/{book_name}_FIGURES.json\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\\nüìò PROCESSING: {book_name}\\n{'='*60}\")\n",
        "\n",
        "    doc = fitz.open(pdf_path)\n",
        "    total = len(doc)\n",
        "    final_p = min(end_p or total, total)\n",
        "\n",
        "    # 1. TEXT (Skip if done)\n",
        "    existing_content = gcs_load_json(path_content)\n",
        "    if not existing_content:\n",
        "        print(f\"üìù Phase 1: Cleaning Text...\")\n",
        "        sem = asyncio.Semaphore(TEXT_CONCURRENCY)\n",
        "        async with aiohttp.ClientSession() as sess:\n",
        "            tasks = [clean_text_async(sess, doc.load_page(p).get_text(\"text\"), p+1, sem) for p in range(start_p-1, final_p)]\n",
        "            results = await tqdm_asyncio.gather(*tasks)\n",
        "        content_data = sorted([{\"source\": fname, \"page_number\": p, \"content\": t} for p, t in results], key=lambda x: x['page_number'])\n",
        "        gcs_upload_json(content_data, path_content)\n",
        "    else:\n",
        "        content_data = existing_content\n",
        "\n",
        "    content_map = {c['page_number']: c['content'] for c in content_data}\n",
        "\n",
        "    # 2. FIGURES\n",
        "    print(\"üñºÔ∏è Phase 2: Figures & Vision (Panel-Aware)...\")\n",
        "    existing_figs = gcs_load_json(path_figures)\n",
        "    processed_pages = {f['source_page'] for f in existing_figs}\n",
        "    vision_tasks = []\n",
        "    new_figures = []\n",
        "    sem_vis = asyncio.Semaphore(VISION_CONCURRENCY)\n",
        "\n",
        "    for p_idx in range(start_p-1, final_p):\n",
        "        p_num = p_idx + 1\n",
        "        if p_num in processed_pages: continue\n",
        "\n",
        "        page = doc.load_page(p_idx)\n",
        "        images = page.get_images(full=True)\n",
        "        if not images: continue\n",
        "\n",
        "        md_ctx = content_map.get(p_num, \"\")\n",
        "\n",
        "        for i, img in enumerate(images):\n",
        "            try:\n",
        "                xref = img[0]\n",
        "                base = doc.extract_image(xref)\n",
        "                if len(base[\"image\"]) < 5000: continue\n",
        "\n",
        "                img_name = f\"{book_name}_page_{p_num}_img_{i+1}.{base['ext']}\"\n",
        "                blob_path = f\"{path_fig_imgs}/{img_name}\"\n",
        "                full_uri = f\"gs://{GCS_BUCKET_NAME}/{blob_path}\"\n",
        "\n",
        "                if not gcs_exists(blob_path):\n",
        "                    gcs_upload_bytes(base[\"image\"], blob_path, f\"image/{base['ext']}\")\n",
        "\n",
        "                b64 = base64.b64encode(base[\"image\"]).decode('utf-8')\n",
        "                vision_tasks.append({\n",
        "                    \"b64\": b64, \"ctx\": md_ctx,\n",
        "                    \"meta\": {\"source_page\": p_num, \"gcs_path\": full_uri}\n",
        "                })\n",
        "            except: pass\n",
        "\n",
        "    if vision_tasks:\n",
        "        print(f\"   -> Analyzing {len(vision_tasks)} figures...\")\n",
        "        async with aiohttp.ClientSession() as sess:\n",
        "            tasks = [analyze_figure_async(sess, t['b64'], t['ctx'], sem_vis) for t in vision_tasks]\n",
        "            results = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "            for i, res in enumerate(results):\n",
        "                if res and res.get('figure_id'):\n",
        "                    meta = vision_tasks[i]['meta']\n",
        "                    new_figures.append({\n",
        "                        \"source_document\": fname,\n",
        "                        \"source_page\": meta['source_page'],\n",
        "                        \"figure_id\": res['figure_id'],\n",
        "                        \"description\": res['matched_caption'],\n",
        "                        \"gcs_path\": meta['gcs_path']\n",
        "                    })\n",
        "\n",
        "        final_list = existing_figs + new_figures\n",
        "        final_list.sort(key=lambda x: x['source_page'])\n",
        "        gcs_upload_json(final_list, path_figures)\n",
        "        print(f\"   -> Added {len(new_figures)} figures.\")\n",
        "\n",
        "# --- RUNNER ---\n",
        "async def main():\n",
        "    pdfs = sorted([f for f in os.listdir(PATHS['source_pdfs']) if f.endswith('.pdf')])\n",
        "    if not pdfs: print(\"‚ùå No PDFs found.\"); return\n",
        "\n",
        "    print(\"\\n--- AVAILABLE TEXTBOOKS ---\")\n",
        "    for i, f in enumerate(pdfs): print(f\"[{i+1}] {f}\")\n",
        "\n",
        "    sel = input(\"\\nSelect book(s) (e.g. 1, 3): \")\n",
        "    indices = [int(x)-1 for x in sel.split(',') if x.strip().isdigit()]\n",
        "\n",
        "    for idx in indices:\n",
        "        if 0 <= idx < len(pdfs):\n",
        "            await process_textbook(os.path.join(PATHS['source_pdfs'], pdfs[idx]))\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "CF5BNCNP4qlL",
        "outputId": "e26564d4-2404-4342-fb53-5630e0531eca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fitz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2714752480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m  \u001b[0;31m# PyMuPDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fitz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF BLOCK 2: TEXTBOOK ARCHITECT (High-Fidelity Monolith)\n"
      ],
      "metadata": {
        "id": "k0P6QtEh3vos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 2: TEXTBOOK ARCHITECT (Robust + ID Swap + Batching)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import difflib\n",
        "import random\n",
        "import os\n",
        "from typing import List, Dict, Set, Any\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gemini-3-pro-preview\"\n",
        "CONCURRENCY_LIMIT = 2\n",
        "PAGES_PER_CHUNK = 40\n",
        "PAGE_OVERLAP = 2\n",
        "MAX_RETRIES = 10\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_read_text(blob_path: str) -> str:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return blob.download_as_string().decode('utf-8') if blob.exists() else \"\"\n",
        "\n",
        "def gcs_load_json(blob_path: str) -> List:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data: Any, blob_path: str):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def validate_tag(tag_input: Any, valid_set: Set[str]) -> str:\n",
        "    \"\"\"Safely validates tags, handling None/Empty/Lists.\"\"\"\n",
        "    if not tag_input: return \"Skin::Unclassified\"\n",
        "    if isinstance(tag_input, list):\n",
        "        tag_str = tag_input[0] if len(tag_input) > 0 else \"Skin::Unclassified\"\n",
        "    else:\n",
        "        tag_str = str(tag_input)\n",
        "    clean = tag_str.strip()\n",
        "    if clean in valid_set: return clean\n",
        "    matches = difflib.get_close_matches(clean, list(valid_set), n=1, cutoff=0.7)\n",
        "    return matches[0] if matches else clean\n",
        "\n",
        "# --- LOGIC: THE ID SWAPPER ---\n",
        "def inject_real_paths(entities, figure_lookup_map):\n",
        "    \"\"\"Replaces AI placeholders with REAL GCS paths.\"\"\"\n",
        "    for ent in entities:\n",
        "        if 'related_figures' in ent:\n",
        "            for fig in ent['related_figures']:\n",
        "                fig_id = fig.get('id')\n",
        "                if fig_id in figure_lookup_map:\n",
        "                    real_data = figure_lookup_map[fig_id]\n",
        "                    fig['src'] = real_data['gcs_path']\n",
        "                    fig['gcs_path'] = real_data['gcs_path']\n",
        "                else:\n",
        "                    fig['src'] = None\n",
        "                    fig['gcs_path'] = None\n",
        "    return entities\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. PROMPT ENGINEERING\n",
        "# ------------------------------------------------------------------------------\n",
        "def construct_textbook_prompt(text_content, figure_list_simple, valid_tags_list):\n",
        "    # Only show ID and Caption to AI\n",
        "    fig_context = \"\\n\".join([f\"ID: {f.get('figure_id', 'Unknown')} | Caption: {f.get('description','')}\" for f in figure_list_simple])\n",
        "\n",
        "    return f\"\"\"\n",
        "Role: Senior Dermatopathologist.\n",
        "Task: Extract disease entities from this textbook section.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. **Extraction:** Extract Definition, Clinical, Microscopic, etc.\n",
        "2. **Tagging:** Use the exact tag from the list.\n",
        "3. **Figure Linking (CRITICAL):**\n",
        "   - I have provided a list of Figures with IDs.\n",
        "   - If a figure is relevant, add it to `related_figures`.\n",
        "   - **IMPORTANT:** In the JSON, put the ID in the `id` field. Leave `src` and `gcs_path` as \"PLACEHOLDER\".\n",
        "\n",
        "REQUIRED JSON SCHEMA:\n",
        "[\n",
        "  {{\n",
        "    \"entity_name\": \"Disease Name\",\n",
        "    \"definition\": \"...\",\n",
        "    \"tags\": [\"Tag\"],\n",
        "    \"html_gcs_path\": null,\n",
        "    \"clinical\": \"...\",\n",
        "    \"microscopic\": \"...\",\n",
        "    \"ancillary_studies\": \"...\",\n",
        "    \"related_figures\": [\n",
        "        {{\n",
        "            \"id\": \"COPY_EXACT_ID_FROM_LIST\",\n",
        "            \"src\": \"PLACEHOLDER\",\n",
        "            \"gcs_path\": \"PLACEHOLDER\",\n",
        "            \"diagnosis\": \"Disease Name\",\n",
        "            \"legend\": \"Full caption.\"\n",
        "        }}\n",
        "    ]\n",
        "  }}\n",
        "]\n",
        "\n",
        "REFERENCE TAGS:\n",
        "{valid_tags_list}\n",
        "\n",
        "AVAILABLE FIGURES:\n",
        "{fig_context}\n",
        "\n",
        "TEXT CONTENT:\n",
        "{text_content}\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. CHUNK PROCESSOR\n",
        "# ------------------------------------------------------------------------------\n",
        "async def process_textbook_chunk(session, chunk_data, figures_in_chunk, valid_tags_text, valid_tags_set, sem):\n",
        "    async with sem:\n",
        "        full_text = \"\\n\\n\".join([f\"--- Page {p['page_number']} ---\\n{p['content']}\" for p in chunk_data])\n",
        "\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "        payload = {\"contents\": [{\"parts\": [{\"text\": construct_textbook_prompt(full_text, figures_in_chunk, valid_tags_text)}]}]}\n",
        "\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                async with session.post(url, json=payload, timeout=600) as response:\n",
        "                    if response.status == 200:\n",
        "                        data = await response.json()\n",
        "                        raw_txt = data['candidates'][0]['content']['parts'][0]['text']\n",
        "                        match = re.search(r'\\[.*\\]', raw_txt.replace(\"```json\", \"\").replace(\"```\", \"\"), re.DOTALL)\n",
        "                        if match:\n",
        "                            entities = json.loads(match.group(0))\n",
        "                            valid_entities = []\n",
        "\n",
        "                            # ID Swap Logic\n",
        "                            chunk_map = {f.get('figure_id'): f for f in figures_in_chunk}\n",
        "                            entities = inject_real_paths(entities, chunk_map)\n",
        "\n",
        "                            for ent in entities:\n",
        "                                if not ent.get('entity_name'): continue\n",
        "\n",
        "                                # Tag & Null Validation\n",
        "                                ent['tags'] = [validate_tag(ent.get('tags', []), valid_tags_set)]\n",
        "                                for k in [\"clinical\", \"microscopic\", \"ancillary_studies\", \"differential_diagnosis\", \"pathogenesis\", \"staging\", \"cytology\"]:\n",
        "                                    if k not in ent: ent[k] = None\n",
        "                                ent['html_gcs_path'] = None\n",
        "\n",
        "                                valid_entities.append(ent)\n",
        "                            return valid_entities\n",
        "                        return []\n",
        "                    elif response.status == 429:\n",
        "                        wait = (2 ** attempt) + random.uniform(5, 15)\n",
        "                        await asyncio.sleep(wait)\n",
        "                        continue\n",
        "            except:\n",
        "                await asyncio.sleep(15)\n",
        "        return []\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MAIN WORKFLOW\n",
        "# ------------------------------------------------------------------------------\n",
        "async def main_definitive():\n",
        "    # 1. Select Tags\n",
        "    tag_files = [b.name for b in bucket.list_blobs(prefix=\"Tags/\") if b.name.endswith('.txt')]\n",
        "    print(\"\\n--- SELECT TAG LIST ---\")\n",
        "    for i, f in enumerate(tag_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    t_idx = int(input(\"Choice: \")) - 1\n",
        "    tags_text = gcs_read_text(tag_files[t_idx])\n",
        "    tags_set = set(l.strip() for l in tags_text.splitlines() if l.strip())\n",
        "\n",
        "    # 2. Select Textbook\n",
        "    content_files = [b.name for b in bucket.list_blobs(prefix=PATHS['gcs_content_textbooks']) if \"_CONTENT.json\" in b.name]\n",
        "    if not content_files: print(\"‚ùå No CONTENT files found.\"); return\n",
        "\n",
        "    print(\"\\n--- SELECT TEXTBOOK ---\")\n",
        "    for i, f in enumerate(content_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    c_idx = int(input(\"Choice: \")) - 1\n",
        "\n",
        "    content_path = content_files[c_idx]\n",
        "    book_base = content_path.split('/')[-1].replace(\"_CONTENT.json\", \"\")\n",
        "    fig_path = content_path.replace(\"_CONTENT.json\", \"_FIGURES.json\")\n",
        "    final_path = f\"{PATHS['gcs_content_textbooks']}/{book_base}_MASTER.json\"\n",
        "\n",
        "    print(f\"\\nüöÄ Processing: {book_base}\")\n",
        "    raw_content = gcs_load_json(content_path)\n",
        "    raw_figures = gcs_load_json(fig_path)\n",
        "    raw_content.sort(key=lambda x: x['page_number'])\n",
        "\n",
        "    # 3. Resume Check\n",
        "    master_kb = []\n",
        "    if bucket.blob(final_path).exists():\n",
        "        print(f\"\\n‚ö†Ô∏è Existing MASTER file found.\")\n",
        "        choice = input(\"Type 'RESUME' to continue or 'RESTART' to overwrite: \").strip().upper()\n",
        "        if choice == 'RESUME':\n",
        "            master_kb = gcs_load_json(final_path)\n",
        "            print(f\"   -> Resuming with {len(master_kb)} existing entities.\")\n",
        "        else:\n",
        "            master_kb = []\n",
        "\n",
        "    # 4. Chunking\n",
        "    chunks = []\n",
        "    total_pages = len(raw_content)\n",
        "    for i in range(0, total_pages, PAGES_PER_CHUNK):\n",
        "        end_idx = min(i + PAGES_PER_CHUNK + PAGE_OVERLAP, total_pages)\n",
        "        chunks.append(raw_content[i : end_idx])\n",
        "    print(f\"üì¶ Total Chunks: {len(chunks)}\")\n",
        "\n",
        "    # 5. Batched Execution\n",
        "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
        "\n",
        "    for i in range(0, len(chunks), BATCH_SIZE):\n",
        "        batch = chunks[i : i + BATCH_SIZE]\n",
        "        print(f\"\\n--- Batch {i//BATCH_SIZE + 1} ---\")\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = []\n",
        "            for chunk in batch:\n",
        "                page_nums = {p['page_number'] for p in chunk}\n",
        "                chunk_figs = [f for f in raw_figures if f['source_page'] in page_nums]\n",
        "                tasks.append(process_textbook_chunk(session, chunk, chunk_figs, tags_text, tags_set, sem))\n",
        "\n",
        "            results = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "            new_count = 0\n",
        "            for res_list in results:\n",
        "                master_kb.extend(res_list)\n",
        "                new_count += len(res_list)\n",
        "\n",
        "            if new_count > 0:\n",
        "                gcs_upload_json(master_kb, final_path)\n",
        "                print(f\"üíæ Saved (+{new_count})\")\n",
        "\n",
        "    # 6. Final Dedupe\n",
        "    print(\"\\nüßπ Final Deduplication...\")\n",
        "    unique_kb = []\n",
        "    seen = set()\n",
        "    for ent in master_kb:\n",
        "        def_text = ent.get('definition') or \"\"\n",
        "        key = (ent.get('entity_name'), def_text[:50])\n",
        "        if key not in seen:\n",
        "            unique_kb.append(ent)\n",
        "            seen.add(key)\n",
        "\n",
        "    gcs_upload_json(unique_kb, final_path)\n",
        "    print(f\"\\n‚úÖ DONE: {final_path}\")\n",
        "    print(f\"üìä Final Entities: {len(unique_kb)}\")\n",
        "\n",
        "await main_definitive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whXpnB40CmGw",
        "outputId": "99bbad67-7a13-4189-c6ad-b0ae44349e68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SELECT TAG LIST ---\n",
            "[1] BST_Tags.txt\n",
            "[2] Breast_Tags.txt\n",
            "[3] Endo_Tags.txt\n",
            "[4] GI_Tags.txt\n",
            "[5] GYN_Tags.txt\n",
            "[6] Skin_Tags.txt\n",
            "Choice: 6\n",
            "\n",
            "--- SELECT TEXTBOOK ---\n",
            "[1] BST_Horvai_CONTENT.json\n",
            "[2] Bone_Atlas_CONTENT.json\n",
            "[3] Bone_Dorfman_CONTENT.json\n",
            "[4] Bone_Pattern_CONTENT.json\n",
            "[5] Breast_Atlas_CONTENT.json\n",
            "[6] Breast_Biopsy_CONTENT.json\n",
            "[7] Breast_FAQ_CONTENT.json\n",
            "[8] Breast_Pattern_CONTENT.json\n",
            "[9] Cyto_Breast_Yokohama_CONTENT.json\n",
            "[10] Cyto_Cibas_CONTENT.json\n",
            "[11] Cyto_Comprehensive_Part_One_CONTENT.json\n",
            "[12] Cyto_Comprehensive_Part_Two_CONTENT.json\n",
            "[13] Cyto_GU_Paris_CONTENT.json\n",
            "[14] Cyto_Gyn_Bethesda_CONTENT.json\n",
            "[15] Cyto_Milan_CONTENT.json\n",
            "[16] Cyto_PSC_Lung_CONTENT.json\n",
            "[17] Cyto_Pattern_CONTENT.json\n",
            "[18] Cyto_Serous_Fluids_CONTENT.json\n",
            "[19] Cyto_Thyroid_Bethesda_CONTENT.json\n",
            "[20] Derm_Barnhill_CONTENT.json\n",
            "[21] Derm_Elston_CONTENT.json\n",
            "[22] Derm_Levers_CONTENT.json\n",
            "[23] Derm_McKeeHY_CONTENT.json\n",
            "[24] Derm_McKee_CONTENT.json\n",
            "[25] Derm_McKee_High_Yield_CONTENT.json\n",
            "[26] Derm_Patterson_CONTENT.json\n",
            "[27] Derm_Weedon_CONTENT.json\n",
            "[28] Endo_Atlas_CONTENT.json\n",
            "[29] GI_Atlas_CONTENT.json\n",
            "[30] GI_Biopsy_Interpretation_(Neoplastic)_CONTENT.json\n",
            "[31] GI_Biopsy_Interpretation_(Non_Neoplastic)_CONTENT.json\n",
            "[32] GI_Intestinal_Atlas1_CONTENT.json\n",
            "[33] GI_Liver_Macsween_CONTENT.json\n",
            "[34] GI_Non-Neoplastic_Zhang_CONTENT.json\n",
            "[35] GU_Biopsy_Interpretation_(Prostate)_CONTENT.json\n",
            "[36] Gyn_Atlas_Part_One_CONTENT.json\n",
            "[37] Gyn_Atlas_Part_Two_CONTENT.json\n",
            "[38] Gyn_Essentials_CONTENT.json\n",
            "[39] HN_Thompson_CONTENT.json\n",
            "[40] Peds_Course_review_CONTENT.json\n",
            "[41] Skin_Elston_CONTENT.json\n",
            "[42] Skin_Levers_CONTENT.json\n",
            "[43] SoftTissue_Enzinger_CONTENT.json\n",
            "[44] SoftTissue_Pattern_CONTENT.json\n",
            "Choice: 20\n",
            "\n",
            "üöÄ Processing: Derm_Barnhill\n",
            "   Model: gemini-3-pro-preview (High Fidelity)\n",
            "üì¶ Total Chunks: 10 (~50 pages each)\n",
            "\n",
            "--- Processing Batch 1/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [08:41<00:00, 104.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved progress... (+197 entities)\n",
            "\n",
            "--- Processing Batch 2/3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [09:10<00:00, 110.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved progress... (+224 entities)\n",
            "\n",
            "üßπ Final Deduplication...\n",
            "\n",
            "‚úÖ COMPLETE: gs://pathology-hub-0/_content_library/textbooks/Derm_Barnhill_MASTER.json\n",
            "üìä Final Count: 421 Entities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zy6oq1WV3s_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3799379-10c3-488c-fc47-fb7691caf12e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SELECT TAG LIST ---\n",
            "[1] BST_Tags.txt\n",
            "[2] Breast_Tags.txt\n",
            "[3] Endo_Tags.txt\n",
            "[4] GI_Tags.txt\n",
            "[5] GYN_Tags.txt\n",
            "[6] Skin_Tags.txt\n",
            "Choice: 6\n",
            "\n",
            "--- SELECT TEXTBOOK ---\n",
            "[1] BST_Horvai_CONTENT.json\n",
            "[2] Bone_Atlas_CONTENT.json\n",
            "[3] Bone_Dorfman_CONTENT.json\n",
            "[4] Bone_Pattern_CONTENT.json\n",
            "[5] Breast_Atlas_CONTENT.json\n",
            "[6] Breast_Biopsy_CONTENT.json\n",
            "[7] Breast_FAQ_CONTENT.json\n",
            "[8] Breast_Pattern_CONTENT.json\n",
            "[9] Cyto_Breast_Yokohama_CONTENT.json\n",
            "[10] Cyto_Cibas_CONTENT.json\n",
            "[11] Cyto_Comprehensive_Part_One_CONTENT.json\n",
            "[12] Cyto_Comprehensive_Part_Two_CONTENT.json\n",
            "[13] Cyto_GU_Paris_CONTENT.json\n",
            "[14] Cyto_Gyn_Bethesda_CONTENT.json\n",
            "[15] Cyto_Milan_CONTENT.json\n",
            "[16] Cyto_PSC_Lung_CONTENT.json\n",
            "[17] Cyto_Pattern_CONTENT.json\n",
            "[18] Cyto_Serous_Fluids_CONTENT.json\n",
            "[19] Cyto_Thyroid_Bethesda_CONTENT.json\n",
            "[20] Derm_Elston_CONTENT.json\n",
            "[21] Derm_Levers_CONTENT.json\n",
            "[22] Derm_McKee_CONTENT.json\n",
            "[23] Derm_McKee_High_Yield_CONTENT.json\n",
            "[24] Derm_Patterson_CONTENT.json\n",
            "[25] Derm_Weedon_CONTENT.json\n",
            "[26] Endo_Atlas_CONTENT.json\n",
            "[27] GI_Atlas_CONTENT.json\n",
            "[28] GI_Biopsy_Interpretation_(Neoplastic)_CONTENT.json\n",
            "[29] GI_Biopsy_Interpretation_(Non_Neoplastic)_CONTENT.json\n",
            "[30] GI_Intestinal_Atlas1_CONTENT.json\n",
            "[31] GI_Liver_Macsween_CONTENT.json\n",
            "[32] GI_Non-Neoplastic_Zhang_CONTENT.json\n",
            "[33] GU_Biopsy_Interpretation_(Prostate)_CONTENT.json\n",
            "[34] Gyn_Atlas_Part_One_CONTENT.json\n",
            "[35] Gyn_Atlas_Part_Two_CONTENT.json\n",
            "[36] Gyn_Essentials_CONTENT.json\n",
            "[37] HN_Thompson_CONTENT.json\n",
            "[38] Peds_Course_review_CONTENT.json\n",
            "[39] Skin_Elston_CONTENT.json\n",
            "[40] Skin_Levers_CONTENT.json\n",
            "[41] SoftTissue_Enzinger_CONTENT.json\n",
            "[42] SoftTissue_Pattern_CONTENT.json\n",
            "Choice: 22\n",
            "\n",
            "üöÄ Processing: Derm_McKee\n",
            "üì¶ Total Chunks: 46 (~40 pages each)\n",
            "\n",
            "--- Processing Batch 1/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [10:43<00:00, 128.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving progress... (+105 entities)\n",
            "\n",
            "--- Processing Batch 2/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [12:57<00:00, 155.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving progress... (+151 entities)\n",
            "\n",
            "--- Processing Batch 3/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [13:39<00:00, 163.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving progress... (+167 entities)\n",
            "\n",
            "--- Processing Batch 4/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [11:33<00:00, 138.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving progress... (+112 entities)\n",
            "\n",
            "--- Processing Batch 5/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [10:22<00:00, 124.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving progress... (+144 entities)\n",
            "\n",
            "--- Processing Batch 6/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [11:54<00:00, 142.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving progress... (+118 entities)\n",
            "\n",
            "--- Processing Batch 7/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [10:06<00:00, 121.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving progress... (+105 entities)\n",
            "\n",
            "--- Processing Batch 8/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [12:03<00:00, 144.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving progress... (+126 entities)\n",
            "\n",
            "--- Processing Batch 9/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [15:03<00:00, 180.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving progress... (+156 entities)\n",
            "\n",
            "--- Processing Batch 10/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:50<00:00, 110.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving progress... (+31 entities)\n",
            "\n",
            "üßπ Final Deduplication...\n",
            "\n",
            "‚úÖ DONE: gs://pathology-hub-0/_content_library/textbooks/Derm_McKee_MASTER.json\n",
            "üìä Final Count: 1207 Entities\n"
          ]
        }
      ],
      "source": [
        "\n",
        "--- SELECT TAG LIST ---\n",
        "[1] BST_Tags.txt\n",
        "[2] Breast_Tags.txt\n",
        "[3] Endo_Tags.txt\n",
        "[4] GI_Tags.txt\n",
        "[5] GYN_Tags.txt\n",
        "[6] Skin_Tags.txt\n",
        "Choice: 6\n",
        "\n",
        "--- SELECT TEXTBOOK ---\n",
        "[1] BST_Horvai_CONTENT.json\n",
        "[2] Bone_Atlas_CONTENT.json\n",
        "[3] Bone_Dorfman_CONTENT.json\n",
        "[4] Bone_Pattern_CONTENT.json\n",
        "[5] Breast_Atlas_CONTENT.json\n",
        "[6] Breast_Biopsy_CONTENT.json\n",
        "[7] Breast_FAQ_CONTENT.json\n",
        "[8] Breast_Pattern_CONTENT.json\n",
        "[9] Cyto_Breast_Yokohama_CONTENT.json\n",
        "[10] Cyto_Cibas_CONTENT.json\n",
        "[11] Cyto_Comprehensive_Part_One_CONTENT.json\n",
        "[12] Cyto_Comprehensive_Part_Two_CONTENT.json\n",
        "[13] Cyto_GU_Paris_CONTENT.json\n",
        "[14] Cyto_Gyn_Bethesda_CONTENT.json\n",
        "[15] Cyto_Milan_CONTENT.json\n",
        "[16] Cyto_PSC_Lung_CONTENT.json\n",
        "[17] Cyto_Pattern_CONTENT.json\n",
        "[18] Cyto_Serous_Fluids_CONTENT.json\n",
        "[19] Cyto_Thyroid_Bethesda_CONTENT.json\n",
        "[20] Derm_Elston_CONTENT.json\n",
        "[21] Derm_Levers_CONTENT.json\n",
        "[22] Derm_McKee_CONTENT.json\n",
        "[23] Derm_McKee_High_Yield_CONTENT.json\n",
        "[24] Derm_Patterson_CONTENT.json\n",
        "[25] Derm_Weedon_CONTENT.json\n",
        "[26] Endo_Atlas_CONTENT.json\n",
        "[27] GI_Atlas_CONTENT.json\n",
        "[28] GI_Biopsy_Interpretation_(Neoplastic)_CONTENT.json\n",
        "[29] GI_Biopsy_Interpretation_(Non_Neoplastic)_CONTENT.json\n",
        "[30] GI_Intestinal_Atlas1_CONTENT.json\n",
        "[31] GI_Liver_Macsween_CONTENT.json\n",
        "[32] GI_Non-Neoplastic_Zhang_CONTENT.json\n",
        "[33] GU_Biopsy_Interpretation_(Prostate)_CONTENT.json\n",
        "[34] Gyn_Atlas_Part_One_CONTENT.json\n",
        "[35] Gyn_Atlas_Part_Two_CONTENT.json\n",
        "[36] Gyn_Essentials_CONTENT.json\n",
        "[37] HN_Thompson_CONTENT.json\n",
        "[38] Peds_Course_review_CONTENT.json\n",
        "[39] Skin_Elston_CONTENT.json\n",
        "[40] Skin_Levers_CONTENT.json\n",
        "[41] SoftTissue_Enzinger_CONTENT.json\n",
        "[42] SoftTissue_Pattern_CONTENT.json\n",
        "Choice: 20\n",
        "\n",
        "üöÄ Processing: Derm_Elston\n",
        "üîÑ Found existing MASTER file. Resuming...\n",
        "üì¶ Total Chunks: 12 (~40 pages each)\n",
        "\n",
        "--- Processing Batch 1/3 ---\n",
        "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:04<00:00,  1.19it/s]\n",
        "üíæ Saving progress... (+0 entities)\n",
        "\n",
        "--- Processing Batch 2/3 ---\n",
        "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.30it/s]\n",
        "üíæ Saving progress... (+0 entities)\n",
        "\n",
        "--- Processing Batch 3/3 ---\n",
        "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.25it/s]\n",
        "üíæ Saving progress... (+0 entities)\n",
        "\n",
        "üßπ Final Deduplication...\n",
        "\n",
        "‚úÖ COMPLETE: gs://pathology-hub-0/_content_library/textbooks/Derm_Elston_MASTER.json\n",
        "üìä Final Count: 1078 Entities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF BLOCK 2.1: THE CONSOLIDATOR (Map-Reduce Merge)"
      ],
      "metadata": {
        "id": "GKMkiyqh4CM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 3: THE CONSOLIDATOR (Logic-Based Figure Merging)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from typing import List, Any\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gemini-3-flash-preview\"\n",
        "CONCURRENCY_LIMIT = 20  # Flash is fast, we can push this\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_load_json(blob_path: str) -> List:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data: Any, blob_path: str):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "# --- PROMPT ---\n",
        "def construct_merge_prompt(entity_name, fragments_text_only):\n",
        "    \"\"\"\n",
        "    Note: We only send TEXT to the AI. We handle figures in Python.\n",
        "    \"\"\"\n",
        "    return f\"\"\"\n",
        "Role: Medical Data Editor.\n",
        "Task: Merge these text fragments for \"{entity_name}\" into ONE comprehensive entry.\n",
        "\n",
        "INPUT FRAGMENTS:\n",
        "{json.dumps(fragments_text_only, indent=2)}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. **Consolidate Text:** Combine 'clinical', 'microscopic', 'definition', etc.\n",
        "   - DO NOT SUMMARIZE. Preserve all distinct details (e.g., if one fragment lists a stain and another lists a gene, keep BOTH).\n",
        "2. **Preserve Tags:** Use the most specific tag available.\n",
        "3. **Output:** A single JSON object.\n",
        "\n",
        "REQUIRED SCHEMA:\n",
        "{{\n",
        "    \"entity_name\": \"{entity_name}\",\n",
        "    \"definition\": \"Merged...\",\n",
        "    \"tags\": [\"...\"],\n",
        "    \"html_gcs_path\": null,\n",
        "    \"localization\": \"...\",\n",
        "    \"clinical\": \"...\",\n",
        "    \"pathogenesis\": \"...\",\n",
        "    \"macroscopic\": \"...\",\n",
        "    \"microscopic\": \"...\",\n",
        "    \"cytology\": \"...\",\n",
        "    \"ancillary_studies\": \"...\",\n",
        "    \"diagnostic_molecular_pathology\": \"...\",\n",
        "    \"differential_diagnosis\": \"...\",\n",
        "    \"staging\": \"...\",\n",
        "    \"prognosis_and_prediction\": \"...\",\n",
        "    \"subtypes\": \"...\",\n",
        "    \"related_terminology\": \"...\",\n",
        "    \"essential_and_desirable_diagnostic_criteria\": \"...\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# --- LOGIC WORKER ---\n",
        "async def merge_entity_group(session, tag, group, sem):\n",
        "    # 1. HARVEST & DEDUPLICATE FIGURES (Python Logic)\n",
        "    seen_urls = set()\n",
        "    all_unique_figures = []\n",
        "\n",
        "    # We also prepare a \"Text Only\" version for the AI to save tokens/confusion\n",
        "    text_only_group = []\n",
        "\n",
        "    for ent in group:\n",
        "        # Collect Figures\n",
        "        if ent.get('related_figures'):\n",
        "            for fig in ent['related_figures']:\n",
        "                url = fig.get('gcs_path')\n",
        "                if url and url not in seen_urls:\n",
        "                    seen_urls.add(url)\n",
        "                    all_unique_figures.append(fig)\n",
        "\n",
        "        # Prepare Text Payload (Strip figures to focus AI on text)\n",
        "        clean_ent = ent.copy()\n",
        "        clean_ent.pop('related_figures', None)\n",
        "        text_only_group.append(clean_ent)\n",
        "\n",
        "    # 2. AI TEXT MERGE\n",
        "    async with sem:\n",
        "        # If only 1 entry, just return it (but ensure figures are clean)\n",
        "        if len(group) == 1:\n",
        "            result = group[0]\n",
        "            result['related_figures'] = all_unique_figures\n",
        "            return result\n",
        "\n",
        "        entity_name = group[0].get('entity_name', 'Unknown')\n",
        "        prompt = construct_merge_prompt(entity_name, text_only_group)\n",
        "\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "\n",
        "        try:\n",
        "            async with session.post(url, json=payload) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "                    raw = data['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
        "                    if match:\n",
        "                        merged_ent = json.loads(match.group(0))\n",
        "\n",
        "                        # 3. INJECT FIGURES BACK\n",
        "                        merged_ent['related_figures'] = all_unique_figures\n",
        "                        return merged_ent\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Fallback: Return first entry with ALL figures appended\n",
        "        print(f\"‚ö†Ô∏è Merge failed for {entity_name}, stacking figures on first entry.\")\n",
        "        fallback = group[0]\n",
        "        fallback['related_figures'] = all_unique_figures\n",
        "        return fallback\n",
        "\n",
        "# --- MAIN ---\n",
        "async def main_consolidator():\n",
        "    # 1. Select Content\n",
        "    content_files = [b.name for b in bucket.list_blobs(prefix=PATHS['gcs_content_textbooks']) if \"_MASTER.json\" in b.name and \"_CONSOLIDATED\" not in b.name]\n",
        "    if not content_files: print(\"‚ùå No MASTER files found. Run Block 2 first.\"); return\n",
        "\n",
        "    print(\"\\n--- SELECT MASTER FILE TO CONSOLIDATE ---\")\n",
        "    for i, f in enumerate(content_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    c_idx = int(input(\"Choice: \")) - 1\n",
        "\n",
        "    master_path = content_files[c_idx]\n",
        "    raw_entities = gcs_load_json(master_path)\n",
        "\n",
        "    print(f\"\\nüöÄ Consolidating {len(raw_entities)} entities...\")\n",
        "\n",
        "    # 2. Group by Tag OR Entity Name\n",
        "    groups = defaultdict(list)\n",
        "    for ent in raw_entities:\n",
        "        # Priority: Tag > Entity Name > \"Unclassified\"\n",
        "        if ent.get('tags') and len(ent['tags']) > 0:\n",
        "            key = ent['tags'][0]\n",
        "        elif ent.get('entity_name'):\n",
        "            key = ent['entity_name']\n",
        "        else:\n",
        "            key = \"Unclassified\"\n",
        "\n",
        "        groups[key].append(ent)\n",
        "\n",
        "    print(f\"   -> Found {len(groups)} unique topics.\")\n",
        "\n",
        "    # 3. Process Groups\n",
        "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
        "    final_kb = []\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for key, group in groups.items():\n",
        "            tasks.append(merge_entity_group(session, key, group, sem))\n",
        "\n",
        "        results = await tqdm_asyncio.gather(*tasks)\n",
        "        final_kb = results\n",
        "\n",
        "    # 4. Save\n",
        "    out_path = master_path.replace(\"_MASTER.json\", \"_CONSOLIDATED.json\")\n",
        "    gcs_upload_json(final_kb, out_path)\n",
        "\n",
        "    # Count total figures to verify\n",
        "    total_figs = sum(len(e.get('related_figures', [])) for e in final_kb)\n",
        "\n",
        "    print(f\"\\n‚úÖ CONSOLIDATED SAVED: gs://{GCS_BUCKET_NAME}/{out_path}\")\n",
        "    print(f\"üìä Entities: {len(raw_entities)} -> {len(final_kb)}\")\n",
        "    print(f\"üñºÔ∏è Total Preserved Figures: {total_figs}\")\n",
        "\n",
        "await main_consolidator()"
      ],
      "metadata": {
        "id": "oZAVfDpP3_DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cacd8702-a579-480a-87ca-0b2d5a6914a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SELECT MASTER FILE TO CONSOLIDATE ---\n",
            "[1] Derm_Barnhill_MASTER.json\n",
            "[2] Derm_Elston_MASTER.json\n",
            "[3] Derm_Levers_MASTER.json\n",
            "[4] Derm_McKeeHY_MASTER.json\n",
            "[5] Derm_McKee_MASTER.json\n",
            "[6] Derm_Patterson_MASTER.json\n",
            "[7] Skin_Elston_MASTER.json\n",
            "Choice: 1\n",
            "\n",
            "üöÄ Consolidating 421 entities...\n",
            "   -> Found 334 unique topics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [00:39<00:00,  8.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ CONSOLIDATED SAVED: gs://pathology-hub-0/_content_library/textbooks/Derm_Barnhill_CONSOLIDATED.json\n",
            "üìä Entities: 421 -> 334\n",
            "üñºÔ∏è Total Preserved Figures: 352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLOCK 3: SINGLE FILE TRANSFORMER (Target: WHO Schema)"
      ],
      "metadata": {
        "id": "BG4EEqK_6V-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 3: SINGLE FILE TRANSFORMER (Target: WHO Schema)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import os\n",
        "from google.cloud import storage\n",
        "\n",
        "# --- SETUP ---\n",
        "if 'PATHS' not in globals():\n",
        "    raise NameError(\"‚ùå PATHS not found. Please run Block 0.\")\n",
        "\n",
        "bucket = storage.Client().bucket(PATHS['gcs_bucket'])\n",
        "\n",
        "# --- TRANSFORMATION LOGIC ---\n",
        "def transform_to_app_schema(entry, filename):\n",
        "    \"\"\"\n",
        "    Reshapes a single entity into the WHO App format.\n",
        "    \"\"\"\n",
        "    # 1. Clean Metadata / Noise\n",
        "    if any(x in entry.get('entity_name', '') for x in [\"Copyright\", \"Preface\", \"Index\", \"Contributors\"]):\n",
        "        return None\n",
        "\n",
        "    new_entry = entry.copy()\n",
        "\n",
        "    # 2. TRANSFORM MEDIA (The Core Requirement)\n",
        "    # Move 'related_figures' -> 'media' array\n",
        "    media_list = []\n",
        "\n",
        "    if 'related_figures' in entry:\n",
        "        for fig in entry['related_figures']:\n",
        "            # Construct Legend (Diagnosis + Description)\n",
        "            diag = fig.get('diagnosis', '').strip()\n",
        "            desc = fig.get('legend', '').strip()\n",
        "\n",
        "            # Avoid \"Lichen Planus. Lichen Planus.\" redundancy\n",
        "            if diag and desc and diag not in desc:\n",
        "                final_legend = f\"{diag}. {desc}\"\n",
        "            elif desc:\n",
        "                final_legend = desc\n",
        "            else:\n",
        "                final_legend = diag\n",
        "\n",
        "            # Build Object\n",
        "            media_item = {\n",
        "                \"type\": \"figure\", # Default for extracted images\n",
        "                \"path\": fig.get('gcs_path') or fig.get('src'),\n",
        "                \"legend\": final_legend\n",
        "            }\n",
        "\n",
        "            # Handle WSI (if present from other sources)\n",
        "            if fig.get('type') == 'wsi' or fig.get('isWSI'):\n",
        "                media_item['type'] = 'wsi'\n",
        "                media_item['url'] = fig.get('url') or fig.get('wsi_link')\n",
        "\n",
        "            media_list.append(media_item)\n",
        "\n",
        "        # Remove old key\n",
        "        del new_entry['related_figures']\n",
        "\n",
        "    new_entry['media'] = media_list\n",
        "\n",
        "    # 3. ENSURE REQUIRED KEYS (Even if null)\n",
        "    # This matches the WHO JSON structure you provided\n",
        "    required_keys = [\"video\", \"html\", \"wsi\"]\n",
        "    for k in required_keys:\n",
        "        if k not in new_entry:\n",
        "            new_entry[k] = None\n",
        "\n",
        "    # 4. MAP SPECIFIC FIELDS\n",
        "    # Map internal html_gcs_path to the 'html' field if it exists\n",
        "    if new_entry.get('html_gcs_path'):\n",
        "        new_entry['html'] = new_entry['html_gcs_path']\n",
        "    elif 'html' not in new_entry or new_entry['html'] is None:\n",
        "        # Optional: Generate a placeholder path if you need one, or keep null\n",
        "        new_entry['html'] = None\n",
        "\n",
        "    # 5. CLEANUP INTERNAL KEYS\n",
        "    keys_to_purge = ['html_gcs_path', 'gcs_origin', 'best_slide_id', 'source_type']\n",
        "    for k in keys_to_purge:\n",
        "        if k in new_entry: del new_entry[k]\n",
        "\n",
        "    return new_entry\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "def main_single_transformer():\n",
        "    # 1. Gather Available Files\n",
        "    print(\"üîç Scanning for Master/Consolidated files...\")\n",
        "    all_files = []\n",
        "\n",
        "    # Check Textbooks\n",
        "    blobs_t = list(bucket.list_blobs(prefix=PATHS['gcs_content_textbooks']))\n",
        "    for b in blobs_t:\n",
        "        if \"_MASTER.json\" in b.name or \"_CONSOLIDATED.json\" in b.name:\n",
        "            all_files.append(b.name)\n",
        "\n",
        "    # Check Lectures\n",
        "    blobs_l = list(bucket.list_blobs(prefix=PATHS['gcs_content_lectures']))\n",
        "    for b in blobs_l:\n",
        "        if \"_MASTER.json\" in b.name or \"_CONSOLIDATED.json\" in b.name:\n",
        "            all_files.append(b.name)\n",
        "\n",
        "    if not all_files:\n",
        "        print(\"‚ùå No processed files found. Run Block 2 or 3 first.\")\n",
        "        return\n",
        "\n",
        "    # 2. User Selection\n",
        "    print(\"\\n--- SELECT FILE TO TRANSFORM ---\")\n",
        "    for i, f in enumerate(all_files):\n",
        "        print(f\"[{i+1}] {f}\")\n",
        "\n",
        "    try:\n",
        "        idx = int(input(\"\\nEnter number: \")) - 1\n",
        "        selected_path = all_files[idx]\n",
        "    except:\n",
        "        print(\"‚ùå Invalid selection.\")\n",
        "        return\n",
        "\n",
        "    # 3. Load & Process\n",
        "    print(f\"\\nüöÄ Loading: {selected_path}\")\n",
        "    blob = bucket.blob(selected_path)\n",
        "    data = json.loads(blob.download_as_string())\n",
        "\n",
        "    transformed_data = []\n",
        "    for item in data:\n",
        "        res = transform_to_app_schema(item, selected_path)\n",
        "        if res:\n",
        "            transformed_data.append(res)\n",
        "\n",
        "    # 4. Save\n",
        "    # Naming convention: replace _MASTER or _CONSOLIDATED with _APP_READY\n",
        "    new_filename = selected_path.replace(\"_MASTER.json\", \"_APP_READY.json\").replace(\"_CONSOLIDATED.json\", \"_APP_READY.json\")\n",
        "\n",
        "    print(f\"üíæ Saving {len(transformed_data)} entries...\")\n",
        "    out_blob = bucket.blob(new_filename)\n",
        "    out_blob.upload_from_string(json.dumps(transformed_data, indent=2), content_type='application/json')\n",
        "\n",
        "    print(f\"\\n‚úÖ DONE. Output saved to:\")\n",
        "    print(f\"   gs://{PATHS['gcs_bucket']}/{new_filename}\")\n",
        "\n",
        "main_single_transformer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pplVULe3zmJr",
        "outputId": "4e6c6fb7-f075-4dd1-97c4-13eb0994c403"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Scanning for Master/Consolidated files...\n",
            "\n",
            "--- SELECT FILE TO TRANSFORM ---\n",
            "[1] _content_library/textbooks/Derm_Barnhill_CONSOLIDATED.json\n",
            "[2] _content_library/textbooks/Derm_Barnhill_MASTER.json\n",
            "[3] _content_library/textbooks/Derm_Elston_CONSOLIDATED.json\n",
            "[4] _content_library/textbooks/Derm_Elston_MASTER.json\n",
            "[5] _content_library/textbooks/Derm_Levers_CONSOLIDATED.json\n",
            "[6] _content_library/textbooks/Derm_Levers_MASTER.json\n",
            "[7] _content_library/textbooks/Derm_McKeeHY_MASTER.json\n",
            "[8] _content_library/textbooks/Derm_McKee_CONSOLIDATED.json\n",
            "[9] _content_library/textbooks/Derm_McKee_MASTER.json\n",
            "[10] _content_library/textbooks/Derm_Patterson_CONSOLIDATED.json\n",
            "[11] _content_library/textbooks/Derm_Patterson_MASTER.json\n",
            "[12] _content_library/textbooks/Skin_Elston_MASTER.json\n",
            "[13] _content_library/lectures/Derm_Lecture_30_miscellaneous_MASTER.json\n",
            "\n",
            "Enter number: 1\n",
            "\n",
            "üöÄ Loading: _content_library/textbooks/Derm_Barnhill_CONSOLIDATED.json\n",
            "üíæ Saving 334 entries...\n",
            "\n",
            "‚úÖ DONE. Output saved to:\n",
            "   gs://pathology-hub-0/_content_library/textbooks/Derm_Barnhill_APP_READY.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIDEO BLOCK 1"
      ],
      "metadata": {
        "id": "HVCDbcXT5MqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 1: LECTURE EXTRACTOR (Whisper + Gemini 3 Flash)\n",
        "# ==============================================================================\n",
        "import shutil, cv2, whisper, json, os, io, base64, re, asyncio, aiohttp\n",
        "import logging\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from google.cloud import storage\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "API_CONCURRENCY_LIMIT = 20\n",
        "VISION_MODEL = \"gemini-3-pro-preview\" # Fast & Cheap for per-slide analysis\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_upload_file(local_path, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_filename(local_path)\n",
        "\n",
        "def gcs_upload_json(data, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def gcs_exists(blob_path):\n",
        "    return bucket.blob(blob_path).exists()\n",
        "\n",
        "def get_comparison_frame(frame):\n",
        "    h, w = frame.shape[:2]\n",
        "    new_w = 200\n",
        "    new_h = int(h * (new_w / w))\n",
        "    small = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "    gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)\n",
        "    return cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# --- AI ANALYST ---\n",
        "async def analyze_slide_async(session, slide_data, local_img_path, sem):\n",
        "    async with sem:\n",
        "        if not os.path.exists(local_img_path): return slide_data\n",
        "\n",
        "        try:\n",
        "            # Prepare Image\n",
        "            with Image.open(local_img_path) as img:\n",
        "                buf = io.BytesIO()\n",
        "                img.convert(\"RGB\").save(buf, format=\"JPEG\")\n",
        "                b64_img = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "            url = f\"https://generativelanguage.googleapis.com/v1beta/models/{VISION_MODEL}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "            # Prompt: Extract raw visual data. We don't need deep reasoning yet, just \"What is on this slide?\"\n",
        "            prompt = (\n",
        "                f\"Transcript Context: \\\"{slide_data['raw_transcript'][:1000]}...\\\"\\n\\n\"\n",
        "                \"TASK: Analyze this slide image. \\n\"\n",
        "                \"1. Extract the Title.\\n\"\n",
        "                \"2. Extract text labels verbatim (e.g. 'CD45', 'H&E', '40x').\\n\"\n",
        "                \"3. Summarize the visual content (e.g., 'Histology showing...').\\n\"\n",
        "                \"Return JSON: {\\\"slide_title\\\": \\\"...\\\", \\\"key_points\\\": [\\\"...\\\"], \\\"visual_desc\\\": \\\"...\\\"}\"\n",
        "            )\n",
        "\n",
        "            payload = {\"contents\": [{\"parts\": [{\"text\": prompt}, {\"inline_data\": {\"mime_type\": \"image/jpeg\", \"data\": b64_img}}]}]}\n",
        "\n",
        "            async with session.post(url, json=payload) as res:\n",
        "                if res.status == 200:\n",
        "                    dat = await res.json()\n",
        "                    txt = dat['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', txt, re.DOTALL)\n",
        "                    if match:\n",
        "                        slide_data.update(json.loads(match.group(0)))\n",
        "        except Exception as e:\n",
        "            pass # Skip frame if AI fails\n",
        "\n",
        "        return slide_data\n",
        "\n",
        "# --- PIPELINE ---\n",
        "async def process_video(video_path, counter, total):\n",
        "    fname = os.path.basename(video_path)\n",
        "    lecture_name = os.path.splitext(fname)[0].replace(\" \", \"_\")\n",
        "\n",
        "    # GCS Paths\n",
        "    asset_base = f\"{PATHS['gcs_asset_lectures']}/{lecture_name}\"\n",
        "    raw_json_path = f\"{PATHS['gcs_content_lectures']}/{lecture_name}_RAW.json\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\\nüé• PROCESSING {counter}/{total}: {lecture_name}\\n{'='*60}\")\n",
        "\n",
        "    if gcs_exists(raw_json_path):\n",
        "        print(\"‚úÖ Already processed in GCS. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # 1. WHISPER TRANSCRIPTION\n",
        "    print(\"üéôÔ∏è Step 1: Whisper Transcription...\")\n",
        "    model = whisper.load_model(\"base\") # Use 'small' if you have GPU RAM, 'base' is fast\n",
        "    result = model.transcribe(video_path, fp16=False)\n",
        "\n",
        "    # 2. FRAME EXTRACTION & MERGING\n",
        "    print(\"üéûÔ∏è Step 2: Extracting Slides...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    slides = []\n",
        "    curr_slide = None\n",
        "    prev_cmp = None\n",
        "\n",
        "    # We use TQDM to track progress through the audio segments\n",
        "    for seg in tqdm(result['segments'], desc=\"Scanning\", unit=\"seg\"):\n",
        "        cap.set(cv2.CAP_PROP_POS_MSEC, seg['start'] * 1000)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: continue\n",
        "\n",
        "        curr_cmp = get_comparison_frame(frame)\n",
        "\n",
        "        if curr_slide is None:\n",
        "            curr_slide = {**seg, 'frame': frame}\n",
        "            prev_cmp = curr_cmp\n",
        "            continue\n",
        "\n",
        "        # SSIM Check (Merge if > 85% similar)\n",
        "        if ssim(prev_cmp, curr_cmp, data_range=255) >= 0.85:\n",
        "            curr_slide['text'] += \" \" + seg['text']\n",
        "            curr_slide['end'] = seg['end']\n",
        "        else:\n",
        "            slides.append(curr_slide)\n",
        "            curr_slide = {**seg, 'frame': frame}\n",
        "            prev_cmp = curr_cmp\n",
        "\n",
        "    if curr_slide: slides.append(curr_slide)\n",
        "    cap.release()\n",
        "    print(f\"   -> Consolidated into {len(slides)} unique slides.\")\n",
        "\n",
        "    # 3. UPLOAD & PREPARE\n",
        "    print(\"‚òÅÔ∏è Step 3: Uploading Images...\")\n",
        "    final_data = []\n",
        "    local_imgs = {} # Map id -> local path for AI step\n",
        "\n",
        "    for i, slide in enumerate(slides):\n",
        "        img_name = f\"{lecture_name}_slide_{i+1:04d}.jpg\"\n",
        "        local_p = f\"/tmp/{img_name}\"\n",
        "        gcs_p = f\"{asset_base}/{img_name}\"\n",
        "        full_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_p}\"\n",
        "\n",
        "        cv2.imwrite(local_p, slide['frame'])\n",
        "\n",
        "        if not gcs_exists(gcs_p):\n",
        "            gcs_upload_file(local_p, gcs_p)\n",
        "\n",
        "        local_imgs[i] = local_p\n",
        "\n",
        "        final_data.append({\n",
        "            \"id\": i,\n",
        "            \"timestamp_start\": slide['start'],\n",
        "            \"timestamp_end\": slide['end'],\n",
        "            \"raw_transcript\": slide['text'].strip(),\n",
        "            \"image_path\": full_uri,\n",
        "            \"gcs_path\": full_uri, # Important for Block 2\n",
        "            \"slide_title\": \"\",\n",
        "            \"key_points\": [],\n",
        "            \"visual_desc\": \"\"\n",
        "        })\n",
        "\n",
        "    # 4. GEMINI ENHANCEMENT\n",
        "    print(\"üß† Step 4: Gemini Vision Analysis...\")\n",
        "    sem = asyncio.Semaphore(API_CONCURRENCY_LIMIT)\n",
        "    async with aiohttp.ClientSession() as sess:\n",
        "        tasks = [analyze_slide_async(sess, d, local_imgs[d['id']], sem) for d in final_data]\n",
        "        enhanced_data = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "    # 5. SAVE RAW JSON\n",
        "    gcs_upload_json(enhanced_data, raw_json_path)\n",
        "    print(f\"‚úÖ Saved RAW data: {raw_json_path}\")\n",
        "\n",
        "    # Cleanup\n",
        "    for p in local_imgs.values():\n",
        "        if os.path.exists(p): os.remove(p)\n",
        "\n",
        "# --- RUNNER ---\n",
        "async def main_lectures():\n",
        "    vid_files = sorted([f for f in os.listdir(PATHS['source_videos']) if f.lower().endswith(('.mp4', '.mov'))])\n",
        "    if not vid_files: print(\"‚ùå No videos found.\"); return\n",
        "\n",
        "    print(\"\\n--- AVAILABLE LECTURES ---\")\n",
        "    for i, v in enumerate(vid_files): print(f\"[{i+1}] {v}\")\n",
        "\n",
        "    sel = input(\"\\nSelect (e.g. 1, 3-5, or 'all'): \")\n",
        "    indices = set()\n",
        "    if sel == 'all': indices = range(len(vid_files))\n",
        "    else:\n",
        "        for part in sel.split(','):\n",
        "            if '-' in part:\n",
        "                s, e = map(int, part.split('-'))\n",
        "                indices.update(range(s-1, e))\n",
        "            elif part.strip().isdigit():\n",
        "                indices.add(int(part)-1)\n",
        "\n",
        "    for idx in sorted(list(indices)):\n",
        "        if 0 <= idx < len(vid_files):\n",
        "            await process_video(os.path.join(PATHS['source_videos'], vid_files[idx]), idx+1, len(indices))\n",
        "\n",
        "await main_lectures()"
      ],
      "metadata": {
        "id": "ZzGRiY1z5XBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIDEO BLOCK 2"
      ],
      "metadata": {
        "id": "x0-3O_Ex5Sf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 2: LECTURE ARCHITECT (Monolith + ID Swap)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import difflib\n",
        "import random\n",
        "from typing import List, Dict, Set, Any\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Using 1.5 Pro for deep reasoning on the whole transcript\n",
        "MODEL_NAME = \"gemini-3-pro-preview\"\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_read_text(blob_path: str) -> str:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return blob.download_as_string().decode('utf-8') if blob.exists() else \"\"\n",
        "\n",
        "def gcs_load_json(blob_path: str) -> List:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data: Any, blob_path: str):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def validate_tag(tag_input: Any, valid_set: Set[str]) -> str:\n",
        "    if not tag_input: return \"Skin::Unclassified\"\n",
        "    tag_str = tag_input[0] if isinstance(tag_input, list) and tag_input else str(tag_input)\n",
        "    clean = tag_str.strip()\n",
        "    if clean in valid_set: return clean\n",
        "    matches = difflib.get_close_matches(clean, list(valid_set), n=1, cutoff=0.7)\n",
        "    return matches[0] if matches else clean\n",
        "\n",
        "# --- LOGIC: THE ID SWAPPER ---\n",
        "def inject_real_paths(entities, slide_lookup_map):\n",
        "    \"\"\"\n",
        "    Replaces AI 'PLACEHOLDER' with real GCS paths based on Slide ID.\n",
        "    \"\"\"\n",
        "    for ent in entities:\n",
        "        if 'related_figures' in ent:\n",
        "            for fig in ent['related_figures']:\n",
        "                # The AI returns \"Slide_1\", we look up the real object\n",
        "                slide_id = fig.get('id')\n",
        "\n",
        "                if slide_id in slide_lookup_map:\n",
        "                    real_data = slide_lookup_map[slide_id]\n",
        "                    fig['src'] = real_data['gcs_path']\n",
        "                    fig['gcs_path'] = real_data['gcs_path']\n",
        "                    # Append timestamp to legend if missing\n",
        "                    time_str = f\"(Time: {real_data['timestamp_start']:.0f}s)\"\n",
        "                    if time_str not in fig.get('legend', ''):\n",
        "                        fig['legend'] = f\"{fig.get('legend', '')} {time_str}\".strip()\n",
        "                else:\n",
        "                    fig['src'] = None\n",
        "                    fig['gcs_path'] = None\n",
        "    return entities\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. PROMPT ENGINEERING\n",
        "# ------------------------------------------------------------------------------\n",
        "def construct_lecture_prompt(transcript_data, valid_tags_list):\n",
        "    return f\"\"\"\n",
        "Role: You are a Senior Dermatopathologist and Data Engineer.\n",
        "Objective: Convert the ENTIRE LECTURE provided below into a standardized Knowledge Base.\n",
        "\n",
        "INPUT DATA:\n",
        "- Chronological sequence of slides (ID, Visual Description, Transcript).\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. **Consolidate:** Merge discussion across multiple slides into single Disease Entities.\n",
        "2. **Detail Extraction (CRITICAL):**\n",
        "   - **Stains (IHC):** List every specific stain mentioned (e.g., \"CK20+\", \"TTF-1 negative\").\n",
        "   - **Genetics:** List specific mutations/translocations.\n",
        "3. **Figure Linking (ID SWAP):**\n",
        "   - Select the BEST slides for 'related_figures'.\n",
        "   - **IMPORTANT:** Use the exact ID provided (e.g., \"Slide_5\"). Leave `src` and `gcs_path` as \"PLACEHOLDER\".\n",
        "\n",
        "REQUIRED JSON SCHEMA:\n",
        "[\n",
        "  {{\n",
        "    \"entity_name\": \"Disease Name\",\n",
        "    \"definition\": \"...\",\n",
        "    \"tags\": [\"Exact_Tag\"],\n",
        "    \"html_gcs_path\": null,\n",
        "\n",
        "    \"clinical\": \"...\",\n",
        "    \"pathogenesis\": \"...\",\n",
        "    \"macroscopic\": \"...\",\n",
        "    \"microscopic\": \"...\",\n",
        "    \"ancillary_studies\": \"List ALL stains/molecular details.\",\n",
        "    \"differential_diagnosis\": \"...\",\n",
        "    \"staging\": \"...\",\n",
        "    \"prognosis_and_prediction\": \"...\",\n",
        "\n",
        "    \"related_figures\": [\n",
        "        {{\n",
        "            \"id\": \"Slide_X\",\n",
        "            \"src\": \"PLACEHOLDER\",\n",
        "            \"gcs_path\": \"PLACEHOLDER\",\n",
        "            \"diagnosis\": \"Disease Name\",\n",
        "            \"legend\": \"Specific description of this slide (e.g. 'CK20 dot-like positivity').\"\n",
        "        }}\n",
        "    ]\n",
        "  }}\n",
        "]\n",
        "\n",
        "REFERENCE TAGS:\n",
        "{valid_tags_list}\n",
        "\n",
        "LECTURE CONTENT:\n",
        "{transcript_data}\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. AI PROCESSING (SINGLE SHOT)\n",
        "# ------------------------------------------------------------------------------\n",
        "async def process_full_lecture(session, slides, valid_tags_text, valid_tags_set):\n",
        "    # 1. Build Monolith Input (Hiding URLs)\n",
        "    formatted_input = \"\"\n",
        "    slide_map = {} # ID -> Real Data\n",
        "\n",
        "    for s in slides:\n",
        "        slide_id = f\"Slide_{s['id']}\"\n",
        "        slide_map[slide_id] = s\n",
        "\n",
        "        formatted_input += f\"\\n--- ID: {slide_id} (Time: {s['timestamp_start']:.0f}s) ---\\n\"\n",
        "        formatted_input += f\"VISUAL: {s.get('visual_desc', '')}\\n\"\n",
        "        formatted_input += f\"KEY POINTS: {s.get('key_points', [])}\\n\"\n",
        "        formatted_input += f\"TRANSCRIPT: {s['raw_transcript']}\\n\"\n",
        "\n",
        "    print(f\"üì¶ Payload: {len(formatted_input)} chars. Sending to Gemini...\")\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "    payload = {\"contents\": [{\"parts\": [{\"text\": construct_lecture_prompt(formatted_input, valid_tags_text)}]}]}\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            # 10 min timeout for full lecture processing\n",
        "            async with session.post(url, json=payload, timeout=600) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "                    raw_txt = data['candidates'][0]['content']['parts'][0]['text']\n",
        "                    raw_txt = raw_txt.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "                    match = re.search(r'\\[.*\\]', raw_txt, re.DOTALL)\n",
        "                    if match:\n",
        "                        entities = json.loads(match.group(0))\n",
        "\n",
        "                        # 2. INJECT REAL PATHS\n",
        "                        entities = inject_real_paths(entities, slide_map)\n",
        "\n",
        "                        # 3. Validation\n",
        "                        valid_entities = []\n",
        "                        for ent in entities:\n",
        "                            ent['tags'] = [validate_tag(ent.get('tags', []), valid_tags_set)]\n",
        "                            ent['html_gcs_path'] = None\n",
        "\n",
        "                            # Null Filling\n",
        "                            req_keys = [\"microscopic\", \"ancillary_studies\", \"differential_diagnosis\"]\n",
        "                            for k in req_keys:\n",
        "                                if k not in ent: ent[k] = None\n",
        "\n",
        "                            valid_entities.append(ent)\n",
        "\n",
        "                        return valid_entities\n",
        "                    return []\n",
        "                else:\n",
        "                    print(f\"‚ùå API Error {response.status}: {await response.text()}\")\n",
        "                    await asyncio.sleep(5)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Exception: {e}\")\n",
        "            await asyncio.sleep(5)\n",
        "\n",
        "    return []\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MAIN WORKFLOW\n",
        "# ------------------------------------------------------------------------------\n",
        "async def main_lecture_definitive():\n",
        "    # 1. Tags\n",
        "    tag_files = [b.name for b in bucket.list_blobs(prefix=\"Tags/\") if b.name.endswith('.txt')]\n",
        "    print(\"\\n--- SELECT TAG LIST ---\")\n",
        "    for i, f in enumerate(tag_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    t_idx = int(input(\"Choice: \")) - 1\n",
        "    tags_text = gcs_read_text(tag_files[t_idx])\n",
        "    tags_set = set(l.strip() for l in tags_text.splitlines() if l.strip())\n",
        "\n",
        "    # 2. Lecture\n",
        "    raw_files = [b.name for b in bucket.list_blobs(prefix=PATHS['gcs_content_lectures']) if \"_RAW.json\" in b.name]\n",
        "    if not raw_files: print(\"‚ùå No RAW files.\"); return\n",
        "\n",
        "    print(\"\\n--- SELECT LECTURE ---\")\n",
        "    for i, f in enumerate(raw_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    c_idx = int(input(\"Choice: \")) - 1\n",
        "\n",
        "    raw_path = raw_files[c_idx]\n",
        "    lecture_name = raw_path.split('/')[-1].replace(\"_RAW.json\", \"\")\n",
        "    slides_data = gcs_load_json(raw_path)\n",
        "\n",
        "    print(f\"\\nüöÄ Processing: {lecture_name}\")\n",
        "    print(f\"   Mode: MONOLITH + ID SWAP\")\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        master_kb = await process_full_lecture(session, slides_data, tags_text, tags_set)\n",
        "\n",
        "    if master_kb:\n",
        "        final_path = f\"{PATHS['gcs_content_lectures']}/{lecture_name}_MASTER.json\"\n",
        "        gcs_upload_json(master_kb, final_path)\n",
        "        print(f\"\\n‚úÖ MASTER SAVED: gs://{GCS_BUCKET_NAME}/{final_path}\")\n",
        "        print(f\"üìä Extracted {len(master_kb)} High-Quality Entities\")\n",
        "    else:\n",
        "        print(\"‚ùå Architecture failed.\")\n",
        "\n",
        "await main_lecture_definitive()"
      ],
      "metadata": {
        "id": "GgyXTjmV5f3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLOCK 3: THE GRAND UNIFIER (Concatenate All Knowledge)"
      ],
      "metadata": {
        "id": "TnFQAQla6fnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 3: THE GRAND UNIFIER (Concatenate All Knowledge)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import os\n",
        "from google.cloud import storage\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OUTPUT_FILENAME = \"GLOBAL_KNOWLEDGE_BASE.json\"\n",
        "# Terms to filter out (Final Polish)\n",
        "BLACKLIST = [\"Copyright\", \"Preface\", \"Contents\", \"Index\", \"Contributors\", \"Dedication\", \"Title Page\"]\n",
        "\n",
        "# --- SETUP ---\n",
        "# Ensure PATHS exists (Run Block 0 if this fails)\n",
        "if 'PATHS' not in globals():\n",
        "    raise NameError(\"‚ùå PATHS not found. Please run Block 0.\")\n",
        "\n",
        "bucket = storage.Client().bucket(PATHS['gcs_bucket'])\n",
        "\n",
        "# --- HELPERS ---\n",
        "def get_best_files(prefix):\n",
        "    \"\"\"\n",
        "    Scans a directory and picks the best version of each book/lecture.\n",
        "    Priority: _CONSOLIDATED.json > _MASTER.json\n",
        "    \"\"\"\n",
        "    blobs = list(bucket.list_blobs(prefix=prefix))\n",
        "    files_map = {}\n",
        "\n",
        "    for b in blobs:\n",
        "        if not b.name.endswith(\".json\"): continue\n",
        "\n",
        "        # Parse filename\n",
        "        fname = b.name.split('/')[-1]\n",
        "        if \"_MASTER\" not in fname and \"_CONSOLIDATED\" not in fname: continue\n",
        "\n",
        "        # Get the base name (e.g., \"Derm_Weedon\")\n",
        "        base_name = fname.replace(\"_MASTER.json\", \"\").replace(\"_CONSOLIDATED.json\", \"\")\n",
        "\n",
        "        # Logic: If we already have a CONSOLIDATED version, keep it.\n",
        "        # If we see a CONSOLIDATED version now, overwrite whatever we had.\n",
        "        # Otherwise, take MASTER.\n",
        "        if \"_CONSOLIDATED\" in fname:\n",
        "            files_map[base_name] = b.name\n",
        "        elif base_name not in files_map:\n",
        "            files_map[base_name] = b.name\n",
        "\n",
        "    return files_map\n",
        "\n",
        "def load_and_tag(blob_name, source_type):\n",
        "    \"\"\"Loads JSON and injects source metadata.\"\"\"\n",
        "    blob = bucket.blob(blob_name)\n",
        "    try:\n",
        "        data = json.loads(blob.download_as_string())\n",
        "\n",
        "        valid_entries = []\n",
        "        filename = blob_name.split('/')[-1]\n",
        "        clean_name = filename.replace(\"_MASTER.json\", \"\").replace(\"_CONSOLIDATED.json\", \"\")\n",
        "\n",
        "        for ent in data:\n",
        "            # 1. Clean Noise\n",
        "            if any(x in ent.get('entity_name', '') for x in BLACKLIST):\n",
        "                continue\n",
        "\n",
        "            # 2. Inject Metadata (Critical for RAG citations)\n",
        "            ent['source_document'] = clean_name\n",
        "            ent['source_type'] = source_type\n",
        "            ent['gcs_origin'] = f\"gs://{PATHS['gcs_bucket']}/{blob_name}\"\n",
        "\n",
        "            valid_entries.append(ent)\n",
        "\n",
        "        return valid_entries\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error loading {blob_name}: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "def main_unifier():\n",
        "    print(f\"üöÄ Starting Grand Unification...\")\n",
        "    global_kb = []\n",
        "\n",
        "    # 1. Scan Textbooks\n",
        "    print(\"   Scanning Textbooks...\")\n",
        "    textbooks = get_best_files(PATHS['gcs_content_textbooks'])\n",
        "    print(f\"   -> Found {len(textbooks)} unique textbooks.\")\n",
        "\n",
        "    for base, path in tqdm(textbooks.items(), desc=\"Textbooks\"):\n",
        "        entries = load_and_tag(path, \"Textbook\")\n",
        "        global_kb.extend(entries)\n",
        "\n",
        "    # 2. Scan Lectures\n",
        "    print(\"   Scanning Lectures...\")\n",
        "    lectures = get_best_files(PATHS['gcs_content_lectures'])\n",
        "    print(f\"   -> Found {len(lectures)} unique lectures.\")\n",
        "\n",
        "    for base, path in tqdm(lectures.items(), desc=\"Lectures\"):\n",
        "        entries = load_and_tag(path, \"Lecture\")\n",
        "        global_kb.extend(entries)\n",
        "\n",
        "    # 3. Final Stats\n",
        "    print(f\"\\nüìä Total Entities Collected: {len(global_kb)}\")\n",
        "\n",
        "    # 4. Upload\n",
        "    print(f\"üíæ Saving Global Database...\")\n",
        "    final_blob = bucket.blob(OUTPUT_FILENAME)\n",
        "    final_blob.upload_from_string(json.dumps(global_kb, indent=2), content_type='application/json')\n",
        "\n",
        "    print(f\"\\n‚úÖ SUCCESS! The Master of Masters is ready.\")\n",
        "    print(f\"   gs://{PATHS['gcs_bucket']}/{OUTPUT_FILENAME}\")\n",
        "\n",
        "main_unifier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297,
          "referenced_widgets": [
            "b15272cc1eac4263bd2483ff224f309b",
            "ad2605415bc54648a036a3960216e73a",
            "2964d4df97b4486d8ef92bbaa49e6509",
            "49dbffde97e14a3d8524f4ba2b15c26b",
            "6c3c4935f50a469088628949c710c1ee",
            "89cb58fbed1f40bebdb05b85e549d3ae",
            "74564169a8614db9b69759b7fec2faab",
            "1e25dc7146b64ffa9c760d76b648211a",
            "51fd7cbf60fb405cbaf0a3676535558f",
            "f802a0cd5d32461e8a539316c4251991",
            "e1e67da2a0a444ebb588ba34fa8a475f",
            "e42abf5e76b4488285a620d622401837",
            "5357d7ef1acf4020b76bcf664f3f904a",
            "c3651d969d0e4a9e9f22a94ec8a08dcb",
            "048db4e0d8914a5a83d9db9c6ee51e73",
            "e4250cb715ba4a16ac7c63cf7079c8f1",
            "d8c5c92a699e4dfebc1316a59cddce73",
            "5711233e32c842f296f66b3660686f71",
            "8de74449c1dd47ffa184ab64f3829bbb",
            "edaf2e3b92dd4e3197586de503bdc060",
            "7162f37c70384ffab571d7bcd9a62ce2",
            "4fd8168fbe3b47d39cab55ae8c54e369"
          ]
        },
        "id": "pveXaVgh3yK3",
        "outputId": "6e3ae8ee-9bd4-404b-bd7b-c553dc5fff0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Grand Unification...\n",
            "   Scanning Textbooks...\n",
            "   -> Found 6 unique textbooks.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textbooks:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b15272cc1eac4263bd2483ff224f309b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Error loading _content_library/textbooks/Skin_Elston_MASTER.json: 'str' object has no attribute 'get'\n",
            "   Scanning Lectures...\n",
            "   -> Found 1 unique lectures.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Lectures:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e42abf5e76b4488285a620d622401837"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Total Entities Collected: 2153\n",
            "üíæ Saving Global Database...\n",
            "\n",
            "‚úÖ SUCCESS! The Master of Masters is ready.\n",
            "   gs://pathology-hub-0/GLOBAL_KNOWLEDGE_BASE.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "-1aa6ztr4GKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLOCK 6: WSI & FIGURE IMPORTER (Fixed Selector + WHO Logic)"
      ],
      "metadata": {
        "id": "U0ZS2kod5toN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 6: SMART BATCH IMPORTER (Save to Input Folder)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import difflib\n",
        "import random\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gemini-3-flash-preview\"\n",
        "CONCURRENCY_LIMIT = 10  # Moderate concurrency\n",
        "SOURCE_FOLDER_PREFIX = \"WSI_JSON/\"\n",
        "\n",
        "# --- SETUP ---\n",
        "if 'PATHS' not in globals():\n",
        "    raise NameError(\"‚ùå PATHS not found. Please run Block 0.\")\n",
        "bucket = storage.Client().bucket(PATHS['gcs_bucket'])\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_read_text(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return blob.download_as_string().decode('utf-8') if blob.exists() else \"\"\n",
        "\n",
        "def gcs_load_json(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. NORMALIZERS\n",
        "# ------------------------------------------------------------------------------\n",
        "def normalize_who_chapter(entry):\n",
        "    new_entry = {k: entry.get(k) for k in [\n",
        "        \"entity_name\", \"definition\", \"clinical\", \"pathogenesis\",\n",
        "        \"macroscopic\", \"microscopic\", \"ancillary_studies\",\n",
        "        \"differential_diagnosis\", \"staging\", \"prognosis_and_prediction\",\n",
        "        \"cytology\", \"diagnostic_molecular_pathology\",\n",
        "        \"related_terminology\", \"subtypes\"\n",
        "    ]}\n",
        "\n",
        "    media_list = []\n",
        "    for fig in entry.get('related_figures', []):\n",
        "        legend = fig.get('legend', '')\n",
        "        diag = fig.get('diagnosis', '')\n",
        "        if diag and diag not in legend:\n",
        "            legend = f\"{diag}. {legend}\"\n",
        "        media_item = {\"legend\": legend}\n",
        "\n",
        "        if fig.get('isWSI') is True:\n",
        "            wsi_id = str(fig.get('id'))\n",
        "            media_item[\"type\"] = \"wsi\"\n",
        "            media_item[\"path\"] = f\"https://tumourclassification.iarc.who.int/static/dzi/{wsi_id}_files/10/0_0.jpeg\"\n",
        "            media_item[\"url\"] = f\"https://tumourclassification.iarc.who.int/Viewer/Index2?fid={wsi_id}\"\n",
        "        else:\n",
        "            media_item[\"type\"] = \"figure\"\n",
        "            media_item[\"path\"] = fig.get('src') or fig.get('gcs_path')\n",
        "            if not media_item[\"path\"]: continue\n",
        "        media_list.append(media_item)\n",
        "\n",
        "    new_entry['media'] = media_list\n",
        "    return new_entry\n",
        "\n",
        "def normalize_simple(entry, source_type):\n",
        "    thumb = entry.get('Thumbnail')\n",
        "    if source_type == \"PathPresenter\": thumb = None\n",
        "    elif not thumb: thumb = None\n",
        "\n",
        "    return {\n",
        "        \"entity_name\": entry.get('Diagnosis'),\n",
        "        \"media\": [{\n",
        "            \"type\": \"wsi\",\n",
        "            \"path\": thumb,\n",
        "            \"url\": entry.get('URL'),\n",
        "            \"legend\": entry.get('Diagnosis')\n",
        "        }]\n",
        "    }\n",
        "\n",
        "def detect_format(filename):\n",
        "    f = filename.lower()\n",
        "    if \"who\" in f: return \"WHO\"\n",
        "    if \"pp_\" in f or \"pathpresenter\" in f: return \"PathPresenter\"\n",
        "    if \"leeds\" in f: return \"Leeds\"\n",
        "    if \"mgh\" in f: return \"MGH\"\n",
        "    if \"rosai\" in f: return \"Rosai\"\n",
        "    return \"Unknown\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. ROBUST AI TAGGING\n",
        "# ------------------------------------------------------------------------------\n",
        "async def assign_tag_async(session, entity, valid_tags_text, valid_tags_set, sem):\n",
        "    async with sem:\n",
        "        diag = entity.get('entity_name', '')\n",
        "        if not diag:\n",
        "            entity['tags'] = [\"Skin::Unclassified\"]\n",
        "            return entity\n",
        "\n",
        "        # Check existing tags first (Save API calls if already tagged properly)\n",
        "        if entity.get('tags') and isinstance(entity['tags'], list) and entity['tags'][0] in valid_tags_set:\n",
        "            return entity\n",
        "\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Role: Pathology Taxonomist.\n",
        "        Task: Map this diagnosis to the EXACT tag from the list.\n",
        "\n",
        "        DIAGNOSIS: \"{diag}\"\n",
        "\n",
        "        INSTRUCTIONS:\n",
        "        1. Ignore suffixes like \"(HE)\", \"(Actin)\", or case numbers.\n",
        "        2. Find the best match in the VALID TAGS list.\n",
        "        3. Return ONLY the tag string.\n",
        "\n",
        "        VALID TAGS:\n",
        "        {valid_tags_text}\n",
        "        \"\"\"\n",
        "\n",
        "        for attempt in range(3):\n",
        "            try:\n",
        "                async with session.post(url, json={\"contents\": [{\"parts\": [{\"text\": prompt}]}]}) as response:\n",
        "                    if response.status == 200:\n",
        "                        data = await response.json()\n",
        "                        tag = data['candidates'][0]['content']['parts'][0]['text'].strip()\n",
        "\n",
        "                        if tag in valid_tags_set:\n",
        "                            entity['tags'] = [tag]\n",
        "                        else:\n",
        "                            matches = difflib.get_close_matches(tag, list(valid_tags_set), n=1, cutoff=0.7)\n",
        "                            if matches:\n",
        "                                entity['tags'] = [matches[0]]\n",
        "                            elif \"::\" in tag:\n",
        "                                entity['tags'] = [tag] # Accept if it looks like a valid hierarchy\n",
        "                            else:\n",
        "                                entity['tags'] = [\"Skin::Unclassified\"]\n",
        "                        return entity\n",
        "\n",
        "                    elif response.status == 429:\n",
        "                        await asyncio.sleep(2 * (attempt + 1))\n",
        "                        continue\n",
        "                    else:\n",
        "                        print(f\"‚ùå API Error {response.status}: {await response.text()}\")\n",
        "                        return entity\n",
        "\n",
        "            except Exception as e:\n",
        "                await asyncio.sleep(1)\n",
        "\n",
        "        entity['tags'] = [\"Skin::Unclassified\"]\n",
        "        return entity\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. MAIN EXECUTION\n",
        "# ------------------------------------------------------------------------------\n",
        "async def main_smart_batch():\n",
        "    # 1. Tags\n",
        "    tag_files = [b.name for b in bucket.list_blobs(prefix=\"Tags/\") if b.name.endswith('.txt')]\n",
        "    print(\"\\n--- SELECT TAG LIST ---\")\n",
        "    for i, f in enumerate(tag_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    t_idx = int(input(\"Choice: \")) - 1\n",
        "    tags_text = gcs_read_text(tag_files[t_idx])\n",
        "    tags_set = set(l.strip() for l in tags_text.splitlines() if l.strip())\n",
        "\n",
        "    # 2. File Scanner\n",
        "    print(f\"\\nüîç Scanning {SOURCE_FOLDER_PREFIX} ...\")\n",
        "    all_blobs = list(bucket.list_blobs(prefix=SOURCE_FOLDER_PREFIX))\n",
        "    candidates = []\n",
        "    ignore = [\"_APP_READY\", \"_MASTER\", \"_CONSOLIDATED\"]\n",
        "\n",
        "    for b in all_blobs:\n",
        "        if not b.name.endswith(\".json\"): continue\n",
        "        if any(x in b.name for x in ignore): continue\n",
        "        candidates.append(b.name)\n",
        "\n",
        "    if not candidates: print(\"‚ùå No raw files found.\"); return\n",
        "\n",
        "    print(\"\\n--- AVAILABLE FILES ---\")\n",
        "    for i, f in enumerate(candidates): print(f\"[{i+1}] {f}\")\n",
        "\n",
        "    sel_input = input(\"\\nSelect files (e.g. 1, 3-5, all): \").strip().lower()\n",
        "\n",
        "    selected_indices = set()\n",
        "    if sel_input == 'all':\n",
        "        selected_indices = set(range(len(candidates)))\n",
        "    else:\n",
        "        for p in sel_input.split(','):\n",
        "            if '-' in p:\n",
        "                start, end = map(int, p.split('-'))\n",
        "                selected_indices.update(range(start-1, end))\n",
        "            elif p.strip().isdigit():\n",
        "                selected_indices.add(int(p)-1)\n",
        "\n",
        "    # 3. Process Loop\n",
        "    for idx in sorted(list(selected_indices)):\n",
        "        if idx < 0 or idx >= len(candidates): continue\n",
        "\n",
        "        src_blob = candidates[idx]\n",
        "        fmt = detect_format(src_blob)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üöÄ Processing: {src_blob}\")\n",
        "        print(f\"   Detected Format: {fmt}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        if fmt == \"Unknown\":\n",
        "            print(f\"‚ö†Ô∏è Skipping (Unknown Format)\")\n",
        "            continue\n",
        "\n",
        "        raw_data = gcs_load_json(src_blob)\n",
        "        normalized_data = []\n",
        "        for item in raw_data:\n",
        "            norm = normalize_who_chapter(item) if fmt == \"WHO\" else normalize_simple(item, fmt)\n",
        "            if norm:\n",
        "                for k in [\"video\", \"html\", \"wsi\", \"definition\", \"clinical\", \"microscopic\"]:\n",
        "                    if k not in norm: norm[k] = None\n",
        "                normalized_data.append(norm)\n",
        "\n",
        "        print(f\"üß† Tagging {len(normalized_data)} entities...\")\n",
        "        sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
        "        final_data = []\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = [assign_tag_async(session, ent, tags_text, tags_set, sem) for ent in normalized_data]\n",
        "            final_data = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "        # SAVE TO SAME FOLDER\n",
        "        # Replaces .json with _APP_READY.json, keeping full path\n",
        "        out_name = src_blob.replace(\".json\", \"_APP_READY.json\")\n",
        "        gcs_upload_json(final_data, out_name)\n",
        "\n",
        "        valid_count = sum(1 for e in final_data if \"Unclassified\" not in e['tags'][0])\n",
        "        print(f\"‚úÖ Saved: {out_name}\")\n",
        "        print(f\"   Tagged: {valid_count}/{len(final_data)}\")\n",
        "\n",
        "    print(\"\\nüéâ Batch Complete.\")\n",
        "\n",
        "await main_smart_batch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Js4scDS4Htq",
        "outputId": "4e46967f-3c64-428b-91c7-ad4bcdf7a849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SELECT TAG LIST ---\n",
            "[1] BST_Tags.txt\n",
            "[2] Breast_Tags.txt\n",
            "[3] Endo_Tags.txt\n",
            "[4] GI_Tags.txt\n",
            "[5] GYN_Tags.txt\n",
            "[6] Skin_Tags.txt\n",
            "Choice: 6\n",
            "\n",
            "üîç Scanning WSI_JSON/ ...\n",
            "\n",
            "--- AVAILABLE FILES ---\n",
            "[1] WSI_JSON/Leeds_WSI_Skin.json\n",
            "[2] WSI_JSON/PP_Skin_1-500.json\n",
            "[3] WSI_JSON/PP_Skin_1001-1500.json\n",
            "[4] WSI_JSON/PP_Skin_1501-2000.json\n",
            "[5] WSI_JSON/PP_Skin_2001-2500.json\n",
            "[6] WSI_JSON/PP_Skin_2501-3000.json\n",
            "[7] WSI_JSON/PP_Skin_3001-3500.json\n",
            "[8] WSI_JSON/PP_Skin_3501-4000.json\n",
            "[9] WSI_JSON/PP_Skin_4001-4500.json\n",
            "[10] WSI_JSON/PP_Skin_4501-5000.json\n",
            "[11] WSI_JSON/PP_Skin_5001-6000.json\n",
            "[12] WSI_JSON/PP_Skin_501-1000.json\n",
            "[13] WSI_JSON/Rosai_Skin_Links.json\n",
            "[14] WSI_JSON/Skin_MGH_Links.json\n",
            "\n",
            "Select files (e.g. 1, 3-5, all): 2\n",
            "\n",
            "============================================================\n",
            "üöÄ Processing: WSI_JSON/PP_Skin_1-500.json\n",
            "   Detected Format: PathPresenter\n",
            "============================================================\n",
            "üß† Tagging 501 entities...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|‚ñà‚ñà‚ñè       | 112/501 [02:31<27:54,  4.31s/it]"
          ]
        }
      ]
    }
  ]
}