{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP9TDiItV/K72LGHautVKCH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/herndoch/dermopath-ai-hub/blob/main/Knowledge_Pipeline_v4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 0"
      ],
      "metadata": {
        "id": "BpbB--6v4yXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 0: UNIVERSAL SETUP (Textbooks + Lectures)\n",
        "# ==============================================================================\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive, userdata, auth\n",
        "from google.cloud import storage\n",
        "import google.generativeai as genai\n",
        "\n",
        "print(\"--- STEP 0: INITIALIZATION ---\")\n",
        "\n",
        "# 1. Install & Configure System (Textbooks + Whisper/Video tools)\n",
        "print(\"üì¶ Installing dependencies (PDF, Video, AI)...\")\n",
        "!sudo apt-get update -qq && sudo apt-get install -y ffmpeg > /dev/null 2>&1\n",
        "!pip install -q -U google-generativeai PyMuPDF scikit-image aiohttp tqdm openai-whisper opencv-python-headless\n",
        "\n",
        "# 2. Authentication\n",
        "print(\"üîë Authenticating with Google Cloud...\")\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "except Exception as e:\n",
        "    raise SystemExit(f\"‚ùå Authentication Failed: {e}\")\n",
        "\n",
        "# 3. Mount Drive (Source Storage)\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# 4. Universal Configuration\n",
        "GCS_BUCKET_NAME = 'pathology-hub-0'\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/1-Projects/Knowledge_Pipeline'\n",
        "\n",
        "# Initialize GCS Client\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "\n",
        "# --- THE MASTER PATH MAP ---\n",
        "# This dictionary handles routing for BOTH workflows.\n",
        "PATHS = {\n",
        "    # --- SOURCES (Local Google Drive) ---\n",
        "    \"source_pdfs\":      os.path.join(DRIVE_ROOT, '_source_materials', 'pdfs'),\n",
        "    \"source_videos\":    os.path.join(DRIVE_ROOT, '_source_materials', 'videos'),\n",
        "\n",
        "    # --- DESTINATIONS (GCS Bucket Paths) ---\n",
        "    \"gcs_bucket\":       GCS_BUCKET_NAME,\n",
        "    \"gcs_tags\":         \"Tags\",  # Where your _Tags.txt files live\n",
        "\n",
        "    # Textbook Pipeline\n",
        "    \"gcs_asset_textbooks\":   \"_asset_library/textbooks\",\n",
        "    \"gcs_content_textbooks\": \"_content_library/textbooks\",\n",
        "\n",
        "    # Lecture Pipeline\n",
        "    \"gcs_asset_lectures\":    \"_asset_library/lectures\",\n",
        "    \"gcs_content_lectures\":  \"_content_library/lectures\"\n",
        "}\n",
        "\n",
        "# 5. Verification\n",
        "print(f\"\\n‚úÖ Connected to Bucket: gs://{GCS_BUCKET_NAME}\")\n",
        "print(f\"‚úÖ Source PDFs:   {PATHS['source_pdfs']}\")\n",
        "print(f\"‚úÖ Source Videos: {PATHS['source_videos']}\")\n",
        "print(\"\\nüöÄ SYSTEM READY. You can now run Block 1 (Textbook) or Block 1 (Lecture).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBl8jRWY41vc",
        "outputId": "e31c7d41-4257-4c0a-ae8f-e175ff441d16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STEP 0: INITIALIZATION ---\n",
            "üì¶ Installing dependencies (PDF, Video, AI)...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "üîë Authenticating with Google Cloud...\n",
            "Mounted at /content/drive\n",
            "\n",
            "‚úÖ Connected to Bucket: gs://pathology-hub-0\n",
            "‚úÖ Source PDFs:   /content/drive/MyDrive/1-Projects/Knowledge_Pipeline/_source_materials/pdfs\n",
            "‚úÖ Source Videos: /content/drive/MyDrive/1-Projects/Knowledge_Pipeline/_source_materials/videos\n",
            "\n",
            "üöÄ SYSTEM READY. You can now run Block 1 (Textbook) or Block 1 (Lecture).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF BLOCK 1: TEXTBOOK EXTRACTOR (Text + Figures)"
      ],
      "metadata": {
        "id": "qp8ILGTF4otI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 1: TEXTBOOK EXTRACTOR (Gemini 3 Flash - Panel Aware)\n",
        "# ==============================================================================\n",
        "import base64\n",
        "import fitz  # PyMuPDF\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import os\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from google.cloud import storage\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TEXT_CONCURRENCY = 20\n",
        "VISION_CONCURRENCY = 20\n",
        "TEXT_MODEL_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent?key={GEMINI_API_KEY}\"\n",
        "VISION_MODEL_NAME = \"gemini-3-flash-preview\"\n",
        "\n",
        "# --- HELPER: GCS UTILS ---\n",
        "def gcs_exists(blob_path):\n",
        "    return bucket.blob(blob_path).exists()\n",
        "\n",
        "def gcs_upload_bytes(data, blob_path, content_type):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(data, content_type=content_type)\n",
        "\n",
        "def gcs_upload_json(data, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def gcs_load_json(blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    if blob.exists():\n",
        "        return json.loads(blob.download_as_string())\n",
        "    return []\n",
        "\n",
        "# --- AI HELPERS ---\n",
        "async def clean_text_async(session, text, page_num, sem):\n",
        "    async with sem:\n",
        "        if not text.strip(): return page_num, \"\"\n",
        "        prompt = (\n",
        "            \"Clean this medical text. Fix OCR errors. Keep structure. \"\n",
        "            \"Preserve Figure Captions exactly. Return JSON: {\\\"markdown\\\": \\\"...\\\"}\"\n",
        "            f\"\\n\\nRAW TEXT:\\n{text}\"\n",
        "        )\n",
        "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "        try:\n",
        "            async with session.post(TEXT_MODEL_URL, json=payload) as res:\n",
        "                if res.status == 200:\n",
        "                    dat = await res.json()\n",
        "                    raw = dat['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
        "                    if match: return page_num, json.loads(match.group(0)).get(\"markdown\", text)\n",
        "                return page_num, text\n",
        "        except: return page_num, text\n",
        "\n",
        "async def analyze_figure_async(session, b64_img, context, sem):\n",
        "    \"\"\"\n",
        "    Panel-Aware Vision Analysis.\n",
        "    Tries to distinguish if the image is just Panel A or Panel B of a multipart figure.\n",
        "    \"\"\"\n",
        "    async with sem:\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{VISION_MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        PAGE CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        TASK: Analyze the image below.\n",
        "        1. Identify the Figure ID (e.g. \"Fig 2.1\") from the context that matches this image.\n",
        "        2. **MULTI-PANEL CHECK:**\n",
        "           - Does the caption describe multiple parts (e.g. \"(A) ... (B) ...\")?\n",
        "           - If yes, determine if THIS specific image is Panel A, Panel B, etc.\n",
        "           - If this image is ONLY Panel A, try to extract ONLY the caption text for (A).\n",
        "           - If you cannot split the text, return the full caption but add \"(Panel A)\" to the ID.\n",
        "\n",
        "        Return JSON: {{\"figure_id\": \"Fig X.X (Panel A)\", \"matched_caption\": \"Specific caption...\"}} or null.\n",
        "        \"\"\"\n",
        "\n",
        "        parts = [\n",
        "            {\"text\": prompt},\n",
        "            {\"inline_data\": {\"mime_type\": \"image/png\", \"data\": b64_img}}\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            async with session.post(url, json={\"contents\": [{\"parts\": parts}]}) as res:\n",
        "                if res.status == 200:\n",
        "                    dat = await res.json()\n",
        "                    raw = dat['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
        "                    if match: return json.loads(match.group(0))\n",
        "        except: return None\n",
        "        return None\n",
        "\n",
        "# --- MAIN PROCESSOR ---\n",
        "async def process_textbook(pdf_path, start_p=1, end_p=None):\n",
        "    fname = os.path.basename(pdf_path)\n",
        "    book_name = os.path.splitext(fname)[0].replace(' ', '_')\n",
        "\n",
        "    base_asset = f\"{PATHS['gcs_asset_textbooks']}/{book_name}\"\n",
        "    path_fig_imgs = f\"{base_asset}/figure_images\"\n",
        "    path_content = f\"{PATHS['gcs_content_textbooks']}/{book_name}_CONTENT.json\"\n",
        "    path_figures = f\"{PATHS['gcs_content_textbooks']}/{book_name}_FIGURES.json\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\\nüìò PROCESSING: {book_name}\\n{'='*60}\")\n",
        "\n",
        "    doc = fitz.open(pdf_path)\n",
        "    total = len(doc)\n",
        "    final_p = min(end_p or total, total)\n",
        "\n",
        "    # 1. TEXT (Skip if done)\n",
        "    existing_content = gcs_load_json(path_content)\n",
        "    if not existing_content:\n",
        "        print(f\"üìù Phase 1: Cleaning Text...\")\n",
        "        sem = asyncio.Semaphore(TEXT_CONCURRENCY)\n",
        "        async with aiohttp.ClientSession() as sess:\n",
        "            tasks = [clean_text_async(sess, doc.load_page(p).get_text(\"text\"), p+1, sem) for p in range(start_p-1, final_p)]\n",
        "            results = await tqdm_asyncio.gather(*tasks)\n",
        "        content_data = sorted([{\"source\": fname, \"page_number\": p, \"content\": t} for p, t in results], key=lambda x: x['page_number'])\n",
        "        gcs_upload_json(content_data, path_content)\n",
        "    else:\n",
        "        content_data = existing_content\n",
        "\n",
        "    content_map = {c['page_number']: c['content'] for c in content_data}\n",
        "\n",
        "    # 2. FIGURES\n",
        "    print(\"üñºÔ∏è Phase 2: Figures & Vision (Panel-Aware)...\")\n",
        "    existing_figs = gcs_load_json(path_figures)\n",
        "    processed_pages = {f['source_page'] for f in existing_figs}\n",
        "    vision_tasks = []\n",
        "    new_figures = []\n",
        "    sem_vis = asyncio.Semaphore(VISION_CONCURRENCY)\n",
        "\n",
        "    for p_idx in range(start_p-1, final_p):\n",
        "        p_num = p_idx + 1\n",
        "        if p_num in processed_pages: continue\n",
        "\n",
        "        page = doc.load_page(p_idx)\n",
        "        images = page.get_images(full=True)\n",
        "        if not images: continue\n",
        "\n",
        "        md_ctx = content_map.get(p_num, \"\")\n",
        "\n",
        "        for i, img in enumerate(images):\n",
        "            try:\n",
        "                xref = img[0]\n",
        "                base = doc.extract_image(xref)\n",
        "                if len(base[\"image\"]) < 5000: continue\n",
        "\n",
        "                img_name = f\"{book_name}_page_{p_num}_img_{i+1}.{base['ext']}\"\n",
        "                blob_path = f\"{path_fig_imgs}/{img_name}\"\n",
        "                full_uri = f\"gs://{GCS_BUCKET_NAME}/{blob_path}\"\n",
        "\n",
        "                if not gcs_exists(blob_path):\n",
        "                    gcs_upload_bytes(base[\"image\"], blob_path, f\"image/{base['ext']}\")\n",
        "\n",
        "                b64 = base64.b64encode(base[\"image\"]).decode('utf-8')\n",
        "                vision_tasks.append({\n",
        "                    \"b64\": b64, \"ctx\": md_ctx,\n",
        "                    \"meta\": {\"source_page\": p_num, \"gcs_path\": full_uri}\n",
        "                })\n",
        "            except: pass\n",
        "\n",
        "    if vision_tasks:\n",
        "        print(f\"   -> Analyzing {len(vision_tasks)} figures...\")\n",
        "        async with aiohttp.ClientSession() as sess:\n",
        "            tasks = [analyze_figure_async(sess, t['b64'], t['ctx'], sem_vis) for t in vision_tasks]\n",
        "            results = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "            for i, res in enumerate(results):\n",
        "                if res and res.get('figure_id'):\n",
        "                    meta = vision_tasks[i]['meta']\n",
        "                    new_figures.append({\n",
        "                        \"source_document\": fname,\n",
        "                        \"source_page\": meta['source_page'],\n",
        "                        \"figure_id\": res['figure_id'],\n",
        "                        \"description\": res['matched_caption'],\n",
        "                        \"gcs_path\": meta['gcs_path']\n",
        "                    })\n",
        "\n",
        "        final_list = existing_figs + new_figures\n",
        "        final_list.sort(key=lambda x: x['source_page'])\n",
        "        gcs_upload_json(final_list, path_figures)\n",
        "        print(f\"   -> Added {len(new_figures)} figures.\")\n",
        "\n",
        "# --- RUNNER ---\n",
        "async def main():\n",
        "    pdfs = sorted([f for f in os.listdir(PATHS['source_pdfs']) if f.endswith('.pdf')])\n",
        "    if not pdfs: print(\"‚ùå No PDFs found.\"); return\n",
        "\n",
        "    print(\"\\n--- AVAILABLE TEXTBOOKS ---\")\n",
        "    for i, f in enumerate(pdfs): print(f\"[{i+1}] {f}\")\n",
        "\n",
        "    sel = input(\"\\nSelect book(s) (e.g. 1, 3): \")\n",
        "    indices = [int(x)-1 for x in sel.split(',') if x.strip().isdigit()]\n",
        "\n",
        "    for idx in indices:\n",
        "        if 0 <= idx < len(pdfs):\n",
        "            await process_textbook(os.path.join(PATHS['source_pdfs'], pdfs[idx]))\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF5BNCNP4qlL",
        "outputId": "c28715a1-af77-436a-8d7b-b9a659df4ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AVAILABLE TEXTBOOKS ---\n",
            "[1] BST Horvai.pdf\n",
            "[2] Bone Atlas.pdf\n",
            "[3] Bone Dorfman.pdf\n",
            "[4] Bone Pattern.pdf\n",
            "[5] Breast Atlas.pdf\n",
            "[6] Breast Biopsy.pdf\n",
            "[7] Breast FAQ.pdf\n",
            "[8] Breast Pattern.pdf\n",
            "[9] Cyto Breast Yokohama.pdf\n",
            "[10] Cyto Cibas.pdf\n",
            "[11] Cyto Comprehensive Part One.pdf\n",
            "[12] Cyto Comprehensive Part Two.pdf\n",
            "[13] Cyto GU Paris.pdf\n",
            "[14] Cyto Gyn Bethesda.pdf\n",
            "[15] Cyto Milan.pdf\n",
            "[16] Cyto PSC Lung.pdf\n",
            "[17] Cyto Pattern.pdf\n",
            "[18] Cyto Serous Fluids.pdf\n",
            "[19] Cyto Thyroid Bethesda.pdf\n",
            "[20] Derm Dermoscopy Atlas.pdf\n",
            "[21] Derm McKee.pdf\n",
            "[22] Derm_Elston.pdf\n",
            "[23] Derm_Levers.pdf\n",
            "[24] Derm_Patterson.pdf\n",
            "[25] Derm_Weedon.pdf\n",
            "[26] Endo Atlas.pdf\n",
            "[27] GI Atlas.pdf\n",
            "[28] GI Biopsy Interpretation (Neoplastic).pdf\n",
            "[29] GI Biopsy Interpretation (Non Neoplastic).pdf\n",
            "[30] GI Intestinal Atlas.pdf\n",
            "[31] GI Liver Atlas Chan.pdf\n",
            "[32] GI Liver Gonzalez.pdf\n",
            "[33] GI Liver Lefkowitch.pdf\n",
            "[34] GI Liver Macsween.pdf\n",
            "[35] GI Liver Mounajjed.pdf\n",
            "[36] GI Non-Neoplastic Zhang.pdf\n",
            "[37] GI Odze.pdf\n",
            "[38] GU Biopsy Interpretation (Prostate).pdf\n",
            "[39] Gyn Atlas Part One.pdf\n",
            "[40] Gyn Atlas Part Two.pdf\n",
            "[41] Gyn Essentials.pdf\n",
            "[42] HN Biopsy Interpretation.pdf\n",
            "[43] HN Cardesa.pdf\n",
            "[44] HN FAQ.pdf\n",
            "[45] HN Gnepp.pdf\n",
            "[46] HN Oral.pdf\n",
            "[47] HN Thompson.pdf\n",
            "[48] Micro Essentials Sastry.pdf\n",
            "[49] Peds Atlas Husain Stocker.pdf\n",
            "[50] Peds Course review.pdf\n",
            "[51] Peds Pediatric pathology.pdf\n",
            "[52] SoftTissue Enzinger.pdf\n",
            "[53] SoftTissue Pattern.pdf\n",
            "[54] xxx GI Mucosal Montgomery.pdf\n",
            "[55] xxx GU Biopsy Interpretation (Bladder).pdf\n",
            "\n",
            "Select book(s) (e.g. 1, 3): 24\n",
            "\n",
            "============================================================\n",
            "üìò PROCESSING: Derm_Patterson\n",
            "============================================================\n",
            "üìù Phase 1: Cleaning Text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|‚ñà‚ñè        | 88/781 [01:50<19:27,  1.69s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF BLOCK 2: TEXTBOOK ARCHITECT (High-Fidelity Monolith)\n"
      ],
      "metadata": {
        "id": "k0P6QtEh3vos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy6oq1WV3s_g"
      },
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 2: TEXTBOOK ARCHITECT (High-Fidelity Pro + Auto-Save)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import difflib\n",
        "import random\n",
        "import os\n",
        "from typing import List, Dict, Set, Any\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gemini-3-pro-preview\"    # \"Our Most Intelligent Model\"\n",
        "CONCURRENCY_LIMIT = 2                  # Strict limit to prevent 429s on Pro\n",
        "PAGES_PER_CHUNK = 40                   # 40 pages = optimal context window\n",
        "PAGE_OVERLAP = 2                       # Prevents cutting entities in half\n",
        "MAX_RETRIES = 10                       # Resilience against API timeouts\n",
        "SAVE_EVERY_N_CHUNKS = 5                # Auto-save to GCS every ~200 pages\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_read_text(blob_path: str) -> str:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return blob.download_as_string().decode('utf-8') if blob.exists() else \"\"\n",
        "\n",
        "def gcs_load_json(blob_path: str) -> List:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data: Any, blob_path: str):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def validate_tag(tag: str, valid_set: Set[str]) -> str:\n",
        "    if not tag: return \"Skin::Unclassified\"\n",
        "    clean = tag.strip()\n",
        "    if clean in valid_set: return clean\n",
        "    matches = difflib.get_close_matches(clean, list(valid_set), n=1, cutoff=0.7)\n",
        "    return matches[0] if matches else clean\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. PROMPT ENGINEERING (THE MENU METHOD)\n",
        "# ------------------------------------------------------------------------------\n",
        "def construct_textbook_prompt(text_content, figure_menu, valid_tags_list):\n",
        "    return f\"\"\"\n",
        "Role: You are a Senior Dermatopathologist and Data Engineer.\n",
        "Objective: Convert the provided TEXTBOOK CONTENT into a standardized Knowledge Base.\n",
        "\n",
        "INPUT CONTEXT:\n",
        "This input represents a ~{PAGES_PER_CHUNK} page section of a medical textbook.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. **Identify Entities:** Extract every distinct disease/pathology entity discussed.\n",
        "2. **High-Fidelity Extraction:**\n",
        "   - **Microscopic:** Extract detailed histological descriptions.\n",
        "   - **Ancillary Studies:** List specific stains (CD45+, S100-) and genetics (t(11;22)).\n",
        "3. **Figure Linking (THE MENU RULE):**\n",
        "   - Below is a \"MENU OF AVAILABLE FIGURES\" found on these pages.\n",
        "   - Each entry has an ID (e.g., Fig 1.2) and a GOLDEN LINK (gs://...).\n",
        "   - If the text mentions \"Figure 1.2 shows Lichen Planus\", you MUST find Fig 1.2 in the menu and copy the **GOLDEN LINK** exactly into the 'src' and 'gcs_path' fields.\n",
        "   - Do NOT invent paths. Only use paths from the menu.\n",
        "\n",
        "REQUIRED JSON SCHEMA (List of Objects):\n",
        "[\n",
        "  {{\n",
        "    \"entity_name\": \"Disease Name\",\n",
        "    \"definition\": \"...\",\n",
        "    \"tags\": [\"Exact_Tag_From_List\"],\n",
        "    \"html_gcs_path\": null,\n",
        "    \"clinical\": \"...\",\n",
        "    \"pathogenesis\": \"...\",\n",
        "    \"macroscopic\": \"...\",\n",
        "    \"microscopic\": \"Detailed histology...\",\n",
        "    \"cytology\": \"...\",\n",
        "    \"ancillary_studies\": \"List specific IHC/Molecular.\",\n",
        "    \"differential_diagnosis\": \"...\",\n",
        "    \"staging\": \"...\",\n",
        "    \"prognosis_and_prediction\": \"...\",\n",
        "\n",
        "    \"related_figures\": [\n",
        "        {{\n",
        "            \"id\": \"Figure ID (e.g. Fig 1.2)\",\n",
        "            \"src\": \"gs://... (COPY FROM MENU)\",\n",
        "            \"gcs_path\": \"gs://... (COPY FROM MENU)\",\n",
        "            \"diagnosis\": \"Disease Name\",\n",
        "            \"legend\": \"Full caption from text.\"\n",
        "        }}\n",
        "    ]\n",
        "  }}\n",
        "]\n",
        "\n",
        "REFERENCE TAGS:\n",
        "{valid_tags_list}\n",
        "\n",
        "--- MENU OF AVAILABLE FIGURES (GOLDEN LINKS) ---\n",
        "{figure_menu}\n",
        "\n",
        "--- TEXTBOOK CONTENT ---\n",
        "{text_content}\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. CHUNK PROCESSOR\n",
        "# ------------------------------------------------------------------------------\n",
        "async def process_textbook_chunk(session, chunk_data, figures_in_chunk, valid_tags_text, valid_tags_set, sem):\n",
        "    async with sem:\n",
        "        full_text = \"\\n\\n\".join([f\"--- Page {p['page_number']} ---\\n{p['content']}\" for p in chunk_data])\n",
        "\n",
        "        fig_menu_list = []\n",
        "        for f in figures_in_chunk:\n",
        "            fig_menu_list.append(\n",
        "                f\"[ID: {f.get('figure_id', 'Unknown')}] -> GOLDEN LINK: {f['gcs_path']} | Caption: {f.get('description', '')}\"\n",
        "            )\n",
        "        fig_menu = \"\\n\".join(fig_menu_list)\n",
        "\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "        payload = {\"contents\": [{\"parts\": [{\"text\": construct_textbook_prompt(full_text, fig_menu, valid_tags_text)}]}]}\n",
        "\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                # 10 minute timeout for deep reasoning\n",
        "                async with session.post(url, json=payload, timeout=600) as response:\n",
        "                    if response.status == 200:\n",
        "                        data = await response.json()\n",
        "                        raw_txt = data['candidates'][0]['content']['parts'][0]['text']\n",
        "                        raw_txt = raw_txt.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "                        match = re.search(r'\\[.*\\]', raw_txt, re.DOTALL)\n",
        "                        if match:\n",
        "                            entities = json.loads(match.group(0))\n",
        "                            valid_entities = []\n",
        "                            for ent in entities:\n",
        "                                if not ent.get('entity_name'): continue\n",
        "                                ent['tags'] = [validate_tag(t, valid_tags_set) for t in ent.get('tags', [])]\n",
        "                                ent['html_gcs_path'] = None\n",
        "\n",
        "                                # Enforce Schema Nulls\n",
        "                                keys = [\"clinical\", \"microscopic\", \"ancillary_studies\", \"differential_diagnosis\", \"pathogenesis\"]\n",
        "                                for k in keys:\n",
        "                                    if k not in ent: ent[k] = None\n",
        "\n",
        "                                valid_entities.append(ent)\n",
        "                            return valid_entities\n",
        "                        return []\n",
        "                    elif response.status == 429:\n",
        "                        wait = (2 ** attempt) + random.uniform(5, 15)\n",
        "                        print(f\"  ‚è≥ Rate Limit (Chunk p{chunk_data[0]['page_number']})... Waiting {wait:.1f}s\")\n",
        "                        await asyncio.sleep(wait)\n",
        "                        continue\n",
        "                    else:\n",
        "                        print(f\"‚ùå Error {response.status} on chunk p{chunk_data[0]['page_number']}\")\n",
        "                        return []\n",
        "            except Exception as e:\n",
        "                await asyncio.sleep(15)\n",
        "        return []\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MAIN WORKFLOW\n",
        "# ------------------------------------------------------------------------------\n",
        "async def main_textbook_monolith():\n",
        "    # A. Select Tags\n",
        "    tag_files = [b.name for b in bucket.list_blobs(prefix=\"Tags/\") if b.name.endswith('.txt')]\n",
        "    if not tag_files: print(\"‚ùå No tags found.\"); return\n",
        "\n",
        "    print(\"\\n--- SELECT TAG LIST ---\")\n",
        "    for i, f in enumerate(tag_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    t_idx = int(input(\"Choice: \")) - 1\n",
        "    tags_text = gcs_read_text(tag_files[t_idx])\n",
        "    tags_set = set(l.strip() for l in tags_text.splitlines() if l.strip())\n",
        "\n",
        "    # B. Select Textbook\n",
        "    content_files = [b.name for b in bucket.list_blobs(prefix=PATHS['gcs_content_textbooks']) if \"_CONTENT.json\" in b.name]\n",
        "    if not content_files: print(\"‚ùå No CONTENT files found.\"); return\n",
        "\n",
        "    print(\"\\n--- SELECT TEXTBOOK ---\")\n",
        "    for i, f in enumerate(content_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    c_idx = int(input(\"Choice: \")) - 1\n",
        "\n",
        "    content_path = content_files[c_idx]\n",
        "    book_base = content_path.split('/')[-1].replace(\"_CONTENT.json\", \"\")\n",
        "    fig_path = content_path.replace(\"_CONTENT.json\", \"_FIGURES.json\")\n",
        "    final_path = f\"{PATHS['gcs_content_textbooks']}/{book_base}_MASTER.json\"\n",
        "\n",
        "    print(f\"\\nüöÄ Processing: {book_base}\")\n",
        "\n",
        "    # C. RESUME LOGIC\n",
        "    master_kb = []\n",
        "\n",
        "    if bucket.blob(final_path).exists():\n",
        "        print(\"üîÑ Found existing MASTER file. Resuming...\")\n",
        "        master_kb = gcs_load_json(final_path)\n",
        "\n",
        "    raw_content = gcs_load_json(content_path)\n",
        "    raw_figures = gcs_load_json(fig_path)\n",
        "    raw_content.sort(key=lambda x: x['page_number'])\n",
        "\n",
        "    # D. Prepare Chunks\n",
        "    chunks = []\n",
        "    total_pages = len(raw_content)\n",
        "    for i in range(0, total_pages, PAGES_PER_CHUNK):\n",
        "        end_idx = min(i + PAGES_PER_CHUNK + PAGE_OVERLAP, total_pages)\n",
        "        chunk = raw_content[i : end_idx]\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    print(f\"üì¶ Total Chunks: {len(chunks)} (~{PAGES_PER_CHUNK} pages each)\")\n",
        "\n",
        "    # E. Batch Processing\n",
        "    # We group chunks into batches (e.g. 5 chunks) to save intermittently\n",
        "    chunk_batches = [chunks[i:i + SAVE_EVERY_N_CHUNKS] for i in range(0, len(chunks), SAVE_EVERY_N_CHUNKS)]\n",
        "\n",
        "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for batch_idx, batch in enumerate(chunk_batches):\n",
        "            print(f\"\\n--- Processing Batch {batch_idx + 1}/{len(chunk_batches)} ---\")\n",
        "\n",
        "            tasks = []\n",
        "            for chunk in batch:\n",
        "                page_nums = {p['page_number'] for p in chunk}\n",
        "                chunk_figs = [f for f in raw_figures if f['source_page'] in page_nums]\n",
        "                tasks.append(process_textbook_chunk(session, chunk, chunk_figs, tags_text, tags_set, sem))\n",
        "\n",
        "            results = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "            # Add to Master List\n",
        "            new_count = 0\n",
        "            for res_list in results:\n",
        "                for ent in res_list:\n",
        "                    master_kb.append(ent)\n",
        "                    new_count += 1\n",
        "\n",
        "            # --- AUTO SAVE ---\n",
        "            if new_count > 0:\n",
        "                print(f\"üíæ Saving progress... (+{new_count} entities)\")\n",
        "                gcs_upload_json(master_kb, final_path)\n",
        "            else:\n",
        "                print(\"   (No new entities in this batch)\")\n",
        "\n",
        "    # F. Final Deduplication & Save\n",
        "    print(\"\\nüßπ Final Deduplication...\")\n",
        "    unique_kb = []\n",
        "    seen_keys = set()\n",
        "    for ent in master_kb:\n",
        "        # Dedupe key: Name + first 50 chars of definition\n",
        "        key = (ent.get('entity_name'), ent.get('definition', '')[:50])\n",
        "        if key not in seen_keys:\n",
        "            unique_kb.append(ent)\n",
        "            seen_keys.add(key)\n",
        "\n",
        "    gcs_upload_json(unique_kb, final_path)\n",
        "    print(f\"\\n‚úÖ DONE: gs://{GCS_BUCKET_NAME}/{final_path}\")\n",
        "    print(f\"üìä Final Count: {len(unique_kb)} Entities\")\n",
        "\n",
        "await main_textbook_monolith()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF BLOCK 3: THE CONSOLIDATOR (Map-Reduce Merge)"
      ],
      "metadata": {
        "id": "GKMkiyqh4CM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 3: THE CONSOLIDATOR (Map-Reduce Merge)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Any  # <--- Added missing imports\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Flash is perfect for merging text. It is fast and respects the data.\n",
        "MODEL_NAME = \"gemini-3-flash-preview\"\n",
        "CONCURRENCY_LIMIT = 15\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_load_json(blob_path: str) -> List:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data: Any, blob_path: str):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "# --- PROMPT ---\n",
        "def construct_merge_prompt(entity_name, fragments):\n",
        "    return f\"\"\"\n",
        "Role: Medical Data Editor.\n",
        "Task: Merge these fragmented records for \"{entity_name}\" into ONE comprehensive entry.\n",
        "\n",
        "INPUT FRAGMENTS (from different chapters):\n",
        "{json.dumps(fragments, indent=2)}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. **Consolidate Text:** Combine 'clinical', 'microscopic', 'definition', etc. Do not lose details.\n",
        "   - If Fragment A has \"Clinical: Itchy\" and Fragment B has \"Clinical: Purple papules\", the result must be \"Itchy, purple papules.\"\n",
        "   - **Crucial:** Preserve all specific stains (CD45+, S100) and genetic findings.\n",
        "2. **Merge Figures:** Combine all 'related_figures' into one list. Remove duplicates if exact same ID.\n",
        "3. **Preserve Tags:** Use the most specific tag available.\n",
        "4. **Output:** A single JSON object.\n",
        "\n",
        "REQUIRED SCHEMA:\n",
        "{{\n",
        "    \"entity_name\": \"{entity_name}\",\n",
        "    \"definition\": \"Merged...\",\n",
        "    \"tags\": [\"...\"],\n",
        "    \"html_gcs_path\": null,\n",
        "    \"clinical\": \"...\",\n",
        "    \"pathogenesis\": \"...\",\n",
        "    \"macroscopic\": \"...\",\n",
        "    \"microscopic\": \"...\",\n",
        "    \"cytology\": \"...\",\n",
        "    \"ancillary_studies\": \"...\",\n",
        "    \"differential_diagnosis\": \"...\",\n",
        "    \"staging\": \"...\",\n",
        "    \"prognosis_and_prediction\": \"...\",\n",
        "    \"related_figures\": [...]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# --- AI WORKER ---\n",
        "async def merge_entity_group(session, tag, group, sem):\n",
        "    async with sem:\n",
        "        # If only 1 entry, no merge needed\n",
        "        if len(group) == 1:\n",
        "            return group[0]\n",
        "\n",
        "        # Construct Prompt\n",
        "        entity_name = group[0]['entity_name']\n",
        "        prompt = construct_merge_prompt(entity_name, group)\n",
        "\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "        payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "\n",
        "        try:\n",
        "            async with session.post(url, json=payload) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "                    raw = data['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
        "                    if match:\n",
        "                        return json.loads(match.group(0))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Fallback: Just return the first one if AI fails (prevents data loss)\n",
        "        print(f\"‚ö†Ô∏è Merge failed for {entity_name}, keeping fragments.\")\n",
        "        return group[0]\n",
        "\n",
        "# --- MAIN ---\n",
        "async def main_consolidator():\n",
        "    # 1. Select Content\n",
        "    content_files = [b.name for b in bucket.list_blobs(prefix=PATHS['gcs_content_textbooks']) if \"_MASTER.json\" in b.name and \"_CONSOLIDATED\" not in b.name]\n",
        "    if not content_files: print(\"‚ùå No MASTER files found. Run Block 2 first.\"); return\n",
        "\n",
        "    print(\"\\n--- SELECT MASTER FILE TO CONSOLIDATE ---\")\n",
        "    for i, f in enumerate(content_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    c_idx = int(input(\"Choice: \")) - 1\n",
        "\n",
        "    master_path = content_files[c_idx]\n",
        "    raw_entities = gcs_load_json(master_path)\n",
        "\n",
        "    print(f\"\\nüöÄ Consolidating {len(raw_entities)} entities...\")\n",
        "\n",
        "    # 2. Group by Tag (or Name if Tag is generic)\n",
        "    groups = defaultdict(list)\n",
        "    for ent in raw_entities:\n",
        "        # Key strategy: Use the first Tag as the primary key.\n",
        "        # If tag is missing/generic, fallback to Entity Name.\n",
        "        tag_list = ent.get('tags', [])\n",
        "        key = tag_list[0] if tag_list else ent.get('entity_name', 'Unknown')\n",
        "        groups[key].append(ent)\n",
        "\n",
        "    print(f\"   -> Found {len(groups)} unique topics (Tags/Names).\")\n",
        "\n",
        "    # 3. Process Groups\n",
        "    sem = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
        "    final_kb = []\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = []\n",
        "        for key, group in groups.items():\n",
        "            tasks.append(merge_entity_group(session, key, group, sem))\n",
        "\n",
        "        results = await tqdm_asyncio.gather(*tasks)\n",
        "        final_kb = results\n",
        "\n",
        "    # 4. Save\n",
        "    out_path = master_path.replace(\"_MASTER.json\", \"_CONSOLIDATED.json\")\n",
        "    gcs_upload_json(final_kb, out_path)\n",
        "    print(f\"\\n‚úÖ CONSOLIDATED MASTER SAVED: gs://{GCS_BUCKET_NAME}/{out_path}\")\n",
        "    print(f\"üìä Reduced {len(raw_entities)} fragments -> {len(final_kb)} unique entities.\")\n",
        "\n",
        "await main_consolidator()"
      ],
      "metadata": {
        "id": "oZAVfDpP3_DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIDEO BLOCK 1"
      ],
      "metadata": {
        "id": "HVCDbcXT5MqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 1: LECTURE EXTRACTOR (Whisper + Gemini 3 Flash)\n",
        "# ==============================================================================\n",
        "import shutil, cv2, whisper, json, os, io, base64, re, asyncio, aiohttp\n",
        "import logging\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from google.cloud import storage\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "API_CONCURRENCY_LIMIT = 20\n",
        "VISION_MODEL = \"gemini-3-pro-preview\" # Fast & Cheap for per-slide analysis\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_upload_file(local_path, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_filename(local_path)\n",
        "\n",
        "def gcs_upload_json(data, blob_path):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def gcs_exists(blob_path):\n",
        "    return bucket.blob(blob_path).exists()\n",
        "\n",
        "def get_comparison_frame(frame):\n",
        "    h, w = frame.shape[:2]\n",
        "    new_w = 200\n",
        "    new_h = int(h * (new_w / w))\n",
        "    small = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "    gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)\n",
        "    return cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# --- AI ANALYST ---\n",
        "async def analyze_slide_async(session, slide_data, local_img_path, sem):\n",
        "    async with sem:\n",
        "        if not os.path.exists(local_img_path): return slide_data\n",
        "\n",
        "        try:\n",
        "            # Prepare Image\n",
        "            with Image.open(local_img_path) as img:\n",
        "                buf = io.BytesIO()\n",
        "                img.convert(\"RGB\").save(buf, format=\"JPEG\")\n",
        "                b64_img = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "            url = f\"https://generativelanguage.googleapis.com/v1beta/models/{VISION_MODEL}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "            # Prompt: Extract raw visual data. We don't need deep reasoning yet, just \"What is on this slide?\"\n",
        "            prompt = (\n",
        "                f\"Transcript Context: \\\"{slide_data['raw_transcript'][:1000]}...\\\"\\n\\n\"\n",
        "                \"TASK: Analyze this slide image. \\n\"\n",
        "                \"1. Extract the Title.\\n\"\n",
        "                \"2. Extract text labels verbatim (e.g. 'CD45', 'H&E', '40x').\\n\"\n",
        "                \"3. Summarize the visual content (e.g., 'Histology showing...').\\n\"\n",
        "                \"Return JSON: {\\\"slide_title\\\": \\\"...\\\", \\\"key_points\\\": [\\\"...\\\"], \\\"visual_desc\\\": \\\"...\\\"}\"\n",
        "            )\n",
        "\n",
        "            payload = {\"contents\": [{\"parts\": [{\"text\": prompt}, {\"inline_data\": {\"mime_type\": \"image/jpeg\", \"data\": b64_img}}]}]}\n",
        "\n",
        "            async with session.post(url, json=payload) as res:\n",
        "                if res.status == 200:\n",
        "                    dat = await res.json()\n",
        "                    txt = dat['candidates'][0]['content']['parts'][0]['text']\n",
        "                    match = re.search(r'\\{.*\\}', txt, re.DOTALL)\n",
        "                    if match:\n",
        "                        slide_data.update(json.loads(match.group(0)))\n",
        "        except Exception as e:\n",
        "            pass # Skip frame if AI fails\n",
        "\n",
        "        return slide_data\n",
        "\n",
        "# --- PIPELINE ---\n",
        "async def process_video(video_path, counter, total):\n",
        "    fname = os.path.basename(video_path)\n",
        "    lecture_name = os.path.splitext(fname)[0].replace(\" \", \"_\")\n",
        "\n",
        "    # GCS Paths\n",
        "    asset_base = f\"{PATHS['gcs_asset_lectures']}/{lecture_name}\"\n",
        "    raw_json_path = f\"{PATHS['gcs_content_lectures']}/{lecture_name}_RAW.json\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\\nüé• PROCESSING {counter}/{total}: {lecture_name}\\n{'='*60}\")\n",
        "\n",
        "    if gcs_exists(raw_json_path):\n",
        "        print(\"‚úÖ Already processed in GCS. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # 1. WHISPER TRANSCRIPTION\n",
        "    print(\"üéôÔ∏è Step 1: Whisper Transcription...\")\n",
        "    model = whisper.load_model(\"base\") # Use 'small' if you have GPU RAM, 'base' is fast\n",
        "    result = model.transcribe(video_path, fp16=False)\n",
        "\n",
        "    # 2. FRAME EXTRACTION & MERGING\n",
        "    print(\"üéûÔ∏è Step 2: Extracting Slides...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    slides = []\n",
        "    curr_slide = None\n",
        "    prev_cmp = None\n",
        "\n",
        "    # We use TQDM to track progress through the audio segments\n",
        "    for seg in tqdm(result['segments'], desc=\"Scanning\", unit=\"seg\"):\n",
        "        cap.set(cv2.CAP_PROP_POS_MSEC, seg['start'] * 1000)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: continue\n",
        "\n",
        "        curr_cmp = get_comparison_frame(frame)\n",
        "\n",
        "        if curr_slide is None:\n",
        "            curr_slide = {**seg, 'frame': frame}\n",
        "            prev_cmp = curr_cmp\n",
        "            continue\n",
        "\n",
        "        # SSIM Check (Merge if > 85% similar)\n",
        "        if ssim(prev_cmp, curr_cmp, data_range=255) >= 0.85:\n",
        "            curr_slide['text'] += \" \" + seg['text']\n",
        "            curr_slide['end'] = seg['end']\n",
        "        else:\n",
        "            slides.append(curr_slide)\n",
        "            curr_slide = {**seg, 'frame': frame}\n",
        "            prev_cmp = curr_cmp\n",
        "\n",
        "    if curr_slide: slides.append(curr_slide)\n",
        "    cap.release()\n",
        "    print(f\"   -> Consolidated into {len(slides)} unique slides.\")\n",
        "\n",
        "    # 3. UPLOAD & PREPARE\n",
        "    print(\"‚òÅÔ∏è Step 3: Uploading Images...\")\n",
        "    final_data = []\n",
        "    local_imgs = {} # Map id -> local path for AI step\n",
        "\n",
        "    for i, slide in enumerate(slides):\n",
        "        img_name = f\"{lecture_name}_slide_{i+1:04d}.jpg\"\n",
        "        local_p = f\"/tmp/{img_name}\"\n",
        "        gcs_p = f\"{asset_base}/{img_name}\"\n",
        "        full_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_p}\"\n",
        "\n",
        "        cv2.imwrite(local_p, slide['frame'])\n",
        "\n",
        "        if not gcs_exists(gcs_p):\n",
        "            gcs_upload_file(local_p, gcs_p)\n",
        "\n",
        "        local_imgs[i] = local_p\n",
        "\n",
        "        final_data.append({\n",
        "            \"id\": i,\n",
        "            \"timestamp_start\": slide['start'],\n",
        "            \"timestamp_end\": slide['end'],\n",
        "            \"raw_transcript\": slide['text'].strip(),\n",
        "            \"image_path\": full_uri,\n",
        "            \"gcs_path\": full_uri, # Important for Block 2\n",
        "            \"slide_title\": \"\",\n",
        "            \"key_points\": [],\n",
        "            \"visual_desc\": \"\"\n",
        "        })\n",
        "\n",
        "    # 4. GEMINI ENHANCEMENT\n",
        "    print(\"üß† Step 4: Gemini Vision Analysis...\")\n",
        "    sem = asyncio.Semaphore(API_CONCURRENCY_LIMIT)\n",
        "    async with aiohttp.ClientSession() as sess:\n",
        "        tasks = [analyze_slide_async(sess, d, local_imgs[d['id']], sem) for d in final_data]\n",
        "        enhanced_data = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "    # 5. SAVE RAW JSON\n",
        "    gcs_upload_json(enhanced_data, raw_json_path)\n",
        "    print(f\"‚úÖ Saved RAW data: {raw_json_path}\")\n",
        "\n",
        "    # Cleanup\n",
        "    for p in local_imgs.values():\n",
        "        if os.path.exists(p): os.remove(p)\n",
        "\n",
        "# --- RUNNER ---\n",
        "async def main_lectures():\n",
        "    vid_files = sorted([f for f in os.listdir(PATHS['source_videos']) if f.lower().endswith(('.mp4', '.mov'))])\n",
        "    if not vid_files: print(\"‚ùå No videos found.\"); return\n",
        "\n",
        "    print(\"\\n--- AVAILABLE LECTURES ---\")\n",
        "    for i, v in enumerate(vid_files): print(f\"[{i+1}] {v}\")\n",
        "\n",
        "    sel = input(\"\\nSelect (e.g. 1, 3-5, or 'all'): \")\n",
        "    indices = set()\n",
        "    if sel == 'all': indices = range(len(vid_files))\n",
        "    else:\n",
        "        for part in sel.split(','):\n",
        "            if '-' in part:\n",
        "                s, e = map(int, part.split('-'))\n",
        "                indices.update(range(s-1, e))\n",
        "            elif part.strip().isdigit():\n",
        "                indices.add(int(part)-1)\n",
        "\n",
        "    for idx in sorted(list(indices)):\n",
        "        if 0 <= idx < len(vid_files):\n",
        "            await process_video(os.path.join(PATHS['source_videos'], vid_files[idx]), idx+1, len(indices))\n",
        "\n",
        "await main_lectures()"
      ],
      "metadata": {
        "id": "ZzGRiY1z5XBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIDEO BLOCK 2"
      ],
      "metadata": {
        "id": "x0-3O_Ex5Sf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "# ==============================================================================\n",
        "# BLOCK 2: LECTURE ARCHITECT (High-Fidelity Monolith)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import re\n",
        "import difflib\n",
        "import random\n",
        "from typing import List, Dict, Set, Any\n",
        "from google.cloud import storage\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Use the smartest model you have access to. 1.5 Pro is the current gold standard for context.\n",
        "MODEL_NAME = \"gemini-3-pro-preview\"\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "# --- HELPERS ---\n",
        "def gcs_read_text(blob_path: str) -> str:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return blob.download_as_string().decode('utf-8') if blob.exists() else \"\"\n",
        "\n",
        "def gcs_load_json(blob_path: str) -> List:\n",
        "    blob = bucket.blob(blob_path)\n",
        "    return json.loads(blob.download_as_string()) if blob.exists() else []\n",
        "\n",
        "def gcs_upload_json(data: Any, blob_path: str):\n",
        "    blob = bucket.blob(blob_path)\n",
        "    blob.upload_from_string(json.dumps(data, indent=2), content_type='application/json')\n",
        "\n",
        "def validate_tag(tag: str, valid_set: Set[str]) -> str:\n",
        "    if not tag: return \"Skin::Unclassified\"\n",
        "    clean = tag.strip()\n",
        "    if clean in valid_set: return clean\n",
        "    matches = difflib.get_close_matches(clean, list(valid_set), n=1, cutoff=0.7)\n",
        "    return matches[0] if matches else clean\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. PROMPT ENGINEERING (MONOLITHIC)\n",
        "# ------------------------------------------------------------------------------\n",
        "def construct_monolith_prompt(transcript_data, valid_tags_list):\n",
        "    return f\"\"\"\n",
        "Role: You are a Senior Dermatopathologist and Data Engineer.\n",
        "Objective: Convert the ENTIRE LECTURE provided below into a standardized Knowledge Base.\n",
        "\n",
        "INPUT DATA:\n",
        "- A chronological sequence of slides and transcript segments.\n",
        "- Each segment includes: Visual Description (from AI), Key Text on Slide, and Audio Transcript.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. **Consolidate:** The lecture may discuss one disease (e.g., Merkel Cell) across 10 slides. You must merge all that info into a SINGLE JSON entry.\n",
        "2. **Detail Extraction (CRITICAL):**\n",
        "   - **Stains (IHC):** You MUST list every specific stain mentioned in text OR visible on the slide (e.g., \"CK20+\", \"TTF-1 negative\", \"CD45-\").\n",
        "   - **Do NOT summarize** as \"ruled out lymphoma\". Explicitly write \"CD45 negative\".\n",
        "   - **Genetics:** List specific mutations (e.g., \"Polyomavirus\", \"t(11;19)\").\n",
        "3. **Images:** Select the BEST representative slides for the 'related_figures' field.\n",
        "   - **Legend:** Must be specific (e.g., \"CK20 immunostain showing perinuclear dot-like positivity\").\n",
        "\n",
        "REQUIRED JSON SCHEMA (Do not deviate):\n",
        "[\n",
        "  {{\n",
        "    \"entity_name\": \"Disease Name\",\n",
        "    \"definition\": \"...\",\n",
        "    \"related_terminology\": \"...\",\n",
        "    \"subtypes\": \"...\",\n",
        "    \"tags\": [\"Exact_Tag_From_List\"],\n",
        "    \"html_gcs_path\": null,\n",
        "\n",
        "    \"localization\": \"...\",\n",
        "    \"clinical\": \"...\",\n",
        "    \"pathogenesis\": \"...\",\n",
        "    \"staging\": \"...\",\n",
        "    \"prognosis_and_prediction\": \"...\",\n",
        "\n",
        "    \"macroscopic\": \"...\",\n",
        "    \"microscopic\": \"Detailed histology...\",\n",
        "    \"cytology\": \"...\",\n",
        "    \"ancillary_studies\": \"List ALL positive/negative stains and molecular findings.\",\n",
        "    \"diagnostic_molecular_pathology\": \"...\",\n",
        "\n",
        "    \"differential_diagnosis\": \"...\",\n",
        "    \"essential_and_desirable_diagnostic_criteria\": \"...\",\n",
        "\n",
        "    \"related_figures\": [\n",
        "        {{\n",
        "            \"id\": \"Slide_ID\",\n",
        "            \"src\": \"gs://...\",\n",
        "            \"gcs_path\": \"gs://...\",\n",
        "            \"diagnosis\": \"Disease Name\",\n",
        "            \"legend\": \"Specific description including stains/features shown.\"\n",
        "        }}\n",
        "    ]\n",
        "  }}\n",
        "]\n",
        "\n",
        "REFERENCE TAGS:\n",
        "{valid_tags_list}\n",
        "\n",
        "LECTURE CONTENT:\n",
        "{transcript_data}\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. AI PROCESSING (SINGLE SHOT)\n",
        "# ------------------------------------------------------------------------------\n",
        "async def process_full_lecture(session, slides, valid_tags_text, valid_tags_set):\n",
        "    # 1. BUILD THE MONOLITH\n",
        "    # Construct one massive string containing the whole lecture.\n",
        "    formatted_input = \"\"\n",
        "    for s in slides:\n",
        "        formatted_input += f\"\\n--- SLIDE {s['id']} (Time: {s['timestamp_start']:.0f}s) ---\\n\"\n",
        "        formatted_input += f\"IMAGE_PATH: {s['gcs_path']}\\n\"\n",
        "        formatted_input += f\"VISUAL_CONTEXT: {s.get('visual_desc', '')}\\n\"\n",
        "        formatted_input += f\"TEXT_SEEN_ON_SLIDE: {s.get('key_points', [])}\\n\" # Vital for \"CD45\"\n",
        "        formatted_input += f\"TRANSCRIPT: {s['raw_transcript']}\\n\"\n",
        "\n",
        "    print(f\"üì¶ Payload Size: {len(formatted_input)} characters. Sending to Gemini Pro...\")\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "    payload = {\"contents\": [{\"parts\": [{\"text\": construct_monolith_prompt(formatted_input, valid_tags_text)}]}]}\n",
        "\n",
        "    # Retry logic for network blips\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            # 5-minute timeout for deep thinking\n",
        "            async with session.post(url, json=payload, timeout=300) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "                    raw_txt = data['candidates'][0]['content']['parts'][0]['text']\n",
        "\n",
        "                    # Clean markdown code blocks\n",
        "                    raw_txt = raw_txt.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "                    match = re.search(r'\\[.*\\]', raw_txt, re.DOTALL)\n",
        "                    if match:\n",
        "                        entities = json.loads(match.group(0))\n",
        "\n",
        "                        print(f\"üß† AI identified {len(entities)} entities.\")\n",
        "                        for ent in entities:\n",
        "                            # Tag Validation\n",
        "                            ent['tags'] = [validate_tag(t, valid_tags_set) for t in ent.get('tags', [])]\n",
        "\n",
        "                            # Null Enforcing\n",
        "                            required_keys = [\n",
        "                                \"microscopic\", \"ancillary_studies\", \"differential_diagnosis\",\n",
        "                                \"diagnostic_molecular_pathology\", \"staging\", \"cytology\"\n",
        "                            ]\n",
        "                            for key in required_keys:\n",
        "                                if key not in ent: ent[key] = None\n",
        "                            ent['html_gcs_path'] = None\n",
        "\n",
        "                        return entities\n",
        "                    else:\n",
        "                        print(\"‚ùå Failed to parse JSON from AI response.\")\n",
        "                        return []\n",
        "                elif response.status == 429:\n",
        "                    print(f\"  ‚è≥ Rate Limit... Waiting 20s\")\n",
        "                    await asyncio.sleep(20)\n",
        "                else:\n",
        "                    print(f\"‚ùå API Error {response.status}\")\n",
        "                    await asyncio.sleep(5)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Exception: {e}\")\n",
        "            await asyncio.sleep(5)\n",
        "\n",
        "    return []\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. MAIN WORKFLOW\n",
        "# ------------------------------------------------------------------------------\n",
        "async def main_lecture_monolith():\n",
        "    # 1. Tags\n",
        "    tag_files = [b.name for b in bucket.list_blobs(prefix=\"Tags/\") if b.name.endswith('.txt')]\n",
        "    if not tag_files: print(\"‚ùå No tags found.\"); return\n",
        "\n",
        "    print(\"\\n--- SELECT TAG LIST ---\")\n",
        "    for i, f in enumerate(tag_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    t_idx = int(input(\"Choice: \")) - 1\n",
        "    tags_text = gcs_read_text(tag_files[t_idx])\n",
        "    tags_set = set(l.strip() for l in tags_text.splitlines() if l.strip())\n",
        "\n",
        "    # 2. Lecture Raw Data\n",
        "    raw_files = [b.name for b in bucket.list_blobs(prefix=PATHS['gcs_content_lectures']) if \"_RAW.json\" in b.name]\n",
        "    if not raw_files: print(\"‚ùå No RAW files. Run Block 1 first.\"); return\n",
        "\n",
        "    print(\"\\n--- SELECT LECTURE ---\")\n",
        "    for i, f in enumerate(raw_files): print(f\"[{i+1}] {f.split('/')[-1]}\")\n",
        "    c_idx = int(input(\"Choice: \")) - 1\n",
        "\n",
        "    raw_path = raw_files[c_idx]\n",
        "    lecture_name = raw_path.split('/')[-1].replace(\"_RAW.json\", \"\")\n",
        "    slides_data = gcs_load_json(raw_path)\n",
        "\n",
        "    print(f\"\\nüöÄ Processing: {lecture_name}\")\n",
        "    print(f\"   Mode: THE MONOLITH (Full Context Analysis)\")\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        master_kb = await process_full_lecture(session, slides_data, tags_text, tags_set)\n",
        "\n",
        "    if master_kb:\n",
        "        final_path = f\"{PATHS['gcs_content_lectures']}/{lecture_name}_MASTER.json\"\n",
        "        gcs_upload_json(master_kb, final_path)\n",
        "        print(f\"\\n‚úÖ MASTER SAVED: gs://{GCS_BUCKET_NAME}/{final_path}\")\n",
        "        print(f\"üìä Extracted {len(master_kb)} High-Quality Entities\")\n",
        "    else:\n",
        "        print(\"‚ùå Architecture failed.\")\n",
        "\n",
        "await main_lecture_monolith()"
      ],
      "metadata": {
        "id": "GgyXTjmV5f3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}